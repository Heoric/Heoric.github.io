<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>回溯算法</title>
    <url>/2023/09/15/%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>回溯，全排列，N皇后。</p>
<span id="more"></span>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">permute</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="built_in">dfs</span>(nums, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(vector&lt;<span class="type">int</span>&gt; nums, <span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == nums.<span class="built_in">size</span>() - <span class="number">1</span>) &#123;</span><br><span class="line">            res.<span class="built_in">push_back</span>(nums);      <span class="comment">// 添加排列方案</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = x; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[x]);   <span class="comment">// 交换，将 nums[i] 固定在第 x 位</span></span><br><span class="line">            <span class="built_in">dfs</span>(nums, x + <span class="number">1</span>);         <span class="comment">// 开启固定第 x + 1 位元素</span></span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[x]);   <span class="comment">// 恢复交换</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title>我的博客</title>
    <url>/2023/01/31/template/</url>
    <content><![CDATA[<p>我的第一篇博客</p>
<span id="more"></span>

<p>正片开始</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">关系模型</span><br><span class="line">关系代数</span><br><span class="line"></span><br><span class="line">parser</span><br><span class="line">	词法</span><br><span class="line">	语法</span><br><span class="line">	</span><br><span class="line">AST</span><br><span class="line"></span><br><span class="line">planner</span><br><span class="line">	启发式（Heuristics）</span><br><span class="line">	启发式+基于代价的JOIN（Heuristics + Cost-based Join Order Search）</span><br><span class="line">	分层优化器框架（Stratified Search）</span><br><span class="line">	统一优化器框架（Unified Search）</span><br><span class="line">	</span><br><span class="line">	system-r</span><br><span class="line">	v</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>template</category>
      </categories>
      <tags>
        <tag>模版</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库SQL总结</title>
    <url>/2023/01/31/2-%E6%95%B0%E6%8D%AE%E5%BA%93/template/</url>
    <content><![CDATA[<p>数据库、操作系统和编译器是计算机软件的三大系统。其中数据库更接近业务层，几乎所有软件都会用到数据库，所以对数据库的熟练使用时每个开发人员必备的技能，而数据库对外最直接的接口就是 SQL。</p>
<p>今天我们来讲讲数据库SQL。</p>
<span id="more"></span>

<p>基本SQL</p>
<p>建表</p>
<p>中级SQL</p>
<p>高级SQL</p>
<p>join</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>c++11 新特性</title>
    <url>/2023/01/31/3-c++/c++11/</url>
    <content><![CDATA[<span id="more"></span>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>编译原理</title>
    <url>/2023/08/22/2-%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>什么是编译器 ？他的作用是什么？他的输入输出是什么？</p>
<p>编译器是一个程序，他将源程序翻译成等价的目标程序，输入数据是类似 C语言的高级语言，输出是类似汇编语言或者机器语言的低级语言。</p>
<p>编译器与解释器有什么区别 ？</p>
<p>对编译器来说，编译和运行时两个阶段。解释器则不需要将这两阶段分开；</p>
<p>编译器和解释器的根本区别在于是否生产目标代码。编译器会生成目标代码，然后运行，而解释器分析后直接运行生成结果；</p>
<span id="more"></span>

<p>编译器和解释器的存储组织方式不同，对编译器来说，在源程序被编译阶段，存储区要为源程序和目标程序开辟空间，存放需要使用的各种表，如符号表，在运行阶段需要存放目标代码和数据，编译阶段的信息不再需要。解释器在整个过程中源程序、符号表都需要存储在存储区中；</p>
<p>编译器比解释器效率高，因为编译器时一次翻译，多次运行。而解释器每次运行都需要逐行翻译。</p>
<p>编译器工作分为几个阶段 ？</p>
<p>一般分为 词法分析、语法分析、语义分析、中间代码生成、代码优化、目标代码生成这六个阶段。期间要有错误处理等。当然不是严格分为几个阶段。不同编译器不一样。</p>
<p>编译过程中的前端和后端如何划分，为什么要划分前后端 ？</p>
<p>前端主要依赖于源代码，与目标机器无关。后端主要依赖于目标机器，与源程序无关。前端包括词法分析，语法分析，语义分析和中间代码生成，还有某些优化工作。后端包括目标代码生成。</p>
<p>按照这种方式，可以实现不同机器使用同一源程序的编译器（前端相同，后端不同），也可以为同一期间生成几个语言的编译程序（前端不同，后端相同）。</p>
<h2 id="文法和语言"><a href="#文法和语言" class="headerlink" title="文法和语言"></a>文法和语言</h2><p>什么是语法，什么是语义 ？</p>
<p>语法是一组规则，用来定义什么样的符号序列是合法的。语义进一步判断合法的符号是否能构成正确的程序。比如类型检查就无法从语法上判断，但是可以在语义分析阶段判断。</p>
<p>文法的分类有哪些 ？</p>
<p>乔姆斯基（Chomsky）于1956年建立了形式语言的描述，把文法分成四种类型，即0型（短语文法）,1型（上下文有关文法）,2型（上下文无关文法）,3型（正规文法）。</p>
<p>上下文无关文法（context-free grammar）是什么？</p>
<p>上下文无关文法由四个元素组成，一个终结符号集合、一个非终结符号集合、一个产生式集合、一个非终结符号为开始符号。</p>
<p>如何进行语法分析，有哪些基本方法？</p>
<p>语法分析分为两大类，自顶向下分析和自底向上分析。</p>
<p>自顶向下分析思想：从文法的开始符号出发，反复使用各种产生式，逐步向下推导，试图推导出句子。存在的问题：在推导中如何选择规则？采用回溯法可行，但是代价太高，还可能陷入死循环。</p>
<p>自底向上分析法思想：从输入符号串开始，逐步进行归约，直到规约到文法的开始符号。存在的问题：每次应归约当前句型的句柄，但如何找句柄，以及句柄是否唯一？对一个句型的短语、直接短语和句柄的判断，常用的方法是：（1）查看语法树的叶子结点（终结符或非终结符），如果叶子结点的父结点还有其他子结点，就将该父结点的所有子结点作为一个字符串集合来判断；（2）对子结点的判断要从左向右处理；向上判断短语是相对哪个非终结符的短语时，要一直处理到祖先结点。</p>
<h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><p>词法分析的主要任务是什么，输入输出是什么？</p>
<p>词法分析从左向右扫描每行源代码，识别出单词和属性，把单词转换成统一的内部表示给语法分析。还需要删除注释，空格，换行等非必要信息。 输入源程序代码，输出单词符号（token）。</p>
<p>单词符号（token）分为哪些 ？</p>
<p>关键字、标识符、常数、运算符、界限符（分号，括号等）。 </p>
<h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><p>什么是自顶向下分析法 ？</p>
<p>自顶向下分析法也称面向目标的分析方法，也就是从文法的开始符号出发企图推导出与输入的单词串完全相匹配的句子，若输入串是给定文法的句子，则必能推出，反之必然出错。自顶向下分析包括确定分析和不确定分析。</p>
]]></content>
      <categories>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title>深入C++虚函数</title>
    <url>/2023/01/31/3-c++/%E8%99%9A%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>我一直认为 c++ 的精髓是 虚函数。</p>
<p>虚函数是运行时多状的基础。</p>
<p>今天就来扒一扒虚函数的内部机制。</p>
<span id="more"></span>

<h3 id="没有虚函数"><a href="#没有虚函数" class="headerlink" title="没有虚函数"></a>没有虚函数</h3><p>我们从 内存 来看 虚函数的 底层机制。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> base_1;</span><br><span class="line">    <span class="type">int</span> base_2;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base b;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(b) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 8</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_1) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 0</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_2) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 4</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 结果很明显， 不需要多说什么。</span></span><br></pre></td></tr></table></figure>

<h3 id="只有一个虚函数"><a href="#只有一个虚函数" class="headerlink" title="只有一个虚函数"></a>只有一个虚函数</h3><p>接下来，我们看一下，带虚函数的对象的内存。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> base_1;</span><br><span class="line">    <span class="type">int</span> base_2;</span><br><span class="line">		<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base b;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(b) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 16</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_1) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 8</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_2) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 12</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看出来，对象前面多了八个字节。让我们来看看多了的是什么。</p>
<p>其实我们都知道，虚函数是在一个虚函数表里面。那么我们看看对象 b 的布局。</p>
<p>可以看到里面有个 指针，我们把里面的值打出来，看到是 一个函数指针，指向了 func()。</p>
<p>新定义一个变量 b1，查看 b1 的虚函数表，和内容，可以看到跟 b 是一摸一样的。说明一个类，只有一个虚函数表。</p>
<p>![image-20230605165441623](&#x2F;Users&#x2F;leoric&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230605165441623.png)</p>
<h3 id="多个虚函数"><a href="#多个虚函数" class="headerlink" title="多个虚函数"></a>多个虚函数</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> base_1;</span><br><span class="line">    <span class="type">int</span> base_2;</span><br><span class="line">		<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">  	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base b;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(b) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 16</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_1) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 8</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_2) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 12</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>可以看到，对象对大小不变， 虚函数表里多了 func1。</p>
<p>![image-20230605170310030](&#x2F;Users&#x2F;leoric&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230605170310030.png)</p>
<h3 id="单继承且本身不存在虚函数"><a href="#单继承且本身不存在虚函数" class="headerlink" title="单继承且本身不存在虚函数"></a>单继承且本身不存在虚函数</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> base_1;</span><br><span class="line">    <span class="type">int</span> base_2;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derive</span> : <span class="keyword">public</span> Base</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> derive_1;</span><br><span class="line">    <span class="type">int</span> derive_2;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base b;</span><br><span class="line">    Base b1; </span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(b) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_1) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">offsetof</span>(Base, base_2) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    Derive d;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(b) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">//16</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>直接看内存， 虚函数表的地址不一样，但是指向了相同的函数。</p>
<p>![image-20230605172826693](&#x2F;Users&#x2F;leoric&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230605172826693.png)</p>
<h3 id="本身不存在虚函数但存在基类虚函数覆盖"><a href="#本身不存在虚函数但存在基类虚函数覆盖" class="headerlink" title="本身不存在虚函数但存在基类虚函数覆盖"></a>本身不存在虚函数但存在基类虚函数覆盖</h3>]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>c++ 模版元编程</title>
    <url>/2023/05/26/3-c++/00-%E6%A8%A1%E7%89%88%E5%85%83%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<p>本系列文章从零开始介绍C++模版元编程，需要有C++基础。</p>
<span id="more"></span>

<h3 id="函数模版"><a href="#函数模版" class="headerlink" title="函数模版"></a>函数模版</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">add</span><span class="params">(T <span class="type">const</span> a, T <span class="type">const</span> b)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> a = <span class="built_in">add</span>(<span class="number">42</span>, <span class="number">21</span>);</span><br><span class="line"><span class="keyword">auto</span> d = <span class="built_in">add</span>&lt;<span class="type">double</span>&gt;(<span class="number">41.0</span>, <span class="number">21</span>); <span class="comment">// 无法自动推导类型的时候可以显示的指定类型</span></span><br></pre></td></tr></table></figure>



<h3 id="类模版"><a href="#类模版" class="headerlink" title="类模版"></a>类模版</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">wrapper</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">wrapper</span>(T <span class="type">const</span> v) : <span class="built_in">value</span>(v) &#123;&#125;</span><br><span class="line">  <span class="function">T <span class="type">const</span>&amp; <span class="title">get</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  T value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">wrapper <span class="title">a</span><span class="params">(<span class="number">1</span>)</span></span>; <span class="comment">// 模版参数推导</span></span><br><span class="line"><span class="function">wrapper&lt;<span class="type">int</span>&gt; <span class="title">b</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="成员函数模版"><a href="#成员函数模版" class="headerlink" title="成员函数模版"></a>成员函数模版</h3><h4 id="模版类的成员函数"><a href="#模版类的成员函数" class="headerlink" title="模版类的成员函数"></a>模版类的成员函数</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">comoosition</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function">T <span class="title">add</span><span class="params">(T <span class="type">const</span> a, T <span class="type">const</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">composition&lt;<span class="type">int</span>&gt; c;</span><br><span class="line">c.<span class="built_in">add</span>(<span class="number">12</span>,<span class="number">32</span>);</span><br></pre></td></tr></table></figure>

<h4 id="非模版类的成员函数模版"><a href="#非模版类的成员函数模版" class="headerlink" title="非模版类的成员函数模版"></a>非模版类的成员函数模版</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">compotion</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function">T <span class="title">add</span><span class="params">(T <span class="type">const</span> a, T <span class="type">const</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">compostion c;</span><br><span class="line">c.<span class="built_in">add</span>&lt;<span class="type">int</span>&gt;(<span class="number">12</span>,<span class="number">13</span>);</span><br><span class="line">c.<span class="built_in">add</span>(<span class="number">1</span>,<span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<h4 id="类模版的成员函数模版"><a href="#类模版的成员函数模版" class="headerlink" title="类模版的成员函数模版"></a>类模版的成员函数模版</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">wrapper</span> &#123;</span><br><span class="line">pbulic:</span><br><span class="line">  <span class="built_in">wrapper</span>(T <span class="type">const</span> v) : <span class="built_in">value</span>(v) &#123;&#125;</span><br><span class="line">  <span class="function">T <span class="type">const</span>&amp; <span class="title">get</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> U&gt;</span><br><span class="line">  <span class="function">U <span class="title">as</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;U&gt;(value);</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  T value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 成员函数模板的模板形参必须与类模板的模板形参不同</span></span><br><span class="line"></span><br><span class="line"><span class="function">wrapper&lt;<span class="type">double</span>&gt; <span class="title">a</span><span class="params">(<span class="number">42.1</span>)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> d = a.<span class="built_in">get</span>();</span><br><span class="line"><span class="keyword">auto</span> b = a.<span class="built_in">as</span>&lt;<span class="type">int</span>&gt;();</span><br></pre></td></tr></table></figure>



<h3 id="模版参数"><a href="#模版参数" class="headerlink" title="模版参数"></a>模版参数</h3><h4 id="类型模版参数"><a href="#类型模版参数" class="headerlink" title="类型模版参数"></a>类型模版参数</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="comment">// 不带默认参数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">wrapper</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>&gt; <span class="comment">// 带默认参数</span></span><br><span class="line"><span class="keyword">class</span> wrapper &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>... T&gt; <span class="comment">// 可变参数模版</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">wrapper</span> &#123;&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="非类型模版"><a href="#非类型模版" class="headerlink" title="非类型模版"></a>非类型模版</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> V&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">foo</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> V = <span class="number">42</span>&gt;</span><br><span class="line"><span class="keyword">class</span> foo &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span>... V&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">foo</span> &#123;&#125;;</span><br></pre></td></tr></table></figure>



<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="type">size_t</span> S&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">buffer</span> &#123;</span><br><span class="line">  T data_[S];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">constexpr</span> T <span class="type">const</span> * <span class="title">data</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> data_; &#125;</span><br><span class="line">  <span class="keyword">constexpr</span> T&amp; <span class="keyword">operator</span>[](<span class="type">size_t</span> <span class="type">const</span> index) &#123;</span><br><span class="line">    <span class="keyword">return</span> data_[index];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">constexpr</span> T <span class="type">const</span>&amp; <span class="keyword">operator</span>[] (<span class="type">size_t</span> <span class="type">const</span> index) <span class="type">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> data_[index];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">buffer&lt;<span class="type">int</span>, <span class="number">10</span>&gt; b1; </span><br><span class="line">buffer&lt;<span class="type">int</span>, <span class="number">2</span>*<span class="number">5</span>&gt; b2;</span><br></pre></td></tr></table></figure>

<p>看一种更常见的类型</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">device</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">output</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">device</span>() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="built_in">void</span> (*action) ()&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">smart_device</span> : device &#123;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">output</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    (*action) ();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">say_hello_in_english</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;Hello, world!\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">auto</span> w1 = std::make_unique&lt;smart_device&lt;&amp;say_hello_in_english&gt;&gt;();</span><br><span class="line">w1-&gt;<span class="built_in">output</span>();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="双重模版参数"><a href="#双重模版参数" class="headerlink" title="双重模版参数"></a>双重模版参数</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simple_wrapper</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	T value; </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">fancy_wrapper</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">fancy_wrapper</span>(T <span class="type">const</span> v) :<span class="built_in">value</span>(v) &#123;&#125;</span><br><span class="line">  </span><br><span class="line">	<span class="function">T <span class="type">const</span>&amp; <span class="title">get</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">  </span><br><span class="line">	<span class="keyword">template</span> &lt;<span class="keyword">typename</span> U&gt; <span class="function">U <span class="title">as</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;U&gt;(value); </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>: </span><br><span class="line">  T value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> U, <span class="keyword">template</span>&lt;<span class="keyword">typename</span>&gt; <span class="keyword">typename</span> W = fancy_wrapper&gt;</span><br><span class="line"><span class="keyword">class</span> wrapping_pair &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">wrapping_pair</span>(T <span class="type">const</span> a, U <span class="type">const</span> b) : <span class="built_in">item1</span>(a), <span class="built_in">item2</span>(b) &#123; &#125;</span><br><span class="line">	W&lt;T&gt; item1;</span><br><span class="line">  W&lt;U&gt; item2;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="默认模版参数"><a href="#默认模版参数" class="headerlink" title="默认模版参数"></a>默认模版参数</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>&gt; </span><br><span class="line"><span class="keyword">class</span> foo &#123; <span class="comment">/*...*/</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>, <span class="keyword">typename</span> U = <span class="type">double</span>&gt; </span><br><span class="line"><span class="keyword">class</span> bar &#123; <span class="comment">/*...*/</span> &#125;;</span><br></pre></td></tr></table></figure>



<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>, <span class="keyword">typename</span> U&gt; </span><br><span class="line"><span class="keyword">class</span> bar &#123; &#125;; <span class="comment">// error, 类模版带默认参数的参数，后面不能跟不带默认参数的参数。</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>, <span class="keyword">typename</span> U&gt; </span><br><span class="line"><span class="type">void</span> <span class="built_in">func</span>() &#123;&#125; <span class="comment">// OK</span></span><br></pre></td></tr></table></figure>



<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>&gt; </span><br><span class="line"><span class="keyword">struct</span> foo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="type">int</span>&gt; <span class="comment">// error redefinition // of default parameter</span></span><br><span class="line"><span class="keyword">struct</span> foo &#123;&#125;;</span><br></pre></td></tr></table></figure>





<h3 id="模版实例化"><a href="#模版实例化" class="headerlink" title="模版实例化"></a>模版实例化</h3><p>模版实例化可以是显式的也可以是隐式的。</p>
<h4 id="隐式实例化"><a href="#隐式实例化" class="headerlink" title="隐式实例化"></a>隐式实例化</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">foo</span> &#123;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	foo&lt;<span class="type">int</span>&gt; *x; <span class="comment">// 不会实例化</span></span><br><span class="line">  foo&lt;<span class="type">int</span>&gt; p;  <span class="comment">// 会</span></span><br><span class="line">  foo&lt;<span class="type">int</span>&gt; p1; <span class="comment">// 会</span></span><br><span class="line">  foo&lt;<span class="type">double</span>&gt; *q; <span class="comment">// 不会</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">foo</span> &#123;</span><br><span class="line">  <span class="type">static</span> T data;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; T foo&lt;T&gt;::data = <span class="number">0</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  foo&lt;<span class="type">int</span>&gt; a;</span><br><span class="line">  foo&lt;<span class="type">double</span>&gt; b;</span><br><span class="line">  foo&lt;<span class="type">double</span>&gt; c;</span><br><span class="line">  std::cout &lt;&lt; a.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 0 </span></span><br><span class="line">  std::cout &lt;&lt; b.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 0 </span></span><br><span class="line">  std::cout &lt;&lt; c.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 0</span></span><br><span class="line">  </span><br><span class="line">  b.data = <span class="number">42</span>;</span><br><span class="line">  </span><br><span class="line">  std::cout &lt;&lt; a.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 0 </span></span><br><span class="line">  std::cout &lt;&lt; b.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 42 </span></span><br><span class="line">  std::cout &lt;&lt; c.data &lt;&lt; <span class="string">&#x27;\n&#x27;</span>; <span class="comment">// 42</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="显示实例化"><a href="#显示实例化" class="headerlink" title="显示实例化"></a>显示实例化</h4><p>显示实例化分为显示实例化定义和显示实例化声明。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 类模版</span></span><br><span class="line"><span class="keyword">template</span> <span class="keyword">class</span>-key <span class="keyword">template</span>-name &lt;argument-list&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 函数模版</span></span><br><span class="line"><span class="keyword">template</span> <span class="keyword">return</span>-type <span class="built_in">name</span>&lt;argument-list&gt; (parameter-list);</span><br><span class="line"><span class="keyword">template</span> <span class="keyword">return</span>-type <span class="built_in">name</span>(parameter-list);</span><br></pre></td></tr></table></figure>



<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> ns &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">wrapper</span> &#123;</span><br><span class="line">    T value;</span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">template</span> <span class="keyword">struct</span> <span class="title class_">wrapper</span>&lt;<span class="type">int</span>&gt;; <span class="comment">// 显示实例化</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> <span class="keyword">struct</span> <span class="title class_">ns</span>::wrapper&lt;<span class="type">double</span>&gt;; <span class="comment">// 显示实例化</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> ns &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function">T <span class="title">add</span><span class="params">(T <span class="type">const</span> a, T <span class="type">const</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">template</span> <span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">template</span> <span class="type">double</span> <span class="title">ns::add</span><span class="params">(<span class="type">double</span>, <span class="type">double</span>)</span></span>;</span><br></pre></td></tr></table></figure>







<h3 id="模版特化"><a href="#模版特化" class="headerlink" title="模版特化"></a>模版特化</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_floatiog_point</span> &#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">static</span> <span class="type">bool</span> value = <span class="literal">false</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_floating_point</span>&lt;<span class="type">float</span>&gt; &#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">static</span> <span class="type">bool</span> value = <span class="literal">true</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_floating_point</span> &lt;<span class="type">double</span>&gt;&#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">static</span> <span class="type">bool</span> value = <span class="literal">true</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_floating_point</span> &lt;<span class="type">long</span> <span class="type">double</span>&gt; &#123;</span><br><span class="line">  <span class="keyword">constexpr</span> <span class="type">static</span> <span class="type">bool</span> value = <span class="literal">true</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="变量模版"><a href="#变量模版" class="headerlink" title="变量模版"></a>变量模版</h3><h3 id="别名模版"><a href="#别名模版" class="headerlink" title="别名模版"></a>别名模版</h3><h3 id="lambda-模版"><a href="#lambda-模版" class="headerlink" title="lambda 模版"></a>lambda 模版</h3>]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP协议深入理解</title>
    <url>/2023/01/31/1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/template/</url>
    <content><![CDATA[<span id="more"></span>







]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统之进程</title>
    <url>/2023/01/31/0-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<span id="more"></span>


]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title>go 接口原理</title>
    <url>/2023/01/31/4-go/%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8C%87%E9%92%88/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>go context 原理</title>
    <url>/2023/01/31/4-go/go%20context/</url>
    <content><![CDATA[<p>原文 <a href="https://go.dev/blog/context">https://go.dev/blog/context</a></p>
<p><a href="https://go.dev/blog/pipelines">https://go.dev/blog/pipelines</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在 Go 服务器中，每个传入的请求都在其自己的 goroutine 中处理。 </p>
<p>请求处理程序通常会启动额外的 goroutine 来访问后台，比如数据库和 RPC 服务等。</p>
<p> 处理请求的 goroutines 集通常需要访问特定于请求的值，例如最终用户的身份、授权令牌和请求的截止日期。</p>
<p> 当请求被取消或超时时，所有处理该请求的 goroutines 都应该快速退出，以便系统可以回收它们正在使用的任何资源。</p>
<span id="more"></span>

<p>Google开发了一个<code>context</code>包，可以轻松地将请求范围的值、取消信号和截止日期跨 API 边界传递给处理请求所涉及的所有 goroutine。context包公开可用 。本文介绍了如何使用该包并提供了一个完整的工作示例。</p>
<h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// A Context carries a deadline, cancellation signal, and request-scoped values</span></span><br><span class="line"><span class="comment">// across API boundaries. Its methods are safe for simultaneous use by multiple</span></span><br><span class="line"><span class="comment">// goroutines.</span></span><br><span class="line"><span class="keyword">type</span> Context <span class="keyword">interface</span> &#123;</span><br><span class="line">    <span class="comment">// Done returns a channel that is closed when this Context is canceled</span></span><br><span class="line">    <span class="comment">// or times out.</span></span><br><span class="line">    Done() &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Err indicates why this context was canceled, after the Done channel</span></span><br><span class="line">    <span class="comment">// is closed.</span></span><br><span class="line">    Err() <span class="type">error</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deadline returns the time when this Context will be canceled, if any.</span></span><br><span class="line">    Deadline() (deadline time.Time, ok <span class="type">bool</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Value returns the value associated with key or nil if none.</span></span><br><span class="line">    Value(key <span class="keyword">interface</span>&#123;&#125;) <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><code>Done</code>方法返回一个通道，该通道充当代表运行的函数的取消信号<code>Context</code>：当通道关闭时，函数应该放弃它们的工作并返回。<code>Err</code>方法返回一个错误，指示<code>Context</code>取消的原因。</p>
<p>Context 没有 Cancel 方法，原因与 Done 通道是仅接收的原因相同：接收取消信号的函数通常不是发送信号的函数。 特别是，当父操作为子操作启动 goroutine 时，这些子操作不应该能够取消父操作。 相反，WithCancel 函数（如下所述）提供了一种取消新 Context 值的方法。</p>
<p>一个 Context 对于多个 goroutine 同时使用是安全的。 代码可以将单个 Context 传递给任意数量的 goroutine，并取消该 Context 以向所有 goroutine 发出信号。</p>
<p>Deadline 方法允许函数确定它们是否应该开始工作； 如果剩下的时间太少，可能就不值得了。 代码也可以使用最后期限来设置 I&#x2F;O 操作的超时。</p>
<p>Value 允许上下文携带请求范围的数据。 该数据必须是安全的，以便多个 goroutine 同时使用。</p>
<h3 id="Derived-contexts"><a href="#Derived-contexts" class="headerlink" title="Derived contexts"></a>Derived contexts</h3><p>context 包提供了从现有值派生新 Context 值的函数。 这些值形成了一个树：当一个上下文被取消时，所有从它派生的上下文也被取消。</p>
<p>Background 是任何 Context 树的根； 它永远不会被取消：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Background returns an empty Context. It is never canceled, has no deadline,</span></span><br><span class="line"><span class="comment">// and has no values. Background is typically used in main, init, and tests,</span></span><br><span class="line"><span class="comment">// and as the top-level Context for incoming requests.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Background</span><span class="params">()</span></span> Context</span><br></pre></td></tr></table></figure>

<p>WithCancel 和 WithTimeout 返回派生的 Context 值，这些值可以比父 Context 更快地取消。 当请求处理程序返回时，通常会取消与传入请求关联的上下文。 WithCancel 对于在使用多个副本时取消冗余请求也很有用。 WithTimeout 对于设置对后端服务器的请求的截止日期很有用：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WithCancel returns a copy of parent whose Done channel is closed as soon as</span></span><br><span class="line"><span class="comment">// parent.Done is closed or cancel is called.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithCancel</span><span class="params">(parent Context)</span></span> (ctx Context, cancel CancelFunc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// A CancelFunc cancels a Context.</span></span><br><span class="line"><span class="keyword">type</span> CancelFunc <span class="function"><span class="keyword">func</span><span class="params">()</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// WithTimeout returns a copy of parent whose Done channel is closed as soon as</span></span><br><span class="line"><span class="comment">// parent.Done is closed, cancel is called, or timeout elapses. The new</span></span><br><span class="line"><span class="comment">// Context&#x27;s Deadline is the sooner of now+timeout and the parent&#x27;s deadline, if</span></span><br><span class="line"><span class="comment">// any. If the timer is still running, the cancel function releases its</span></span><br><span class="line"><span class="comment">// resources.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithTimeout</span><span class="params">(parent Context, timeout time.Duration)</span></span> (Context, CancelFunc)</span><br></pre></td></tr></table></figure>

<p>WithValue 提供了一种将请求范围的值与 Context 相关联的方法：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// WithValue returns a copy of parent whose Value method returns val for key.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithValue</span><span class="params">(parent Context, key <span class="keyword">interface</span>&#123;&#125;, val <span class="keyword">interface</span>&#123;&#125;)</span></span> Context</span><br></pre></td></tr></table></figure>

<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>我们的示例是一个 HTTP 服务器，它通过将查询“golang”转发到 Google Web Search API 并呈现结果来处理像 &#x2F;search?q&#x3D;golang&amp;timeout&#x3D;1s 这样的 URL。 timeout 参数告诉服务器在该持续时间过去后取消请求。</p>
<p>代码分为三个包：</p>
<p>server 为 &#x2F;search 提供主要功能和处理程序。<br>userip 提供了从请求中提取用户 IP 地址并将其与 Context 相关联的功能。<br>google 提供了用于向 Google 发送查询的搜索功能。</p>
<h3 id="The-server-program"><a href="#The-server-program" class="headerlink" title="The server program"></a>The server program</h3><p>server 通过为 golang 提供前几个 Google 搜索结果来处理像 &#x2F;search?q&#x3D;golang 这样的请求。 它注册 handleSearch 来处理 &#x2F;search 端点。 处理程序创建一个名为 ctx 的初始上下文，并安排在处理程序返回时取消它。 如果请求中包含 timeout URL 参数，则 Context 在超时后自动取消：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">handleSearch</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> &#123;</span><br><span class="line">    <span class="comment">// ctx is the Context for this handler. Calling cancel closes the</span></span><br><span class="line">    <span class="comment">// ctx.Done channel, which is the cancellation signal for requests</span></span><br><span class="line">    <span class="comment">// started by this handler.</span></span><br><span class="line">    <span class="keyword">var</span> (</span><br><span class="line">        ctx    context.Context</span><br><span class="line">        cancel context.CancelFunc</span><br><span class="line">    )</span><br><span class="line">    timeout, err := time.ParseDuration(req.FormValue(<span class="string">&quot;timeout&quot;</span>))</span><br><span class="line">    <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">// The request has a timeout, so create a context that is</span></span><br><span class="line">        <span class="comment">// canceled automatically when the timeout expires.</span></span><br><span class="line">        ctx, cancel = context.WithTimeout(context.Background(), timeout)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ctx, cancel = context.WithCancel(context.Background())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> cancel() <span class="comment">// Cancel ctx as soon as handleSearch returns.</span></span><br></pre></td></tr></table></figure>

<p>处理程序从请求中提取查询，并通过调用 userip 包提取客户端的 IP 地址。 后端请求需要客户端的 IP 地址，因此 handleSearch 将其附加到 ctx：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Check the search query.</span></span><br><span class="line">query := req.FormValue(<span class="string">&quot;q&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> query == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">    http.Error(w, <span class="string">&quot;no query&quot;</span>, http.StatusBadRequest)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Store the user IP in ctx for use by code in other packages.</span></span><br><span class="line">userIP, err := userip.FromRequest(req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    http.Error(w, err.Error(), http.StatusBadRequest)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">ctx = userip.NewContext(ctx, userIP)</span><br></pre></td></tr></table></figure>

<p>The handler calls <code>google.Search</code> with <code>ctx</code> and the <code>query</code>:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Run the Google search and print the results.</span></span><br><span class="line">start := time.Now()</span><br><span class="line">results, err := google.Search(ctx, query)</span><br><span class="line">elapsed := time.Since(start)</span><br></pre></td></tr></table></figure>

<p>If the search succeeds, the handler renders the results:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> err := resultsTemplate.Execute(w, <span class="keyword">struct</span> &#123;</span><br><span class="line">    Results          google.Results</span><br><span class="line">    Timeout, Elapsed time.Duration</span><br><span class="line">&#125;&#123;</span><br><span class="line">    Results: results,</span><br><span class="line">    Timeout: timeout,</span><br><span class="line">    Elapsed: elapsed,</span><br><span class="line">&#125;); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    log.Print(err)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Package-userip"><a href="#Package-userip" class="headerlink" title="Package userip"></a>Package userip</h3><p>userip 包提供了从请求中提取用户 IP 地址并将其与上下文相关联的功能。 上下文提供键值映射，其中键和值都是 interface{} 类型。 键类型必须支持相等，并且值必须安全地被多个 goroutine 同时使用。 像 userip 这样的包隐藏了这个映射的细节，并提供了对特定上下文值的强类型访问。</p>
<p>为了避免键冲突，userip 定义了一个未导出的类型键，并使用此类型的值作为上下文键：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// The key type is unexported to prevent collisions with context keys defined in</span></span><br><span class="line"><span class="comment">// other packages.</span></span><br><span class="line"><span class="keyword">type</span> key <span class="type">int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// userIPkey is the context key for the user IP address.  Its value of zero is</span></span><br><span class="line"><span class="comment">// arbitrary.  If this package defined other context keys, they would have</span></span><br><span class="line"><span class="comment">// different integer values.</span></span><br><span class="line"><span class="keyword">const</span> userIPKey key = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>FromRequest 从 http.Request 中提取 userIP 值：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">FromRequest</span><span class="params">(req *http.Request)</span></span> (net.IP, <span class="type">error</span>) &#123;</span><br><span class="line">    ip, _, err := net.SplitHostPort(req.RemoteAddr)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;userip: %q is not IP:port&quot;</span>, req.RemoteAddr)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>NewContext 返回一个带有提供的 userIP 值的新 Context：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewContext</span><span class="params">(ctx context.Context, userIP net.IP)</span></span> context.Context &#123;</span><br><span class="line">    <span class="keyword">return</span> context.WithValue(ctx, userIPKey, userIP)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FromContext 从 Context 中提取用户 IP：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">FromContext</span><span class="params">(ctx context.Context)</span></span> (net.IP, <span class="type">bool</span>) &#123;</span><br><span class="line">    <span class="comment">// ctx.Value returns nil if ctx has no value for the key;</span></span><br><span class="line">    <span class="comment">// the net.IP type assertion returns ok=false for nil.</span></span><br><span class="line">    userIP, ok := ctx.Value(userIPKey).(net.IP)</span><br><span class="line">    <span class="keyword">return</span> userIP, ok</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Package-google"><a href="#Package-google" class="headerlink" title="Package google"></a>Package google</h3><p>google.Search 函数向 Google Web Search API 发出 HTTP 请求并解析 JSON 编码的结果。 它接受 Context 参数 ctx 并在请求运行时如果 ctx.Done 关闭则立即返回。</p>
<p>Google Web Search API 请求包括搜索查询和用户 IP 作为查询参数：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Search</span><span class="params">(ctx context.Context, query <span class="type">string</span>)</span></span> (Results, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="comment">// Prepare the Google Search API request.</span></span><br><span class="line">    req, err := http.NewRequest(<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;https://ajax.googleapis.com/ajax/services/search/web?v=1.0&quot;</span>, <span class="literal">nil</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    q := req.URL.Query()</span><br><span class="line">    q.Set(<span class="string">&quot;q&quot;</span>, query)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If ctx is carrying the user IP address, forward it to the server.</span></span><br><span class="line">    <span class="comment">// Google APIs use the user IP to distinguish server-initiated requests</span></span><br><span class="line">    <span class="comment">// from end-user requests.</span></span><br><span class="line">    <span class="keyword">if</span> userIP, ok := userip.FromContext(ctx); ok &#123;</span><br><span class="line">        q.Set(<span class="string">&quot;userip&quot;</span>, userIP.String())</span><br><span class="line">    &#125;</span><br><span class="line">    req.URL.RawQuery = q.Encode()</span><br></pre></td></tr></table></figure>

<p>Search 使用辅助函数 httpDo 来发出 HTTP 请求，如果在处理请求或响应时 ctx.Done 关闭，则将其取消。 搜索传递一个闭包给 httpDo 处理 HTTP 响应：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> results Results</span><br><span class="line">err = httpDo(ctx, req, <span class="function"><span class="keyword">func</span><span class="params">(resp *http.Response, err <span class="type">error</span>)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Parse the JSON search result.</span></span><br><span class="line">    <span class="comment">// https://developers.google.com/web-search/docs/#fonje</span></span><br><span class="line">    <span class="keyword">var</span> data <span class="keyword">struct</span> &#123;</span><br><span class="line">        ResponseData <span class="keyword">struct</span> &#123;</span><br><span class="line">            Results []<span class="keyword">struct</span> &#123;</span><br><span class="line">                TitleNoFormatting <span class="type">string</span></span><br><span class="line">                URL               <span class="type">string</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err := json.NewDecoder(resp.Body).Decode(&amp;data); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> _, res := <span class="keyword">range</span> data.ResponseData.Results &#123;</span><br><span class="line">        results = <span class="built_in">append</span>(results, Result&#123;Title: res.TitleNoFormatting, URL: res.URL&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">// httpDo waits for the closure we provided to return, so it&#x27;s safe to</span></span><br><span class="line"><span class="comment">// read results here.</span></span><br><span class="line"><span class="keyword">return</span> results, err</span><br></pre></td></tr></table></figure>

<p>httpDo 函数运行 HTTP 请求并在新的 goroutine 中处理其响应。 如果 ctx.Done 在 goroutine 退出之前关闭，它将取消请求：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">httpDo</span><span class="params">(ctx context.Context, req *http.Request, f <span class="keyword">func</span>(*http.Response, <span class="type">error</span>)</span></span> <span class="type">error</span>) <span class="type">error</span> &#123;</span><br><span class="line">    <span class="comment">// Run the HTTP request in a goroutine and pass the response to f.</span></span><br><span class="line">    c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">error</span>, <span class="number">1</span>)</span><br><span class="line">    req = req.WithContext(ctx)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; c &lt;- f(http.DefaultClient.Do(req)) &#125;()</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">        &lt;-c <span class="comment">// Wait for f to return.</span></span><br><span class="line">        <span class="keyword">return</span> ctx.Err()</span><br><span class="line">    <span class="keyword">case</span> err := &lt;-c:</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Adapting-code-for-Contexts"><a href="#Adapting-code-for-Contexts" class="headerlink" title="Adapting code for Contexts"></a>Adapting code for Contexts</h2><p>许多服务器框架提供包和类型来承载请求范围的值。 我们可以定义 Context 接口的新实现，以在使用现有框架的代码和需要 Context 参数的代码之间架起一座桥梁。</p>
<p>例如，Gorilla 的 <a href="http://www.gorillatoolkit.org/pkg/context">github.com&#x2F;gorilla&#x2F;context</a>  包允许处理程序通过提供从 HTTP 请求到键值对的映射来将数据与传入请求相关联。 在 gorilla.go 中，我们提供了一个 Context 实现，其 Value 方法返回与 Gorilla 包中特定 HTTP 请求关联的值。</p>
<p>其他包提供了类似于 Context 的取消支持。 例如，Tomb 提供了一个 Kill 方法，该方法通过关闭 Dying 通道来发出取消信号。 Tomb 还提供了等待这些 goroutine 退出的方法，类似于 sync.WaitGroup。 在 tomb.go 中，我们提供了一个 Context 实现，当它的父 Context 被取消或提供的 Tomb 被杀死时，该实现被取消。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在 Google，我们要求 Go 程序员将 Context 参数作为第一个参数传递给传入和传出请求之间调用路径上的每个函数。 这使得许多不同团队开发的 Go 代码能够很好地互操作。 它提供了对超时和取消的简单控制，并确保安全凭证等关键值正确传输 Go 程序。</p>
<p>想要在 Context 上构建的服务器框架应该提供 Context 的实现，以便在它们的包和那些需要 Context 参数的包之间架起一座桥梁。 然后，他们的客户端库将接受来自调用代码的上下文。 通过为请求范围的数据和取消建立一个通用接口，Context 使包开发人员更容易共享代码以创建可扩展的服务。</p>
]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>go channel 用法及原理</title>
    <url>/2023/01/31/4-go/go%20channel/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="无缓冲"><a href="#无缓冲" class="headerlink" title="无缓冲"></a>无缓冲</h2><p>创建一个channel</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">chan1 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>)</span><br></pre></td></tr></table></figure>



<h2 id="有缓冲"><a href="#有缓冲" class="headerlink" title="有缓冲"></a>有缓冲</h2><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">chan2 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span> <span class="number">1</span>)</span><br></pre></td></tr></table></figure>



<span id="more"></span>

<h2 id="通道方向"><a href="#通道方向" class="headerlink" title="通道方向"></a>通道方向</h2><p>指定channel的方向</p>
<h2 id="通道选择器"><a href="#通道选择器" class="headerlink" title="通道选择器"></a>通道选择器</h2><p>select </p>
<h3 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h3><h3 id="非阻塞"><a href="#非阻塞" class="headerlink" title="非阻塞"></a>非阻塞</h3><p>default</p>
<h3 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h3><p>time</p>
<h2 id="通道关闭"><a href="#通道关闭" class="headerlink" title="通道关闭"></a>通道关闭</h2><p>close ，</p>
<p>可以接收关闭channel的数据，但是不能向关闭的channel发送数据</p>
<h2 id="通道遍历"><a href="#通道遍历" class="headerlink" title="通道遍历"></a>通道遍历</h2><p>for range 会阻塞</p>
<h2 id="Timer-和-ticker"><a href="#Timer-和-ticker" class="headerlink" title="Timer 和 ticker"></a>Timer 和 ticker</h2><p>timer 可以再到期之前结束</p>
<p>ticker 时隔一定时间执行一次。</p>
<h2 id="通道的实现"><a href="#通道的实现" class="headerlink" title="通道的实现"></a>通道的实现</h2>]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>go 闭包现象和原理</title>
    <url>/2023/01/31/4-go/go%20%E7%B3%BB%E5%88%97%E4%B9%8B%E9%97%AD%E5%8C%85/</url>
    <content><![CDATA[<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><h3 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h3><span id="more"></span>

<h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3>]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>学习方法</title>
    <url>/2023/06/09/6-%E9%97%B2%E8%B0%88/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>平时突然感悟到的学习方法。</p>
<p>解题：极端，全是，全不是。</p>
<span id="more"></span>







]]></content>
      <categories>
        <category>方法</category>
      </categories>
      <tags>
        <tag>灵感</tag>
      </tags>
  </entry>
  <entry>
    <title>乐理初级</title>
    <url>/2023/08/19/9-%E9%9F%B3%E4%B9%90/%E4%B9%90%E7%90%86%E5%88%9D%E7%BA%A7/</url>
    <content><![CDATA[<h1 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h1><p>初级乐理知识。</p>
<span id="more"></span>

<h2 id="音阶"><a href="#音阶" class="headerlink" title="音阶"></a>音阶</h2><h3 id="相邻两个音之间的关系"><a href="#相邻两个音之间的关系" class="headerlink" title="相邻两个音之间的关系"></a>相邻两个音之间的关系</h3><p>全音关系</p>
<p>半音关系</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1 2 3	(半音关系) 4 5 6 7 (半音关系) 1 </span><br><span class="line"><span class="comment"># 其余都是全音关系</span></span><br></pre></td></tr></table></figure>

<p>吉他上相邻两个品格之间是半音关系，间隔一个品位的为全音关系。</p>
<h3 id="不相邻两个音之间的关系"><a href="#不相邻两个音之间的关系" class="headerlink" title="不相邻两个音之间的关系"></a>不相邻两个音之间的关系</h3><p>音程（度）</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1 2 3 4 5 6 7</span><br><span class="line"><span class="comment"># 1 - 5 , 5 度音程关系</span></span><br></pre></td></tr></table></figure>

<h4 id="三度音程关系"><a href="#三度音程关系" class="headerlink" title="三度音程关系"></a>三度音程关系</h4><p>大三度（不包含半音关系）</p>
<p>小三度（含有半音关系）</p>
<h2 id="和弦"><a href="#和弦" class="headerlink" title="和弦"></a>和弦</h2><h3 id="构成"><a href="#构成" class="headerlink" title="构成"></a>构成</h3><p>有三个或三个以上有一定音程关系的音的组合</p>
<p>三度音叠加构成和谐</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 三度音叠加构成和谐  1 3 5 构成一个和弦</span></span><br><span class="line">1 2 3 4 5 6 7 </span><br></pre></td></tr></table></figure>

<h3 id="名称"><a href="#名称" class="headerlink" title="名称"></a>名称</h3><p>大三和弦构成： 一个大三度加一个小三度  </p>
<p><strong>色彩特点</strong> 明亮坚定</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1 3 5</span><br></pre></td></tr></table></figure>

<p>小三和弦构成： 一个小三度加一个大三度</p>
<p><strong>色彩特点</strong> 忧伤紧张</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">2 4 6</span><br></pre></td></tr></table></figure>



<h2 id="节奏"><a href="#节奏" class="headerlink" title="节奏"></a>节奏</h2><h3 id="拍-（音符的单位）"><a href="#拍-（音符的单位）" class="headerlink" title="拍 （音符的单位）"></a>拍 （音符的单位）</h3><p>记录音的长短 (注意不同歌曲都有自己的拍速，每首歌的拍速是固定的)</p>
<p>一下一上为一拍</p>
<h3 id="音符-（单位-拍）"><a href="#音符-（单位-拍）" class="headerlink" title="音符 （单位 拍）"></a>音符 （单位 拍）</h3><p>全音符        4 拍 x</p>
<p>二分音符    2 拍 </p>
<p>四分音符   1 拍</p>
<p>八分音符  半拍</p>
<h2 id="动手"><a href="#动手" class="headerlink" title="动手"></a>动手</h2><p>四拍em和弦</p>
<p>两拍</p>
<p>一拍</p>
<p>半拍 (下上)</p>
<h1 id="初级进阶"><a href="#初级进阶" class="headerlink" title="初级进阶"></a>初级进阶</h1><h2 id="音名和唱名"><a href="#音名和唱名" class="headerlink" title="音名和唱名"></a>音名和唱名</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">简谱  1   2   3   4   5   6   7</span><br><span class="line">唱名  Do  Re  Mi  Fa  Sol La  Si   名字  为了全世界统一</span><br><span class="line">音名  C   D   E   F   G   A   B    位置  多种乐器统一</span><br><span class="line"></span><br><span class="line">音名的位置不变</span><br><span class="line">唱名的位置会变化</span><br></pre></td></tr></table></figure>

<h2 id="调"><a href="#调" class="headerlink" title="调"></a>调</h2><p>一串音阶在乐器上不同的位置</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1 2 3 4 5 6 7</span><br><span class="line">1 主音决定调 </span><br><span class="line">1 在 C 这个位置，表名是 C 调</span><br></pre></td></tr></table></figure>

<h3 id="大调和小调"><a href="#大调和小调" class="headerlink" title="大调和小调"></a>大调和小调</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1  2  3  4  5  6  7  1   大调音阶              以 1 为主音度音阶</span><br><span class="line"> 全  全 半 全 全  全  半</span><br><span class="line">6  7  1  2  3  4  5  6   小调音阶(自然小调音阶)  以 6 为主音的音阶</span><br><span class="line"> 全  半 全 全 半  全 全</span><br></pre></td></tr></table></figure>

<p>大调音阶 ：明亮 坚定  欢快</p>
<p>小调音阶 ：忧郁 悲伤 不安</p>
<h3 id="关系大小调"><a href="#关系大小调" class="headerlink" title="关系大小调"></a>关系大小调</h3><p>自然调式当中 音的排列相同 音的位置也相同 使用的调号也相同</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">1 = C</span><br><span class="line">C D E F G A B  <span class="comment"># C 大调音阶</span></span><br><span class="line"></span><br><span class="line">A B C D E F G  <span class="comment"># A 小调音阶</span></span><br><span class="line"></span><br><span class="line">1 = G</span><br><span class="line">G A B C D E F  <span class="comment"># G 大调音阶</span></span><br><span class="line">1 2 3 4 5 6 7</span><br><span class="line">E F G A B C D  <span class="comment"># E 小调音阶</span></span><br><span class="line">6 7 1 2 3 4 5</span><br></pre></td></tr></table></figure>

<h2 id="和弦进阶"><a href="#和弦进阶" class="headerlink" title="和弦进阶"></a>和弦进阶</h2><h3 id="调内原位三和弦"><a href="#调内原位三和弦" class="headerlink" title="调内原位三和弦"></a>调内原位三和弦</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">C调</span><br><span class="line">音阶 1 2 3 4 5 6 7</span><br><span class="line">音名 C D E F G A B</span><br><span class="line">1 3 5 一级和弦 C</span><br><span class="line">2 4 6 二级和弦 Dm</span><br><span class="line">3 5 7 三级和弦 Em</span><br><span class="line">4 5 1 四级和弦 F</span><br><span class="line">5 7 2 五级和弦 G</span><br><span class="line">6 1 3 六级和弦 Am</span><br><span class="line">7 2 4 七级和弦 Bdim 减和弦，很少用</span><br><span class="line"><span class="comment"># 一个调内，七级和弦构成方式 ，所有调中和弦的大三度和小三度是固定的</span></span><br></pre></td></tr></table></figure>

<h2 id="转调"><a href="#转调" class="headerlink" title="转调"></a>转调</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># C 调</span></span><br><span class="line">级数：  1   2   3   4  5  6   7</span><br><span class="line">和弦：	 C   Dm  Em  F  G  Am  Bdim </span><br><span class="line"></span><br><span class="line"><span class="comment"># G 调</span></span><br><span class="line">级数：  1   2   3   4  5  6   7</span><br><span class="line">和弦：	 G   Am  Bm  C  D  Em  <span class="comment">#Fmdim </span></span><br><span class="line"><span class="comment"># C调转G调，转到对应的级数就可以， </span></span><br><span class="line"><span class="comment"># 可以快速记忆曲谱。只需要记住 调号和级数就可以</span></span><br></pre></td></tr></table></figure>

<h2 id="关系大小调和和弦的关系"><a href="#关系大小调和和弦的关系" class="headerlink" title="关系大小调和和弦的关系"></a>关系大小调和和弦的关系</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="节奏-1"><a href="#节奏-1" class="headerlink" title="节奏"></a>节奏</h2><p>拍号</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">6/8  一小节有六拍，以八分音符为一拍</span><br><span class="line"><span class="comment"># 强 弱 弱 次强 弱 弱</span></span><br><span class="line">4/4  一小节有四拍，以四分音符为一拍</span><br><span class="line"><span class="comment"># 强 弱 次强 弱</span></span><br><span class="line"><span class="comment"># 速度不同，轻重关系不同</span></span><br></pre></td></tr></table></figure>

<p>4&#x2F;4 拍的常用节奏型</p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>乐理</tag>
      </tags>
  </entry>
  <entry>
    <title>linux 基本分析工具</title>
    <url>/2023/01/31/8-linux/Linux%20%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<span id="more"></span>



]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>理解分布式 cap 理论</title>
    <url>/2023/01/31/7-%E5%88%86%E5%B8%83%E5%BC%8F/cap%20/</url>
    <content><![CDATA[<h3 id="CAP理论概述"><a href="#CAP理论概述" class="headerlink" title="CAP理论概述"></a>CAP理论概述</h3><p>一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。</p>
<span id="more"></span>



<h3 id="Consistency-一致性"><a href="#Consistency-一致性" class="headerlink" title="Consistency 一致性"></a>Consistency 一致性</h3><p>系统结果一致</p>
<h3 id="Availability-可用性"><a href="#Availability-可用性" class="headerlink" title="Availability 可用性"></a>Availability 可用性</h3><p>系统可用</p>
<h3 id="Partition-Tolerance分区容错性"><a href="#Partition-Tolerance分区容错性" class="headerlink" title="Partition Tolerance分区容错性"></a>Partition Tolerance分区容错性</h3><p>出现分区问题，仍然可提供服务</p>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式之抽象工厂模式</title>
    <url>/2023/01/31/5-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/01-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82/</url>
    <content><![CDATA[<span id="more"></span>







]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title>深入理解raft</title>
    <url>/2023/12/07/7-%E5%88%86%E5%B8%83%E5%BC%8F/raft/</url>
    <content><![CDATA[<p>Raft 是一种用来管理日志复制的一致性算法。</p>
<p>列举资料：</p>
<p>论文：<a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf</a></p>
<p>动画：<a href="https://thesecretlivesofdata.com/raft/">https://thesecretlivesofdata.com/raft/</a></p>
<p>go库：<a href="https://pkg.go.dev/go.etcd.io/etcd@v3.3.27+incompatible/raft">https://pkg.go.dev/go.etcd.io/etcd@v3.3.27+incompatible/raft</a></p>
<p>go版实现：<a href="https://github.com/etcd-io/raft">https://github.com/etcd-io/raft</a></p>
<p><a href="https://raft.github.io/">https://raft.github.io/</a></p>
<span id="more"></span>



<h2 id="Raft-问题分解"><a href="#Raft-问题分解" class="headerlink" title="Raft 问题分解"></a>Raft 问题分解</h2><ul>
<li>leader election</li>
<li>log replication</li>
<li>safety</li>
<li>membership changes</li>
</ul>
<h2 id="Raft-角色"><a href="#Raft-角色" class="headerlink" title="Raft 角色"></a>Raft 角色</h2><ul>
<li><p>leader</p>
<p>处理来自客户端的请求</p>
</li>
<li><p>candidate</p>
<p>用来选举一个新的 leader</p>
</li>
<li><p>follower</p>
<p>不发送任何请求，只响应 leader 和 candidate 发送的请求。</p>
</li>
</ul>
<h2 id="Raft-中的-RPC"><a href="#Raft-中的-RPC" class="headerlink" title="Raft 中的 RPC"></a>Raft 中的 RPC</h2><p><strong>RequestVote RPC</strong></p>
<p><strong>AppendEntries RPC</strong></p>
<p><strong>InstallSnapshot RPC</strong></p>
<h2 id="Leader-选举"><a href="#Leader-选举" class="headerlink" title="Leader 选举"></a>Leader 选举</h2><p>服务器刚启动的时候， 都是 follower。</p>
<h2 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h2><p>Raft guarantees that committed entries are durable and will eventually be exe- cuted by all of the available state machines.</p>
<p>A log entry is committed once the leader that created the entry has replicated it on a majority of the servers。</p>
<h2 id="Safety"><a href="#Safety" class="headerlink" title="Safety"></a>Safety</h2><p>Raft 使用了一种更简单的方式来保证 在新的领导人开始选举的时候 在之前任期的 所有已提交 的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。</p>
<p>Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries.</p>
<p>The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log, and the voter denies its vote if its own log is more up-to-date than that of the candidate.</p>
<p>Raft never commits log entries from previ- ous terms by counting replicas. Only log entries from the leader’s current term are committed by counting replicas; </p>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>高效表达</title>
    <url>/2023/06/09/6-%E9%97%B2%E8%B0%88/%E9%AB%98%E6%A0%A1%E8%A1%A8%E8%BE%BE/</url>
    <content><![CDATA[<p>我们在和别人沟通的时候，经常会遇到这种情况，自己觉得表达清楚了，可是对方却不明白你在说什么。</p>
<p>比如跟同事分享自己的设计方案，明明准备的非常好，别人却不理解。跟领导汇报工作，明明做了很多，领导确认为你没做多少。下面我们就来聊聊如何高效表达。用最简单的话表达最有价值的信息。</p>
<span id="more"></span>

<h2 id="定主题"><a href="#定主题" class="headerlink" title="定主题"></a>定主题</h2><p>高效表达的第一步就是先说观点。</p>
<p>第一用一句话先表达你的观点。</p>
<p>第二直接表达，不要绕弯子。</p>
]]></content>
      <categories>
        <category>方法</category>
      </categories>
      <tags>
        <tag>表达</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式之设计原则</title>
    <url>/2023/01/31/5-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/00-%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h2 id="面向对象设计原则"><a href="#面向对象设计原则" class="headerlink" title="面向对象设计原则"></a>面向对象设计原则</h2><h3 id="依赖倒置原则（DIP）"><a href="#依赖倒置原则（DIP）" class="headerlink" title="依赖倒置原则（DIP）"></a>依赖倒置原则（DIP）</h3><ul>
<li><p>高层模块（稳定）不应该依赖底层模块（变化），二者都应该依赖于抽象（稳定）。</p>
</li>
<li><p>抽象（稳定）不应该依赖于实现细节（变化），实现细节应该依赖于抽象（稳定）。</p>
</li>
</ul>
<h3 id="开放封闭原则（OCP）"><a href="#开放封闭原则（OCP）" class="headerlink" title="开放封闭原则（OCP）"></a>开放封闭原则（OCP）</h3><ul>
<li><p>对拓展开放，对更改封闭。</p>
</li>
<li><p>类模块应该是可拓展的，但是不可修改。</p>
</li>
</ul>
<h3 id="单一职责原则（SRP）"><a href="#单一职责原则（SRP）" class="headerlink" title="单一职责原则（SRP）"></a>单一职责原则（SRP）</h3><ul>
<li>一个类应该仅有一个引起它变化的原因。</li>
<li>变化的方向隐含着类的责任。</li>
</ul>
<h3 id="Liskov-替换原则（LSP）"><a href="#Liskov-替换原则（LSP）" class="headerlink" title="Liskov 替换原则（LSP）"></a>Liskov 替换原则（LSP）</h3><ul>
<li>子类必须能够体会他们的基类（IS-A）。</li>
<li>继承表达类型抽象。</li>
</ul>
<h3 id="接口隔离原则（ISP）"><a href="#接口隔离原则（ISP）" class="headerlink" title="接口隔离原则（ISP）"></a>接口隔离原则（ISP）</h3><ul>
<li><p>不应该强迫客户程序依赖他们不用的方法</p>
</li>
<li><p>接口应该小而完备</p>
</li>
</ul>
<h3 id="优先使用对象组合，而不是类继承"><a href="#优先使用对象组合，而不是类继承" class="headerlink" title="优先使用对象组合，而不是类继承"></a>优先使用对象组合，而不是类继承</h3><ul>
<li>类继承通常为 “白箱复用”，对象组合通常为 “黑箱复用”。</li>
<li>继承在某种程度上破坏了封装性，子类父类耦合性高。</li>
</ul>
<h3 id="封装变化点"><a href="#封装变化点" class="headerlink" title="封装变化点"></a>封装变化点</h3><span id="more"></span>



<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>创建型</p>
<p>结构型</p>
<p>行为型</p>
<h3 id="针对接口编程，而不是针对实现编程"><a href="#针对接口编程，而不是针对实现编程" class="headerlink" title="针对接口编程，而不是针对实现编程"></a>针对接口编程，而不是针对实现编程</h3><h2 id="模版方法"><a href="#模版方法" class="headerlink" title="模版方法"></a>模版方法</h2><p>library 提供需需函数，application 实现library 的虚函数，librayr 提供程序的流程。</p>
<p>晚绑定 代替 早绑定。   早-&gt;晚。</p>
<h2 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h2><p>支持多种税收的计算，</p>
<p>枚举，if-else。 设计抽象类，和子类。</p>
<h2 id="观察者模式-x2F-event"><a href="#观察者模式-x2F-event" class="headerlink" title="观察者模式&#x2F; event"></a>观察者模式&#x2F; event</h2><p>文件分割器，进度条的显示。</p>
<h2 id="桥模式"><a href="#桥模式" class="headerlink" title="桥模式"></a>桥模式</h2>]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title>设计模式之简单工厂模式</title>
    <url>/2023/01/31/5-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/02-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82/</url>
    <content><![CDATA[<span id="more"></span>







]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title>安装软件加速</title>
    <url>/2023/01/31/%E8%AE%BA%E6%96%87/0-brew/</url>
    <content><![CDATA[<p>安装软件加速</p>
<span id="more"></span>



<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">ALL_PROXY=socks5://127.0.0.1:1086 brew update</span><br><span class="line"></span><br><span class="line">brew upgrade go</span><br><span class="line"></span><br><span class="line">ALL_PROXY=socks5://127.0.0.1:1086 brew install xxx</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/02/13/5-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<h2 id="面向对象设计原则"><a href="#面向对象设计原则" class="headerlink" title="面向对象设计原则"></a>面向对象设计原则</h2><h3 id="依赖倒置原则（DIP）"><a href="#依赖倒置原则（DIP）" class="headerlink" title="依赖倒置原则（DIP）"></a>依赖倒置原则（DIP）</h3><ul>
<li><p>高层模块（稳定）不应该依赖底层模块（变化），二者都应该依赖于抽象（稳定）。</p>
</li>
<li><p>抽象（稳定）不应该依赖于实现细节（变化），实现细节应该依赖于抽象（稳定）。</p>
</li>
</ul>
<h3 id="开放封闭原则（OCP）"><a href="#开放封闭原则（OCP）" class="headerlink" title="开放封闭原则（OCP）"></a>开放封闭原则（OCP）</h3><ul>
<li><p>对拓展开放，对更改封闭。</p>
</li>
<li><p>类模块应该是可拓展的，但是不可修改。</p>
</li>
</ul>
<h3 id="单一职责原则（SRP）"><a href="#单一职责原则（SRP）" class="headerlink" title="单一职责原则（SRP）"></a>单一职责原则（SRP）</h3><ul>
<li>一个类应该仅有一个引起它变化的原因。</li>
<li>变化的方向隐含着类的责任。</li>
</ul>
<h3 id="Liskov-替换原则（LSP）"><a href="#Liskov-替换原则（LSP）" class="headerlink" title="Liskov 替换原则（LSP）"></a>Liskov 替换原则（LSP）</h3><ul>
<li>子类必须能够体会他们的基类（IS-A）。</li>
<li>继承表达类型抽象。</li>
</ul>
<h3 id="接口隔离原则（ISP）"><a href="#接口隔离原则（ISP）" class="headerlink" title="接口隔离原则（ISP）"></a>接口隔离原则（ISP）</h3><ul>
<li><p>不应该强迫客户程序依赖他们不用的方法</p>
</li>
<li><p>接口应该小而完备</p>
</li>
</ul>
<h3 id="优先使用对象组合，而不是类继承"><a href="#优先使用对象组合，而不是类继承" class="headerlink" title="优先使用对象组合，而不是类继承"></a>优先使用对象组合，而不是类继承</h3><ul>
<li>类继承通常为 “白箱复用”，对象组合通常为 “黑箱复用”。</li>
<li>继承在某种程度上破坏了封装性，子类父类耦合性高。</li>
</ul>
<h3 id="封装变化点"><a href="#封装变化点" class="headerlink" title="封装变化点"></a>封装变化点</h3><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>创建型</p>
<p>结构型</p>
<p>行为型</p>
<h3 id="针对接口编程，而不是针对实现编程"><a href="#针对接口编程，而不是针对实现编程" class="headerlink" title="针对接口编程，而不是针对实现编程"></a>针对接口编程，而不是针对实现编程</h3><h2 id="模版方法"><a href="#模版方法" class="headerlink" title="模版方法"></a>模版方法</h2><p>library 提供需需函数，application 实现library 的虚函数，librayr 提供程序的流程。</p>
<p>晚绑定 代替 早绑定。   早-&gt;晚。</p>
<h2 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h2><p>支持多种税收的计算，</p>
<p>枚举，if-else。 设计抽象类，和子类。</p>
<h2 id="观察者模式-x2F-event"><a href="#观察者模式-x2F-event" class="headerlink" title="观察者模式&#x2F; event"></a>观察者模式&#x2F; event</h2><p>文件分割器，进度条的显示。</p>
<h2 id="桥模式"><a href="#桥模式" class="headerlink" title="桥模式"></a>桥模式</h2>]]></content>
  </entry>
  <entry>
    <title>CockRoackDB 概述</title>
    <url>/2023/01/31/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockorachdb%20%E4%B9%8B%2001-%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>CockroachDB 设计的两个目标：可拓展和一致性。开发者经常会有疑问我们是如何做到的。这篇文章详细解释CockroachDB的内部工作原理。对于使用者来说，不需要了解底层架构，所以此文是为了那些想要了解底层的用户。</p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>CockroachDB 设计目标：</p>
<ul>
<li>使人们的生活更轻松，这意味着对于用户来说高度自动化，对于开发者来说更简单。</li>
<li>提供行业领先的一致性，即使在大规模部署当中也是，这意味着使用分布式事务，以及消除最终一致性问题和 过期读的问题。</li>
<li>创建一个始终在线的数据库，该数据库所有节点接受读和写，而不产生冲突。</li>
<li>允许在任何平台中部署，不需要绑定平台和供应商。</li>
<li>支持处理关系数据的工具，比如SQL。</li>
</ul>
<p>通过这些特性的融合，我们希望 CockroachDB 帮助您构建全球性、可扩展、弹性的部署和应用程序。</p>
<p>在阅读我们的架构文档之前了解一些术语会很有帮助。</p>
<span id="more"></span>

<h2 id="数据库术语"><a href="#数据库术语" class="headerlink" title="数据库术语"></a>数据库术语</h2><table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody><tr>
<td>Consistency</td>
<td>事务必须仅以允许的方式更改受影响数据的要求。 CockroachDB 在 ACID 语义和 CAP 定理的意义上都使用了“一致性”，尽管没有任何一个定义那么正式。</td>
</tr>
<tr>
<td>isolation</td>
<td>一个事务可能受同时运行的其他事务影响的程度。 CockroachDB 提供了 SERIALIZABLE 隔离级别，这是可能的最高级别，并保证每个提交的事务具有相同的结果，就好像每个事务一次运行一个一样。</td>
</tr>
<tr>
<td>Consensus</td>
<td>就事务是提交还是中止达成一致的过程。 CockroachDB 使用 Raft 共识协议。 在 CockroachDB 中，当一个范围接收到写入时，包含该范围副本的节点的法定人数会确认写入。 这意味着您的数据得到安全存储，并且大多数节点都同意数据库的当前状态，即使某些节点处于脱机状态。<br/><br/>当写入未达成共识时，前进进程停止以保持集群内的一致性。</td>
</tr>
<tr>
<td>Replication</td>
<td>创建和分发数据副本以及确保这些副本保持一致的过程。 CockroachDB 要求所有写入在被视为已提交之前传播到数据副本的法定人数。 这确保了数据的一致性。</td>
</tr>
<tr>
<td>transation</td>
<td>在数据库上执行的一组满足 ACID 语义要求的操作。 这是确保开发人员可以信任其数据库中的数据的一致系统的关键特性。 有关 CockroachDB 中事务如何工作的更多信息，请参阅事务层。</td>
</tr>
<tr>
<td>Multi-active availability</td>
<td>一种基于共识的高可用性概念，允许集群中的每个节点处理存储数据子集的读取和写入（基于每个范围）。 这与主动-被动复制（主动节点接收 100% 的请求流量）和主动-主动复制（所有节点都接受请求但通常不能保证读取是最新的和快速的）形成对比。</td>
</tr>
</tbody></table>
<h2 id="CockroachDB-架构术语"><a href="#CockroachDB-架构术语" class="headerlink" title="CockroachDB 架构术语"></a>CockroachDB 架构术语</h2><table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody><tr>
<td>cluster</td>
<td>一组相互连接的存储节点，它们协作组织事务、容错和数据重新平衡。</td>
</tr>
<tr>
<td>node</td>
<td>CockroachDB 的单个实例。 一个或多个节点形成一个集群。</td>
</tr>
<tr>
<td>range</td>
<td>CockroachDB 将所有用户数据（表、索引等）和几乎所有系统数据存储在键值对的排序映射中。 这个键空间被分成称为范围的连续块，这样每个键都可以在一个范围内找到。<br/><br/>从 SQL 的角度来看，表及其二级索引最初映射到单个范围，其中范围中的每个键值对代表表中的单个行（也称为主索引，因为表是按主键排序的） 或二级索引中的单行。 一旦范围的大小达到 512 MiB（默认值），它就会被分成两个范围。 随着表及其索引的不断增长，这些新范围的过程将继续进行。</td>
</tr>
<tr>
<td>Replica</td>
<td>存储在节点上的范围的副本。 默认情况下，每个范围在不同节点上有三个副本。</td>
</tr>
<tr>
<td>Leasholder</td>
<td>持有“范围租约”的副本。 此副本接收并协调该范围的所有读取和写入请求。<br/><br/>对于大多数类型的表和查询，租用者是唯一可以提供一致读取（返回“最新”数据的读取）的副本。</td>
</tr>
<tr>
<td>raft protocol</td>
<td>CockreactDB中采用的共识协议可确保您的数据安全存储在多个节点上，并且这些节点即使其中一些暂时断开连接，也可以同意当前状态。</td>
</tr>
<tr>
<td>Raft leader</td>
<td>对于每个范围，是写作请求的“领导者”的副本。 领导者使用 raft 协议来确保大多数复本（领导者和足够的追随者）根据其 raft 日志达成一致，然后再进行写作。 raft 领导人几乎总是与 leaseholder 是相同的复本。</td>
</tr>
<tr>
<td>Raft log</td>
<td>按时间订购的日志写入其复制品已达成协议的范围。 该日志与每个副本一起存在，并且是一致复制的真实范围的来源。</td>
</tr>
</tbody></table>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CockroachDB 在机器上启动使用下面两条命令。</p>
<ul>
<li>cockroach  start 启动集群中所有节点使用 –jion 标记 ，因此该过程知道它可以与之通信的所有其他机器。</li>
<li>cockroach init执行初始化集群。</li>
</ul>
<p>一旦初始化了CockacredDB群集，开发人员就会通过兼容 postgresql 的SQL API与CockreactDB相互作用。 得益于群集中所有节点的对称行为，您可以将SQL请求发送到任何节点； 这使蟑螂易于与负载平衡器集成。</p>
<p>接收SQL远程过程调用（RPC）后，节点将它们转换为 kv 操作 在分布式事务的存储中。</p>
<p>当这些RPC开始用数据填充群集时，CockroachdB开始算法将您的数据分配在群集的节点之间，将数据分解为我们称为 ragne 的512个MIB块。 默认情况下，每个range 至少复制至至少3个节点，以确保生存能力。 这样可以确保如果有任何节点下降，您仍然拥有可用于以下数据的数据的副本：</p>
<ul>
<li>继续提供读写。</li>
<li>持续负责数据到其他节点。</li>
</ul>
<p>如果一个节点接收到读或写请求无法直接处理，它会找到可以处理该请求的节点，并与该节点通信。这意味着您不需要知道在群集中存储了数据的特定部分；CockroachDB 为您跟踪它，并启用每个节点的对称读&#x2F;写行为。</p>
<p>对数据范围内对数据进行的任何更改都取决于共识算法，以确保大多数范围的复制品同意进行更改。 这就是蟑螂实现行业领先的隔离的方式，可以使其能够为您的应用提供一致的读取和写入，而不管您与哪种节点进行了交流。</p>
<p>最终，使用有效的存储引擎将数据写入并从磁盘上读取，该引擎能够跟踪数据的时间戳。 这是一个好处，可以让我们支持AS OF SYSTEM TIME 的SQL标准，从而让您在一段时间内找到历史数据。</p>
<h2 id="五层"><a href="#五层" class="headerlink" title="五层"></a>五层</h2><table>
<thead>
<tr>
<th>Layer</th>
<th>Order</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td>SQL</td>
<td>1</td>
<td>转换 sql 到 kv 操作</td>
</tr>
<tr>
<td>Transitional</td>
<td>2</td>
<td>允许原子修改 多个kv 操作</td>
</tr>
<tr>
<td>Distribution</td>
<td>3</td>
<td>复制 kv range</td>
</tr>
<tr>
<td>Replication</td>
<td>4</td>
<td>一致和同步复制KV范围在许多节点上范围。 该层还可以使用共识算法进行一致的读取</td>
</tr>
<tr>
<td>Storage</td>
<td>5</td>
<td>读写 kv 在磁盘</td>
</tr>
</tbody></table>
<h1 id="SQL-层"><a href="#SQL-层" class="headerlink" title="SQL 层"></a>SQL 层</h1><p>sql 层给 开发者提供 SQL API 并且将 sql 语句转换为 读写请求到 key-velue store。并且通过事务层。</p>
<p>包含以下子层：</p>
<ul>
<li>sql api，用户的接口</li>
<li>Parser， 将sql 语句转换为 AST树</li>
<li>Cost-based optimizer，将 AST树转换为被优化的逻辑计划</li>
<li>Physical planner 将逻辑计划转换为物理计划，给集群中一个或者多个节点执行</li>
<li>SQL execution engine，执行物理计划通过创建 读写请求给底层的 key-value store</li>
</ul>
<h2 id="概览-1"><a href="#概览-1" class="headerlink" title="概览"></a>概览</h2><p>一旦CockroachDB被部署成功，开发者们需要一个连接串连接到集群，他们可以开始工作使用 SQL。</p>
<p>因为CockroachDB集群中每个节点上对等的，开发者可以从任何一个节点发送请求。接受请求的节点称为”gateway node”，执行请求并且响应客户端。</p>
<p>对集群的请求以 SQL 语句的形式到达，但数据最终以键值 (KV) 对的形式写入和读取存储层。 为了处理这个问题，SQL 层将 SQL 语句转换为 KV 操作计划，然后将其传递给事务层。</p>
<h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="关系结构"><a href="#关系结构" class="headerlink" title="关系结构"></a>关系结构</h3><p>开发人员将存储在 CockroachDB 中的数据视为由行和列组成的关系结构。 行和列的集合被进一步组织成表格。 然后将表的集合组织到数据库中。 CockroachDB 集群可以包含许多数据库。</p>
<p>CockroachDB 提供了典型的关系特征，如约束（例如外键）。 这些特性意味着应用程序开发人员可以相信数据库将确保应用程序数据的结构一致； 数据验证不需要单独构建到应用程序逻辑中。</p>
<h3 id="SQL-API"><a href="#SQL-API" class="headerlink" title="SQL API"></a>SQL API</h3><p>CockroachDB 实现了大部分 ANSI SQL 标准以显示其关系结构。</p>
<p>重要的是，通过 SQL API，开发人员可以像通过任何 SQL 数据库（使用 BEGIN、COMMIT 等）一样访问 ACID 语义事务。</p>
<h3 id="PostgreSQL-wire-protocol"><a href="#PostgreSQL-wire-protocol" class="headerlink" title="PostgreSQL wire protocol"></a>PostgreSQL wire protocol</h3><p>SQL 查询通过 PostgreSQL 协议访问集群。 </p>
<h3 id="SQL-parser-planner-executor"><a href="#SQL-parser-planner-executor" class="headerlink" title="SQL parser, planner, executor"></a>SQL parser, planner, executor</h3><p>当 CockroachDB 集群中的节点接收到来自客户端的 SQL 请求时，它会解析语句并创建优化的逻辑查询计划，该计划会进一步转换为物理查询计划。 最后，执行物理计划。</p>
<h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p>SQL 查询根据我们的 yacc 文件（描述我们支持的语法）进行解析，每个查询的 SQL 版本都被转换为抽象语法树 (AST)。</p>
<h4 id="Logical-planning"><a href="#Logical-planning" class="headerlink" title="Logical planning"></a>Logical planning</h4><p>在逻辑计划阶段，AST 树被转换为一个查询计划通过以下步骤。</p>
<ol>
<li>AST 被转换为高级逻辑查询计划。 在此转换过程中，CockroachDB 还执行语义分析，其中包括以下操作：</li>
</ol>
<ul>
<li>检查查询是否是 SQL 语言中的有效语句。</li>
<li>将名称（例如表或变量的名称）解析为其值。</li>
<li>消除不需要的中间计算，例如，将 0.6 + 0.4 替换为 1.0。 这也称为常量折叠。</li>
<li>最终确定用于中间结果的数据类型，例如，当查询包含一个或多个子查询时。</li>
</ul>
<ol start="2">
<li>使用一系列始终有效的转换简化了逻辑计划。 例如，a BETWEEN b AND c 可以转换为 a &gt;&#x3D; b AND a &lt;&#x3D; c。</li>
<li>使用搜索算法优化逻辑计划，该算法评估执行查询的许多可能方式并选择成本最低的执行计划。</li>
</ol>
<p>上述最后一步的结果是优化的逻辑计划。 要查看基于成本的优化器生成的逻辑计划，请使用 EXPLAIN (OPT) 语句。</p>
<h4 id="Physical-planning"><a href="#Physical-planning" class="headerlink" title="Physical planning"></a>Physical planning</h4><p>物理计划阶段根据 range 的位置信息决定那个节点参与查询的执行。这就是CockroachDB 决定分布式执行在靠近存储位置的地方。</p>
<p>更具体地说，物理计划阶段将 逻辑计划期间生成 的 逻辑计划 转换为物理 SQL 运算符的有向无环图 (DAG)。 可以通过运行 EXPLAIN(DISTSQL) 语句查看这些运算符。</p>
<p>关于是否在多个节点上分发查询的决定是由一种启发式方法做出的，该方法估计需要通过网络发送的数据量。 只需要少量行的查询在网关节点上执行。 其他查询分布在多个节点上。</p>
<p>例如，当一个查询被分发时，物理计划阶段将扫描操作从逻辑计划拆分为多个物理 TableReader 操作符，每个节点一个包含扫描读取的范围。 然后将剩余的逻辑操作（可能执行过滤器、连接和聚合）安排在与 TableReader 相同的节点上。 这导致执行的计算尽可能接近物理数据。</p>
<h4 id="Query-execution"><a href="#Query-execution" class="headerlink" title="Query execution"></a>Query execution</h4><p>物理计划的组成部分被发送到一个或多个节点执行。 在每个节点上，CockroachDB 生成一个逻辑处理器来计算查询的一部分。 节点内部或跨节点的逻辑处理器通过逻辑数据流相互通信。 查询的组合结果被发送回接收查询的第一个节点，以进一步发送到 SQL 客户端。</p>
<p>每个处理器对查询操作的标量值使用编码形式。 这是一种二进制形式，不同于 SQL 中使用的形式。 因此，SQL 查询中列出的值必须进行编码，并且在逻辑处理器之间通信以及从磁盘读取的数据必须在将其发送回 SQL 客户端之前进行解码。</p>
<h4 id="Vectorized-query-execution"><a href="#Vectorized-query-execution" class="headerlink" title="Vectorized query execution"></a>Vectorized query execution</h4><p>如果启用了向量化执行，则将物理计划发送到节点以由向量化执行引擎处理。</p>
<p>向量化引擎收到物理计划后，从磁盘中批量读取表数据，并将数据从行格式转换为列格式。 这些批次的列数据存储在内存中，因此引擎可以在执行过程中快速访问它们。</p>
<p>矢量化引擎使用专门的预编译函数，可以快速迭代特定类型的列数据数组。 当引擎处理每列数据时，函数的列输出存储在内存中。</p>
<p>在处理完输入缓冲区中的所有列数据后，引擎将列输出转换回行格式，然后将处理后的行返回给 SQL 接口。 当一批表数据处理完毕后，引擎会读取下一批表数据进行处理，直到查询执行完毕。</p>
<h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><p>尽管 SQL 查询是用可解析的字符串编写的，但 CockroachDB 的较低层主要以字节为单位处理。 这意味着在 SQL 层，在查询执行中，CockroachDB 必须将行数据从它们的 SQL 表示形式转换为字节，并将从较低层返回的字节转换为可以传递回客户端的 SQL 数据。</p>
<p>同样重要的是——对于索引列——这种字节编码保持与它所代表的数据类型相同的排序顺序。 这是因为 CockroachDB 最终将数据存储在排序的键值映射中的方式； 以与其所代表的数据相同的顺序存储字节使我们能够有效地扫描 KV 数据。</p>
<p>然而，对于非索引列（例如，非 PRIMARY KEY 列），CockroachDB 改为使用占用较少空间但不保留排序的编码（称为“值编码”）。</p>
<h3 id="DistSQL"><a href="#DistSQL" class="headerlink" title="DistSQL"></a>DistSQL</h3><p>因为 CockroachDB 是一个分布式数据库，所以我们为一些查询开发了一个分布式 SQL（DistSQL）优化工具，它可以显着加快涉及多个范围的查询。 尽管 DistSQL 的体系结构值得拥有它自己的文档，但这个粗略的解释可以提供一些关于它是如何工作的洞察力。</p>
<p>在非分布式查询中，协调节点接收与其查询匹配的所有行，然后对整个数据集执行任何计算。</p>
<p>但是，对于与 DistSQL 兼容的查询，每个节点都会对其包含的行进行计算，然后将结果（而不是整个行）发送到协调节点。 协调节点然后聚合来自每个节点的结果，最后向客户端返回单个响应。</p>
<p>这大大减少了带到协调节点的数据量，并利用了经过充分验证的并行计算概念，最终减少了完成复杂查询所需的时间。 此外，这会在已经存储数据的节点上处理数据，这让 CockroachDB 可以处理大于单个节点存储的行集。</p>
<p>为了以分布式方式运行 SQL 语句，我们引入了几个概念：</p>
<ul>
<li>逻辑计划：类似于上面描述的 AST&#x2F;planNode 树，它表示通过计算阶段的抽象（非分布式）数据流。</li>
<li>物理计划：物理计划在概念上是逻辑计划节点到运行 cockroach 的物理机器的映射。 逻辑计划节点根据集群拓扑进行复制和专门化。 与上面的 planNodes 一样，物理计划的这些组件是在集群上调度和运行的。</li>
</ul>
<h2 id="Schema-changes"><a href="#Schema-changes" class="headerlink" title="Schema changes"></a>Schema changes</h2><p>CockroachDB 使用允许表在模式更改期间保持在线（即能够提供读取和写入服务）的协议来执行模式更改，例如添加列或二级索引。 该协议允许集群中的不同节点在不同时间异步转换到新的表模式。</p>
<p>模式更改协议将每个模式更改分解为一系列增量更改，以达到预期的效果。</p>
<p>例如，添加二级索引需要在开始版本和结束版本之间有两个中间架构版本，以确保索引在整个集群中的写入时更新，然后才可用于读取。 为了确保数据库在整个模式更改过程中保持一致状态，我们强制执行不变量，即在集群中始终使用最多两个连续版本的该模式。</p>
<h1 id="事务层"><a href="#事务层" class="headerlink" title="事务层"></a>事务层</h1><p>事务层实现对事务 ACID 的支持通过协调当前的操作。</p>
<h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>CockroachDB 认为一致性是数据库最重要的特征。没有他，开发者不能构建可靠的工具，企业将遭受潜在的微妙的难以发现的异常。</p>
<p>为了提供一致性，CockroachDB实现了完整的 ACID事务语法在事务层。然而，最重要的是，所有的语句都是作为事务来处理的，包括单条语句，这有时被称为“autocommit mode”，因为它的行为就像每条语句后面都有一个 COMMIT。</p>
<p>因为 CockroachDB 支持跨整个集群的事务（包括跨范围和跨表事务），所以它使用称为 Parallel Commits 的分布式原子提交协议来实现正确性。</p>
<h3 id="Writes-and-reads-phase-1"><a href="#Writes-and-reads-phase-1" class="headerlink" title="Writes and reads (phase 1)"></a>Writes and reads (phase 1)</h3><h4 id="Writing"><a href="#Writing" class="headerlink" title="Writing"></a>Writing</h4><p>当事务层执行写操作的时候，他不直接将值写到磁盘中，他会创建以下几种事情帮助协调分布式事务：</p>
<ul>
<li><p><strong>Locks</strong> 所有事务的写，表示临时的、未提交的状态。 CockroachDB 有几种不同类型的锁：</p>
<ul>
<li><p><strong>Unreplicated Locks</strong> are stored in an in-memory, per-node lock table by the <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer.html#concurrency-control">concurrency control</a> machinery. These locks are not replicated via <a href="https://www.cockroachlabs.com/docs/v21.2/architecture/replication-layer#raft">Raft</a>.</p>
</li>
<li><p><strong>Replicated Locks</strong> (also known as <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer.html#write-intents">write intents</a>) are replicated via <a href="https://www.cockroachlabs.com/docs/v21.2/architecture/replication-layer#raft">Raft</a>, and act as a combination of a provisional value and an exclusive lock. They are essentially the same as standard <a href="https://www.cockroachlabs.com/docs/v21.2/architecture/storage-layer#mvcc">multi-version concurrency control (MVCC)</a> values but also contain a pointer to the <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer.html#transaction-records">transaction record</a> stored on the cluster.</p>
</li>
</ul>
</li>
<li><p>A <strong>transaction record</strong> stored in the range where the first write occurs, which includes the transaction’s current state (which is either <code>PENDING</code>, <code>STAGING</code>, <code>COMMITTED</code>, or <code>ABORTED</code>).</p>
</li>
</ul>
<h4 id="Reading"><a href="#Reading" class="headerlink" title="Reading"></a>Reading</h4><p>如果事务没有被终止，事务层执行读操作。如果一个读操作遇到了标准的 MVCC 值，一切都很好。但是如果遇到了 写意向，操作必须解决事务冲突。</p>
<p>CockroachDB 提供两种类型的读：</p>
<ul>
<li>强一致性读：这是默认的也是最常用的读。这些读通过 leaseholder 并且可以看到读事务开始之前的所有写。他们总是返回正确最新的数据。</li>
<li>stale reads：这是有用的在你想要获取更快的读，而不是更新的数据的时候。他们可以用只读事务使用 <code>AS OF SYSTEM TIME</code> 语句。他们不需要通过 leaseholder ，因为他们可以确保一致性读通过读取不高于 closed timestamp 时间戳的本地副本。详细看 foller Rdads。<font color='red'> 为什么在非 leaseholder 上读，说明 leaseholder 可以看到最新的写，非leaseholder看不到 ？</font></li>
</ul>
<h3 id="Commits-phase-2"><a href="#Commits-phase-2" class="headerlink" title="Commits (phase 2)"></a>Commits (phase 2)</h3><p>CockroachDB 检查运行时的事务的记录，如果是 ABORTED。将重试事务。<font color='red'> 为什么会是 aborted ？</font></p>
<p>大多数情况下，他设置事务记录状态为 STAGING。检查 pending 状态事务的写意向事务已经成功（被集群复制）。</p>
<p><font color='red'>  如何检查是否被复制 ？</font></p>
<p>如果事务通过了检查，CockroacDB 返回客户端成功，进入 cleanup 阶段。到这时候，事务已经被提交。</p>
<p>关于更多的提交协议，看 并行提交。</p>
<h3 id="Cleanup-asynchronous-phase-3"><a href="#Cleanup-asynchronous-phase-3" class="headerlink" title="Cleanup (asynchronous phase 3)"></a>Cleanup (asynchronous phase 3)</h3><p>事务被提交后，他应当被标记，所有的写意向应该被解析。为了达到这个目的。 Coordinating 节点记录了所有写过的 keys。</p>
<p><font color='red'> 记录所有写过的 keys 干嘛用 ？清理的时候用</font></p>
<ul>
<li>将事务状态从 STAGING 改为 COMMITED。</li>
<li>解析事务的写意向到 MVCC。通过移除事务记录的指针。</li>
<li>删除写意向。</li>
</ul>
<p>这是一个简单的优化，如果操作遇到了写意向，他们总是检查事务状态，任何操作可以解析和移除写意向通过检查事务记录的状态。</p>
<h2 id="技术细节和组件"><a href="#技术细节和组件" class="headerlink" title="技术细节和组件"></a>技术细节和组件</h2><h3 id="时间和混合逻辑时钟"><a href="#时间和混合逻辑时钟" class="headerlink" title="时间和混合逻辑时钟"></a>时间和混合逻辑时钟</h3><p>在分布式系统中，排序和因果关系是一个复杂的问题。虽然可以完全依赖 Raft 共识来保持可序列化性，但读取数据效率低下。为了优化读性能，CockroachDB 实现了混合逻辑时钟，由一个物理组件（总是接近本地时间） 和一个逻辑组件（用于区分相同物理组件的事件）。这意味着 HTC 时间总是大于等于 wall time。</p>
<p><font color='red'> HLC 如何提高读性能 ？</font></p>
<p>在事务方面，网关节点总是给事务选择一个 HLC 时间戳。无论何时，当提及事务时间戳的时候，他一定是一个 HLC 时间，这个时间戳也会用来跟踪 MVCC 的值。提供事务一致性的保障。</p>
<p>当节点向其他节点发送请求时，它们包括由其本地 HLC 生成的时间戳（包括物理和逻辑组件）。 当节点接收到请求时，它们会将发送者随事件提供的时间戳通知其本地 HLC。 这对于保证节点上读取&#x2F;写入的所有数据的时间戳小于下一个 HLC 时间很有用。</p>
<p>然后，这让主要负责范围的节点（即，leaseholder）通过确保读取数据的事务处于大于它正在读取的 MVCC 值的 HLC 时间（即，读取总是发生在“之后 “写）。</p>
<h4 id="Max-clock-offset-enforcement"><a href="#Max-clock-offset-enforcement" class="headerlink" title="Max clock offset enforcement"></a>Max clock offset enforcement</h4><p>CockroachDB 需要中等级别的时钟同步以保持数据一致性。 出于这个原因，当一个节点检测到它的时钟与集群中至少一半的其他节点不同步到允许的最大偏移量（默认 500ms）的 80% 时，它会立即崩溃。</p>
<p>尽管无论时钟偏移如何都可以保持可串行化的一致性，但在配置的时钟偏移范围之外的偏移可能会导致因果相关事务之间的单键线性化违规。 因此，通过在每个节点上运行 NTP 或其他时钟同步软件来防止时钟漂移太远非常重要。</p>
<p><font color='red'>物理时钟不一致会导致什么问题  ? </font></p>
<h3 id="时间戳缓存-Timestamp-cache"><a href="#时间戳缓存-Timestamp-cache" class="headerlink" title="时间戳缓存 (Timestamp cache)"></a>时间戳缓存 (Timestamp cache)</h3><p>作为提供可串行化的一部分，每当一个操作读取一个值时，我们将操作的时间戳存储在时间戳缓存中，它显示正在读取的值的高水位标记。</p>
<p>时间戳缓存是一种数据结构，用于存储有关 leaseholders 执行的读取的信息。 这用于确保一旦某个事务 t1 读取一行，另一个出现并尝试写入该行的事务 t2 将在 t1 之后排序，从而确保事务的串行顺序，即可串行化。</p>
<p><font color='red'> 时间戳缓存在哪个节点存储 ？使用的时候怎么查找 ？</font></p>
<p>每当发生写入时，都会根据时间戳缓存检查其时间戳。 如果时间戳小于时间戳缓存的最新值，我们会尝试将其事务的时间戳推到稍后的时间。 推送时间戳可能会导致事务在事务的第二阶段重新启动（请参阅读取刷新）。</p>
<h3 id="Closed-timestamps"><a href="#Closed-timestamps" class="headerlink" title="Closed timestamps"></a>Closed timestamps</h3><p>每个 CockroachDB range 都跟踪一个称为其关闭时间戳的属性，这意味着永远不会在该时间戳或低于该时间戳时引入新的写入。 关闭的时间戳在 leaseholder 上连续提前，并且比当前时间滞后某个目标时间间隔。 随着关闭的时间戳提前，通知会发送给每个 follower。 如果某个 range 接受小于或等于其关闭时间戳的写，则写入将被迫更改其时间戳，这可能会导致事务重试错误（请参阅读取刷新）。</p>
<p><font color='red'> 所以到底能不能接受 小于 closed timestamps 的写 ？如果可以，什么情况下可以？ </font></p>
<p>换句话说，closed timestamp 是该 range 的 leaseholder 向其 follower 副本的承诺，即它不会接受低于该时间戳的写入。 一般来说，leaseholder 会在过去几秒钟内连续关闭时间戳。 ？？？</p>
<p><font color='red'> colsed timestamp 是在什么情况下生成？什么时候更新？  什么时候会删除 ？</font></p>
<p>Closed timestamps 子系统通过将 closed timestamps 捎带到 Raft 命令上来将信息从 leaseholders 有者传播到 followers，使复制流与timestamp closing 同步。 这意味着，一旦 followers 副本将所有 Raft 命令应用到由 leaseholder 指定的 Raft log 中的位置，它就可以开始提供时间戳等于或低于关闭时间戳的读取。</p>
<p>一旦 followers 副本应用了上述 Raft 命令，它就拥有了为时间戳小于或等于关闭时间戳的读取提供服务所需的所有数据。</p>
<p>请注意，即使承租人发生变化，关闭的时间戳也是有效的，因为它们会在租约转移中保留。 一旦发生租约转移，新的租约人将不会违反旧租约人作出的封闭时间戳承诺。</p>
<p>封闭的时间戳提供了用于为低延迟历史（陈旧）读取（也称为跟随者读取）提供支持的保证。 跟随者读取在多区域部署中特别有用。</p>
<p>有关关闭时间戳和 Follower Reads 实现的更多信息，请参阅我们的博客文章 An Epic Read on Follower Reads。</p>
<h3 id="client-Txn-and-TxnCoordSender"><a href="#client-Txn-and-TxnCoordSender" class="headerlink" title="client.Txn and TxnCoordSender"></a>client.Txn and TxnCoordSender</h3><p>正如我们在 SQL 层的架构概述中提到的，CockroachDB 将所有 SQL 语句转换为键值（KV）操作，这就是数据最终存储和访问的方式。</p>
<p>SQL 层生成的所有 KV 操作都使用 client.Txn，它是 CockroachDB KV 层的事务接口——但是，正如我们上面讨论的，所有语句都被视为事务，因此所有语句都使用该接口。</p>
<p>然而，client.Txn 实际上只是 TxnCoordSender 的一个包装器，它在我们的代码库中起着至关重要的作用：</p>
<ul>
<li>处理事务的状态。 事务启动后，TxnCoordSender 开始向该事务的事务记录异步发送心跳消息，这表明它应该保持活动状态。 如果 TxnCoordSender 的心跳停止，则事务记录将移至 ABORTED 状态。</li>
<li>在事务进行中跟踪每个 被写的 key 和 key range 。</li>
<li>清理累计的写意向当事务终止或者提交的时候，所有的请求都作为事务的一部分通过相同的 TxnCoordSender 统计所有的 写意向，从而优化 cleanup 阶段。</li>
</ul>
<p>在设置了这个簿记之后，请求被传递到分布层中的 DistSender。</p>
<h3 id="Transaction-records"><a href="#Transaction-records" class="headerlink" title="Transaction records"></a>Transaction records</h3><p>为了跟踪事务执行的状态，我们将一个称为事务记录的值写入我们的键值存储。 一个事务的所有写意图都指向这个记录，这让任何事务都可以检查它遇到的任何写意图的状态。 这种规范记录对于支持分布式环境中的并发性至关重要。</p>
<p>事务记录总是写入与事务中的第一个 key 相同的 range ，这由 TxnCoordSender 知道。 但是，在出现以下任一情况之前，不会创建交易记录本身：</p>
<ul>
<li>写操作提交</li>
<li>The <code>TxnCoordSender</code> heartbeats the transaction</li>
<li>An operation forces the transaction to abort 强制终止事务</li>
</ul>
<p>鉴于这种机制，事务记录使用以下状态：</p>
<ul>
<li><code>PENDING</code>: Indicates that the write intent’s transaction is still in progress. 表示写入意图的事务仍在进行中。</li>
<li><code>COMMITTED</code>: Once a transaction has completed, this status indicates that write intents can be treated as committed values.  一旦事务完成，此状态表明写入意图可以被视为已提交的值。</li>
<li>STAGING：用于启用并行提交功能。 根据此记录引用的写入意图的状态，事务可能处于提交状态，也可能不处于提交状态。<font color='red'>  什么情况下提交 ？什么情况下未提交？ </font></li>
<li><code>ABORTED</code>: Indicates that the transaction was aborted and its values should be discarded.</li>
<li>记录不存在：如果一个事务遇到一个事务记录不存在的写意图，它使用写意图的时间戳来确定如何进行。 如果写入意图的时间戳在事务活跃度阈值内，则写入意图的事务被视为处于待处理状态，否则视为事务已中止。</li>
</ul>
<p>已提交事务的事务记录将一直保留，直到其所有写入意图都转换为 MVCC 值。</p>
<h3 id="Write-intents"><a href="#Write-intents" class="headerlink" title="Write intents"></a>Write intents</h3><p>CockroachDB 中的值不会直接写入存储层； 相反，值以称为“写入意图”的临时状态写入。 这些本质上是 MVCC 记录，其中添加了一个附加值，用于标识该值所属的事务记录。 它们可以被认为是复制锁和复制临时值的组合。</p>
<p>每当操作遇到写入意图（而不是 MVCC 值）时，它都会查找事务记录的状态以了解它应该如何处理写入意图值。 如果事务记录丢失，则操作检查写入意图的时间戳并评估它是否被视为过期。</p>
<p>CockroachDB 使用每个节点的内存锁表来管理并发控制。 此表包含正在进行的事务获取的锁的集合，并在评估期间发现写入意图时包含有关写入意图的信息。 有关详细信息，请参阅下面有关并发控制的部分。</p>
<h4 id="Resolving-write-intents"><a href="#Resolving-write-intents" class="headerlink" title="Resolving write intents"></a>Resolving write intents</h4><p>当一个操作遇到了写意向，他尝试解析，解析的结果依赖写意向上的事务记录：</p>
<p>COMMITTED：该操作读取写意图并通过删除写意图指向事务记录的指针将其转换为 MVCC 值。</p>
<p>ABORTED：写入意图被忽略并删除。</p>
<p>PENDING：这表明存在必须解决的事务冲突。</p>
<p>STAGING：这表明操作需要检查 是否 staging 状态的事务还在运行中，通过检查 事务 coordinator 是否仍然在心跳事务记录，如果 coordinator 仍然在心跳事务记录，操作需要等待。 关于更多，参考并行提交。</p>
<h3 id="Concurrency-control"><a href="#Concurrency-control" class="headerlink" title="Concurrency control"></a>Concurrency control</h3><p>并发管理器对传入请求进行排序，并在发出那些打算执行冲突操作的请求的事务之间提供隔离。 此活动也称为并发控制。</p>
<p>并发管理器结合了 lath manager  和 lock table 的操作来完成这项工作：</p>
<ul>
<li>latch manager 对传入请求进行排序并在这些请求之间提供隔离。</li>
<li>锁表提供请求的锁定和排序（与锁管理器一致）。 它是一个每个节点的内存数据结构，其中包含由进行中事务获取的锁的集合。 为了确保与现有的写入意图系统（也称为复制的排他锁）的兼容性，它会在评估请求的过程中发现这些外部锁时根据需要提取有关这些外部锁的信息。</li>
</ul>
<p>并发管理器支持使用 SELECT FOR UPDATE 语句通过 SQL 进行悲观锁定。 此语句可用于增加吞吐量并减少竞争操作的尾部延迟。</p>
<p>有关 concurrency manager 如何与 latch manager  和 lock table 一起工作的更多详细信息，请参阅以下部分：</p>
<h4 id="Concurrency-manager"><a href="#Concurrency-manager" class="headerlink" title="Concurrency manager"></a>Concurrency manager</h4><p>并发管理器是一种结构，它对传入的请求进行排序，并在发出那些打算执行冲突操作的请求的事务之间提供隔离。 在排序过程中，发现冲突并通过被动排队和主动推送的组合来解决任何发现的问题。 一旦对请求进行排序，就可以自由评估，而不必担心由于管理器提供的隔离而与其他进行中的请求发生冲突。 这种隔离在请求的生命周期内得到保证，但在请求完成后终止。</p>
<h1 id="分布层"><a href="#分布层" class="headerlink" title="分布层"></a>分布层</h1><p>分布层为集群中的数据提供一个统一的视图。</p>
<h2 id="概览-2"><a href="#概览-2" class="headerlink" title="概览"></a>概览</h2><p>为了使用户可以从任何一个节点访问集群中的数据，CockroackDB 将数据以 kv 对的形式存储在一个整体有序的map中。这个 key 空间描述集群中的所有数据和数据的位置，数据被划分到了 ranges 中，所有的key 都可以被找到在 range中。</p>
<p>CockroachDB 实现排序的map以便：</p>
<ul>
<li><strong>Simple lookups</strong>：因为我们确定了哪些节点负责数据的某些部分，所以查询能够快速定位到哪里可以找到他们想要的数据。</li>
<li><strong>Efficient scans</strong>：通过定义数据的顺序，在扫描过程中很容易找到特定范围内的数据。</li>
</ul>
<h3 id="排序map-整体结构"><a href="#排序map-整体结构" class="headerlink" title="排序map 整体结构"></a>排序map 整体结构</h3><p>整体排序的 map 由两个基本元素组成：</p>
<ul>
<li>System data，包括集群中源数据的位置。（在学多其他集群范围和本地数据中）。</li>
<li>User data，存储集群中表数据。</li>
</ul>
<h4 id="Meta-ranges"><a href="#Meta-ranges" class="headerlink" title="Meta ranges"></a>Meta ranges</h4><p>集群中所有 range 的位置存储在键空间开头的两级索引中，称为meta ragnes，其中第一级 (meta1) 寻址第二级，第二级 (meta2) 寻址数据 集群。</p>
<p>这个两级索引加上用户数据可以可视化为一棵树，根在 meta1，第二级在 meta2，树的叶子由保存用户数据的范围组成。</p>
<p>重要的是，每个节点都有关于在哪里定位 meta1 范围的信息（称为其范围描述符，详情如下），并且该范围永远不会被分割。</p>
<p>默认情况下，这种元范围结构允许我们处理多达 4EiB 的用户数据：我们可以处理 2^(18 + 18) &#x3D; 2^36 个范围； 每个范围寻址 2^26 B，我们总共寻址 2^(36+26) B &#x3D; 2^62 B &#x3D; 4EiB。 但是，使用更大的范围大小，可以进一步扩展此容量。</p>
<p>元范围主要被视为正常范围，并且像集群的 KV 数据的其他元素一样被访问和复制。</p>
<p>每个节点缓存它之前访问过的 meta2 范围的值，从而优化将来对该数据的访问。 每当一个节点发现它的 meta2 缓存对于特定键无效时，缓存会通过对 meta2 范围执行定期读取来更新。</p>
<h4 id="Table-data"><a href="#Table-data" class="headerlink" title="Table data"></a>Table data</h4><p>在节点的元范围之后是集群存储的 KV 数据。</p>
<p>每个表及其二级索引最初都映射到一个范围，其中范围中的每个键值对代表表中的单个行（也称为主索引，因为该表是按主键排序的）或二级索引。一旦范围的大小达到 512 MiB，它就会分成两个范围。这个过程继续作为一个表，它的索引继续增长。一旦表被拆分为多个范围，表和二级索引很可能将存储在不同的范围中。但是，范围仍然可以包含表和二级索引的数据。</p>
<p>默认的 512 MiB 范围大小对我们来说是一个最佳点，它既小到可以在节点之间快速移动，又大到可以存储一组有意义的连续数据，这些数据的键更有可能被一起访问。然后，这些范围会在您的集群周围重新排列，以确保可生存性。</p>
<p>这些表范围被复制（在恰当命名的复制层中），并且每个副本的地址存储在 meta2 范围中。</p>
<h3 id="Using-the-monolithic-sorted-map"><a href="#Using-the-monolithic-sorted-map" class="headerlink" title="Using the monolithic sorted map"></a>Using the monolithic sorted map</h3><p>如元范围部分所述，集群中所有范围的位置存储在两级索引中：</p>
<ul>
<li>第一级 (meta1) 处理第二级。</li>
<li>第二级（meta2）处理用户数据。</li>
</ul>
<p>这也可以可视化为一棵树，根在 meta1，第二层在 meta2，树的叶子由保存用户数据的范围组成。</p>
<p>当一个节点收到一个请求时，它会以自下而上的方式查找包含请求中键的范围的位置，从这棵树的叶子开始。 此过程的工作方式如下：</p>
<ol>
<li>对于每个键，节点会在第二级范围元数据 (meta2) 中查找包含指定键的范围的位置。 该信息被缓存以提高性能； 如果在缓存中找到范围的位置，则立即返回。</li>
<li>如果在缓存中找不到范围的位置，则节点查找 meta2 的实际值所在的范围的位置。 此信息也被缓存； 如果在缓存中找到 meta2 范围的位置，则节点向 meta2 范围发送 RPC 以获取请求要操作的键的位置，并返回该信息。</li>
<li>最后，如果在缓存中找不到 meta2 范围的位置，则节点查找第一级范围元数据（meta1）的实际值所在的范围的位置。 这种查找总是成功的，因为 meta1 的位置使用 gossip 协议分布在集群中的所有节点之间。 然后节点使用来自 meta1 的信息来查找 meta2 的位置，并从 meta2 中查找包含请求中键的范围的位置。</li>
</ol>
<p>请注意，上述过程是递归的； 每次执行查找时，它要么 (1) 从缓存中获取位置，要么 (2) 对树中下一级“向上”的值执行另一次查找。 由于缓存了范围元数据，因此通常可以执行查找，而无需将 RPC 发送到另一个节点。</p>
<p>既然节点有了来自请求的键所在的范围的位置，它就会将请求中的 KV 操作发送到 BatchRequest 中的范围（使用 DistSender 机器）。</p>
<h2 id="技术细节和组件-1"><a href="#技术细节和组件-1" class="headerlink" title="技术细节和组件"></a>技术细节和组件</h2><h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><p>gRPC 是用于相互通信的软件节点。 因为分发层是与其他节点通信的第一层，所以 CockroachDB 在这里实现了 gRPC。</p>
<p>gRPC 要求将输入和输出格式化为协议缓冲区（protobufs）。 为了利用 gRPC，CockroachDB 实现了 api.proto 中定义的基于协议缓冲区的 API。</p>
<h3 id="BatchRequest"><a href="#BatchRequest" class="headerlink" title="BatchRequest"></a>BatchRequest</h3><p>所有 KV 操作请求都捆绑到一个 protobuf 中，称为 BatchRequest。 此批次的目的地在 BatchRequest 标头中标识，以及指向请求的事务记录的指针。 （另一方面，当一个节点在回复一个 BatchRequest 时，它使用了一个 protobuf————BatchResponse。）</p>
<p>这个 BatchRequest 也用于使用 gRPC 在节点之间发送请求，它接受和发送协议缓冲区。</p>
<h3 id="DistSender"><a href="#DistSender" class="headerlink" title="DistSender"></a>DistSender</h3><h1 id="复制层"><a href="#复制层" class="headerlink" title="复制层"></a>复制层</h1><p>CockroachDB 的复制层拷贝数据在节点之间，并且通过一致性算法来确保拷贝数据的一致性。</p>
<h2 id="概览-3"><a href="#概览-3" class="headerlink" title="概览"></a>概览</h2><p>高可用需要数据库可以容忍节点下线而不打断服务，保证应用可用。这意味着需要复制数据在节点之间来确保数据仍然可以访问。</p>
<p>在节点离线的时候确保一致性是数据库的挑战。为了解决这个问题，CockroachDB 使用的一致性算法需要多数的副本同意修改才能提交修改。因为3 是一个最小的数字达到多数派，CockroachDB 最少需要 3 个节点。</p>
<h1 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h1><p>CockroachDB的存储层读写数据从磁盘上。</p>
<p><a href="https://www.cockroachlabs.com/blog/pebble-rocksdb-kv-store/">https://www.cockroachlabs.com/blog/pebble-rocksdb-kv-store/</a></p>
<h2 id="概览-4"><a href="#概览-4" class="headerlink" title="概览"></a>概览</h2><p>每个CockroachDB节点至少包含一个 store，在节点启动的时候指定，这是cockroach进程读写磁盘数据的位置。</p>
<p>数据以 key-value 的形式存储在存储引擎，被认为是 黑盒API。</p>
<p>CockroachDB使用 pebble 存储引擎，pebble 是受 RocksDB 启发，但是有不同：</p>
<ul>
<li>使用go语言并且实现 RocksDB 的一个子集。</li>
<li>包含有利于 CockroachDB 的优化。</li>
</ul>
<p>在内部，每个 store 包含两个存储引擎的实例：</p>
<ul>
<li>一个用来存储临时的分布式SQL数据。</li>
<li>一个存储节点所有的其他数据。</li>
</ul>
<p>此外，还有一个块缓存在一个节点的所有 store之间共享，这些 stores 又有 range 副本的集合。一个 range 的多个副本永远不会被放在一个 stroe 甚至是一个 节点。</p>
<h2 id="组件-1"><a href="#组件-1" class="headerlink" title="组件"></a>组件</h2><h3 id="pebble"><a href="#pebble" class="headerlink" title="pebble"></a>pebble</h3><p>pebble 和 CockroachDB很好的集成有许多原因：</p>
<ul>
<li>他是一个 kv 存储，使得很容易引射成key -value。</li>
<li>提供原子写 batnches 和 snapshots，是事务的一个子集。</li>
<li>自研。</li>
<li>包含 rocksdb 中没有的优化，灵感来自 CockraochDb 如何使用 存储引擎。<a href="https://www.cockroachlabs.com/blog/bulk-data-import/">https://www.cockroachlabs.com/blog/bulk-data-import/</a></li>
</ul>
<p>底层 Pebble 引擎通过前缀压缩来保证 key 的高效存储。</p>
<h4 id="LSM-tree"><a href="#LSM-tree" class="headerlink" title="LSM tree"></a>LSM tree</h4><p>pebble 使用lsm tree 来管理数据存储。LSM tree 是一个层次树。在树的每一层，有对应的在磁盘上的文件来存储数据。这些文件叫做 SST 文件（sorted string table）。</p>
<h5 id="SSTs"><a href="#SSTs" class="headerlink" title="SSTs"></a>SSTs</h5><p>SST 是 key- value对 排序列表的磁盘表示。 </p>
<p>SST文件上不可变的，他们不会被修改，即使在 compaction 进程中。</p>
<h5 id="LSM-levels"><a href="#LSM-levels" class="headerlink" title="LSM levels"></a>LSM levels</h5><p>LSM tree 被组织为 L0 到 L6 层。L0上最顶层，L6是最底层。新数据是被添加到 L0层，然后随时间被 merged 到底层。</p>
<p>LSM tree 的每一层都有 SSTs 的集合，每个 SST 是不可变的并且是唯一的，序号是单调递增的。</p>
<p>每个一层的 SST 的 key 不会重叠：例如 如果一个 SST 包含key [A-F] ，下一个文件包含 [F-R]。但是 L0是特殊的，L0是唯一一层可以包含重叠的 keys 。由于以下原因，这种例外是必要的：</p>
<ul>
<li>为了使 基于LSM tree 的存储引擎，比如 pebble 支持大量数据的摄取，比如当使用 IMPORT 语句的时候。</li>
<li>为了使 memtales 更容易和高效的 flushes。</li>
</ul>
<h5 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h5><p>merging SSTs 文件并且从 L0移动到 L6到过程叫做 compaction，存储引擎compact 数据越快越好。这个过程的结果使得底层的包含更大的 SSTs文件，并且包含较少的最近更新的 keys。</p>
<p>compaction 过程是必要的为了使 LSM 更加高效； 从 L0到 L6，每一层应该包含下一层大约十分之一的数据。比如，L1大约有L2十分之一的数据。理想情况下，尽可能多的数据将存储在 LSM 较低级别引用的较大 SST 中。如果 compaction 过程落后，可能导致 inverted LSM。</p>
<p>SST 文件在压缩过程中永远不会被修改。 相反，新的 SST 被写入，旧的 SST 被删除。 这种设计利用了顺序磁盘访问比随机磁盘访问快得多的事实。</p>
<p>压缩的过程是这样的：如果需要合并两个 SST 文件 A 和 B，则将它们的内容（键值对）读入内存。 内容在内存中进行排序和合并，然后打开一个新文件 C 并将其写入磁盘，其中包含新的、更大的键值对排序列表。 此步骤在概念上类似于归并排序。 最后，旧文件 A 和 B 被删除。</p>
<h5 id="inverted-LSMs"><a href="#inverted-LSMs" class="headerlink" title="inverted LSMs"></a>inverted LSMs</h5><p>如果压缩过程落后于添加的数据量，并且树的更高层存储的数据多于下面的层级，则 LSM 形状可能会反转。</p>
<p>反向 LSM 会降低读取性能。</p>
<p>反向LSM 的读放大很高，在倒置 LSM 状态下，读取需要从更高级别开始，并通过大量 SST“向下看”以读取key的正确（最新）值。 当存储引擎需要从多个 SST 文件中读取以服务于单个逻辑读取时，这种状态称为读取放大。</p>
<p>如果大量的 IMPORT 使集群过载（由于 CPU 和&#x2F;或 IOPS 不足）并且存储引擎必须咨询 L0 中的许多小型 SST 以确定正在使用的键的最新值，则读取放大可能会特别糟糕。（例如，使用 SELECT）。</p>
<p>写放大比读放大更复杂，但可以广义地定义为：“我在压缩期间重写了多少物理文件？” 例如，如果存储引擎在 L5 中进行大量压缩，它将一遍又一遍地重写 L5 中的 SST 文件。 这是一个折衷，因为如果引擎没有足够频繁地执行压缩，L0 的大小会变得太大，并且会导致反向 LSM，这也会产生不良影响。</p>
<p>读取放大和写入放大是 LSM 性能的关键指标。 两者都不是天生的“好”或“坏”，但它们不能过度出现，并且为了获得最佳性能，它们必须保持平衡。 这种平衡涉及权衡。</p>
<p>倒置的 LSM 也有过多的压实债务。 在这种状态下，存储引擎有大量的压缩积压要做，以使反转的 LSM 恢复到正常的非反转状态。</p>
<p>有关如何监控集群的 LSM 运行状况的说明，请参阅 LSM 运行状况。 要监控集群的 LSM L0 运行状况，请参阅 LSM L0 运行状况。</p>
<h5 id="Memtable和wal"><a href="#Memtable和wal" class="headerlink" title="Memtable和wal"></a>Memtable和wal</h5><p>为了便于管理 LSM 树结构，存储引擎维护 LSM 的内存表示，称为 memtable； 内存表中的数据会定期刷新到磁盘上的 SST 文件中。</p>
<p>磁盘上另一个名为 write-ahead log（以下称为 WAL）的文件与每个 memtable 相关联，以确保在断电或其他故障的情况下的持久性。 WAL 是复制层向存储引擎发布的最新更新存储在磁盘上的位置。 每个 WAL 与一个 memtable 是一一对应的； 它们保持同步，并且作为存储引擎正常操作的一部分，来自 WAL 和 memtable 的更新会定期写入 SST。</p>
<p> 新值在写入memtable的同时写入 WAL。 它们最终会从 memtable 写入磁盘上的 SST 文件以进行长期存储。</p>
<h5 id="LSM-设计的权衡"><a href="#LSM-设计的权衡" class="headerlink" title="LSM 设计的权衡"></a>LSM 设计的权衡</h5><p>LSM 树设计优化了写入性能而不是读取性能。 通过将排序的键值数据保存在 SST 中，可以避免写入时的随机磁盘寻道。 它试图通过从 LSM 树中尽可能低的位置、从更少、更大的文件中进行读取来降低读取（随机搜索）的成本。 这就是存储引擎执行压缩的原因。 存储引擎还尽可能使用块缓存来进一步加快读取速度。</p>
<p>LSM 设计中的权衡是为了利用现代磁盘的工作方式，因为即使它们由于缓存提供了对磁盘上随机位置的更快读取，但它们在写入随机位置时的性能仍然相对较差。</p>
<h3 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h3><p>CockroachDB 严重依赖多版本并发控制（MVCC）来处理并发请求并保证一致性。 大部分工作是通过使用混合逻辑时钟 (HLC) 时间戳来区分数据版本、跟踪提交时间戳和识别值的垃圾收集到期来完成的。 然后，所有这些 MVCC 数据都存储在 Pebble 中。</p>
<p>尽管在存储层中实现，但 MVCC 值被广泛用于在事务层中强制执行一致性。 例如，CockroachDB 维护一个时间戳缓存，它存储最后一次读取密钥的时间戳。 如果写入操作发生的时间戳低于读取时间戳缓存中的最大值，则表示存在潜在异常，事务必须在稍后的时间戳重新启动。</p>
<h4 id="Time-travel"><a href="#Time-travel" class="headerlink" title="Time-travel"></a>Time-travel</h4><p>如 SQL:2011 标准中所述，CockroachDB 支持时间旅行查询（由 MVCC 启用）。</p>
<p>为此，所有模式信息背后也有一个类似 MVCC 的模型。 这使您可以执行 SELECT…AS OF SYSTEM TIME，CockroachDB 使用当时的模式信息来制定查询。</p>
<p>使用这些工具，您可以从数据库中获得一致的数据在垃圾回收之前。</p>
<h3 id="Garbage-collection"><a href="#Garbage-collection" class="headerlink" title="Garbage collection"></a>Garbage collection</h3><p>CockroachDB 定期垃圾收集 MVCC 值以减少存储在磁盘上的数据大小。 为此，当有一个较新的 MVCC 值具有比垃圾回收期更早的时间戳时，我们会压缩旧的 MVCC 值。 通过配置 gc.ttlseconds 复制区域变量，可以在集群、数据库或表级别设置垃圾收集周期。 </p>
<h4 id="Protected-timestamps"><a href="#Protected-timestamps" class="headerlink" title="Protected timestamps"></a>Protected timestamps</h4><p>垃圾收集只能在不受保护的时间戳覆盖的 MVCC 值上运行。 受保护的时间戳子系统的存在是为了确保依赖历史数据的操作的安全性，例如：</p>
<ul>
<li><a href="https://www.cockroachlabs.com/docs/v21.2/import">Imports</a>, including <a href="https://www.cockroachlabs.com/docs/v21.2/import-into"><code>IMPORT INTO</code></a></li>
<li><a href="https://www.cockroachlabs.com/docs/v21.2/backup">Backups</a></li>
<li><a href="https://www.cockroachlabs.com/docs/v21.2/change-data-capture-overview">Changefeeds</a></li>
<li><a href="https://www.cockroachlabs.com/docs/v21.2/online-schema-changes">Online schema changes</a></li>
</ul>
<p>受保护的时间戳可确保历史数据的安全性，同时还可以实现更短的 GC TTL。 较短的 GC TTL 意味着保留较少的先前 MVCC 值。 这有助于降低全天频繁更新行的工作负载的查询执行成本，因为 SQL 层必须扫描以前的 MVCC 值才能找到行的当前值。</p>
<h5 id="How-protected-timestamps-work"><a href="#How-protected-timestamps-work" class="headerlink" title="How protected timestamps work"></a>How protected timestamps work</h5><p>保护时间戳通过在内部系统表中创建记录来工作。当一个长时间运行的 job ，比如备份想要保护某个时间戳的数据不被 GC 时，他会创建改数据和时间戳关联的 记录。</p>
<p>成功创建保护记录后，时间戳小于或等于受保护时间戳的指定数据的 MVCC 值将不会被垃圾收集。 当创建保护记录的作业完成其工作时，它会删除该记录，从而允许垃圾收集器在以前受保护的值上运行。</p>
<h2 id="Transaction-contention-争用"><a href="#Transaction-contention-争用" class="headerlink" title="Transaction contention(争用)"></a>Transaction contention(争用)</h2><p>对相同索引键值进行操作的事务（特别是对给定索引键在相同列族上进行操作的事务）被严格序列化以遵守事务隔离语义。 为了保持这种隔离，编写事务“锁定”行以防止与并发事务的危险交互。 但是，如果多个事务试图同时访问相同的“锁定”数据，锁定可能会导致处理延迟。 这称为事务（或锁）争用。</p>
<p>当满足以下三个条件时，就会发生事务争用：</p>
<ul>
<li>有多个并发事务或语句（由同时连接到单个 CockroachDB 集群的多个客户端发送）。</li>
<li>它们对具有相同索引键值（主键或二级索引）的表行进行操作。</li>
<li>至少一个事务修改数据。</li>
</ul>
<p>经历争用的事务通常会延迟完成或重新启动。 事务重启的可能性需要客户端实现事务重试。</p>
<h3 id="Find-transaction-contention"><a href="#Find-transaction-contention" class="headerlink" title="Find transaction contention"></a>Find transaction contention</h3><p>查找事务中发生争用的事务和语句。 CockroachDB 有几个工具可以帮助您追踪此类事务和语句：</p>
<ul>
<li>在 DB Console 中，访问事务和语句页面并按争用对事务和语句进行排序。</li>
<li>查询数据库的 crdb_internal.cluster_contended_indexes 和 crdb_internal.cluster_contended_tables 表，以查找发生争用的索引和表。</li>
</ul>
<h3 id="Reduce-transaction-contention"><a href="#Reduce-transaction-contention" class="headerlink" title="Reduce transaction contention"></a>Reduce transaction contention</h3><p>为了减少事务争用：</p>
<ul>
<li>让事务更小，让每个事务做的工作更少。 特别是，避免每个事务进行多次客户端-服务器交换。 例如，使用公用表表达式将多个 SELECT 和 INSERT、UPDATE、DELETE 和 UPSERT 子句组合到一个 SQL 语句中。</li>
<li>一次性发送事务中的所有语句，以便 CockroachDB 自动为您重试事务。</li>
<li>在事务执行读取然后更新它刚刚读取的行的情况下使用 SELECT FOR UPDATE 语句。 该语句通过控制对表的一行或多行的并发访问来对事务进行排序。 它通过锁定选择查询返回的行来工作，这样试图访问这些行的其他事务被迫等待锁定这些行的事务完成。 这些其他事务被有效地放入队列中，该队列根据它们何时尝试读取锁定行的值进行排序。</li>
<li>替换行中的值时，使用 UPSERT 并为插入行中的所有列指定值。 与 SELECT、INSERT 和 UPDATE 的组合相比，这通常在争用情况下具有最佳性能。</li>
</ul>
<h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><h2 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h2><p>CockroachDB 支持将多个 SQL 语句捆绑到单个全有或全无事务中。 每个事务保证 ACID 语义跨越任意表和行，即使数据是分布式的。 如果事务成功，则所有突变都将与虚拟同时一起应用。 如果事务的任何部分失败，则整个事务将中止，并且数据库保持不变。 CockroachDB 保证当一个事务处于挂起状态时，它与其他具有可序列化隔离的并发事务隔离。</p>
<p>有关 CockroachDB 事务语义的详细讨论，请参阅 <a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/">https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/</a> 和  <a href="https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/">https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/</a> 这篇博文中描述的事务模型的解释有些过时了。 有关更多详细信息，请参阅事务重试部分。</p>
<h3 id="SQL-statements"><a href="#SQL-statements" class="headerlink" title="SQL statements"></a>SQL statements</h3><table>
<thead>
<tr>
<th>Statement</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/begin-transaction"><code>BEGIN</code></a></td>
<td>Initiate a transaction, as well as control its <a href="https://www.cockroachlabs.com/docs/v22.1/transactions#transaction-priorities">priority</a>.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/set-transaction"><code>SET TRANSACTION</code></a></td>
<td>Control a transaction’s <a href="https://www.cockroachlabs.com/docs/v22.1/transactions#transaction-priorities">priority</a>.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/commit-transaction"><code>COMMIT</code></a></td>
<td>Commit a regular transaction, or clear the connection after committing a transaction using the <a href="https://www.cockroachlabs.com/docs/v22.1/advanced-client-side-transaction-retries">advanced retry protocol</a>.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/rollback-transaction"><code>ROLLBACK</code></a></td>
<td>Abort a transaction and roll the database back to its state before the transaction began.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/show-vars"><code>SHOW</code></a></td>
<td>Display the current transaction settings.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/savepoint"><code>SAVEPOINT</code></a></td>
<td>Used for <a href="https://www.cockroachlabs.com/docs/v22.1/transactions#nested-transactions">nested transactions</a>; also used to implement <a href="https://www.cockroachlabs.com/docs/v22.1/advanced-client-side-transaction-retries">advanced client-side transaction retries</a>.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/release-savepoint"><code>RELEASE SAVEPOINT</code></a></td>
<td>Commit a <a href="https://www.cockroachlabs.com/docs/v22.1/transactions#nested-transactions">nested transaction</a>; also used for <a href="https://www.cockroachlabs.com/docs/v22.1/advanced-client-side-transaction-retries">retryable transactions</a>.</td>
</tr>
<tr>
<td><a href="https://www.cockroachlabs.com/docs/v22.1/rollback-transaction"><code>ROLLBACK TO SAVEPOINT</code></a></td>
<td>Roll back a <a href="https://www.cockroachlabs.com/docs/v22.1/transactions#nested-transactions">nested transaction</a>; also used to handle <a href="https://www.cockroachlabs.com/docs/v22.1/advanced-client-side-transaction-retries">retryable transaction errors</a>.</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>从零写数据库系列-scanner</title>
    <url>/2023/02/01/2-%E6%95%B0%E6%8D%AE%E5%BA%93/3-%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93/%E4%BB%8E%E9%9B%B6%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B02-scanner/</url>
    <content><![CDATA[<span id="more"></span>



]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>从零写数据库系列-parser</title>
    <url>/2023/02/01/2-%E6%95%B0%E6%8D%AE%E5%BA%93/3-%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93/%E4%BB%8E%E9%9B%B6%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B03-parser/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>从零写数据库系列-背景知识</title>
    <url>/2023/02/01/2-%E6%95%B0%E6%8D%AE%E5%BA%93/3-%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93/%E4%BB%8E%E9%9B%B6%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B01-%E8%83%8C%E6%99%AF/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>1970 年，IBM的研究员Edgar Frank Codd (TED) 发表了《A Relational Model of Data for Large Shared Data Banks》，这篇论文首次提出了关系模型。</p>
<p>1972年，TED 又提出了关系大事和关系演算的概念。</p>
<p>1974年，IBM的Ray Boyce和Don Chamberlin提出了SQL（Structured Query Language）语言。</p>
<p>有了关系模型和关系代数的理论基础，又有了SQL 这种语言来表达。那么我们可以开始设计数据库系统了。</p>
<p>在关系模型提出后，出现过两个著名的产品，System R 和 Ingres。Ingres使用的是一种叫做 QUEL 的语言，System R使用的就是早期的 SQL。由于 SQL 成为了 ANSI 的标准，所以QUEL 成为了历史。</p>
<span id="more"></span>

<p>有了这些历史背景，现在可以谈谈SQL的执行流程了。</p>
<h2 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h2><p>词法(lexing)，语法(syntax)。</p>
<h2 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h2><p>语意(Semantic)。绑定，类型检查。</p>
<h2 id="Planner"><a href="#Planner" class="headerlink" title="Planner"></a>Planner</h2><p>System R优化器第一次提出了自底向上的动态规划搜索策略，影响了后续的很多系统。另一个创新点在于提出来基于cost-based的优化方法，如何根据sargable条件计算selective，增加了interesting order属性来对访问方法（Access Path）进行影响。</p>
<p>Volcano&#x2F;Cascades</p>
<p>Cascades 的实现，orca，cockroachdb。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *scope)</span></span> endAggFunc(cols opt.ColSet) (g *groupby) &#123;</span><br><span class="line">	<span class="keyword">if</span> !s.inAgg &#123;</span><br><span class="line">		<span class="built_in">panic</span>(errors.AssertionFailedf(<span class="string">&quot;mismatched calls to start/end aggFunc&quot;</span>))</span><br><span class="line">	&#125;</span><br><span class="line">	s.inAgg = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> curr := s; curr != <span class="literal">nil</span>; curr = curr.parent &#123;</span><br><span class="line">		<span class="keyword">if</span> cols.Len() == <span class="number">0</span> || cols.Intersects(curr.colSet()) &#123;</span><br><span class="line">			curr.verifyAggregateContext()</span><br><span class="line">			<span class="keyword">if</span> curr.groupby == <span class="literal">nil</span> &#123;</span><br><span class="line">				curr.initGrouping()</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">return</span> curr.groupby</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">panic</span>(errors.AssertionFailedf(<span class="string">&quot;aggregate function is not allowed in this context&quot;</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>git 忽略文件</title>
    <url>/2023/09/17/10-%E5%B7%A5%E5%85%B7/git/0-brew/</url>
    <content><![CDATA[<p>git 用法记录</p>
<span id="more"></span>



<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。</span><br><span class="line">正确的做法是在每个<span class="built_in">clone</span>下来的仓库中手动设置不要检查特定文件的更改情况。</span><br><span class="line">git update-index --assume-unchanged FILE </span><br><span class="line">在FILE处输入要忽略的文件。</span><br><span class="line">如果要还原的话，使用命令：</span><br><span class="line">git update-index --no-assume-unchanged FILE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pkg/.../manual/manual.go</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>The Many Faces of Consistency</title>
    <url>/2023/10/26/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/The%20Many%20Faces%20of%20Consistency/</url>
    <content><![CDATA[<p><em>The notion of consistency is used across di</em>ff<em>erent computer science disciplines from distributed systems to database systems to computer architecture. It turns out that consistency can mean quite di</em>ff<em>erent things across these disciplines, depending on who uses it and in what context it appears. We identify two broad types of consistency,</em> state consistency <em>and</em> operation consistency*, which di<em>ff</em>er fundamentally in meaning and scope. We explain how these types map to the many examples of consistency in each discipline.*</p>
<p>一致性的概念用于不同的计算机科学学科，从分布式系统到数据库系统再到计算机体系结构。 事实证明，一致性在这些学科中可能意味着完全不同的事情，具体取决于谁使用它以及它出现在什么背景下。 我们确定了两大类一致性：状态一致性和操作一致性，它们在含义和范围上有根本的不同。 我们解释这些类型如何映射到每个学科中的许多一致性示例。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Consistency is an important consideration in computer systems that <em>share</em> and <em>replicate</em> data. Whereas early computing systems had private data exclusively, shared data has become increasingly common as computers have evolved from calculating machines to tools of information exchange. Shared data occurs in many types of systems, from distributed systems to database systems to multiprocessor systems. For example, in distributed systems, users across the network share files (e.g., source code), network names (e.g., DNS entries), data blobs (e.g., images in a key-value store), or system metadata (e.g., configuration information). In database systems, users share tables containing account information, product descriptions, flight bookings, and seat assignments. Within a computer, processor cores share cache lines and physical memory.</p>
<p>一致性是共享和复制数据的计算机系统中的一个重要考虑因素。 早期的计算系统只拥有私有数据，而随着计算机从计算机器发展为信息交换工具，共享数据变得越来越普遍。 共享数据出现在许多类型的系统中，从分布式系统到数据库系统再到多处理器系统。 例如，在分布式系统中，网络上的用户共享文件（例如源代码）、网络名称（例如 DNS 条目）、数据 blob（例如键值存储中的图像）或系统元数据（例如配置） 信息）。 在数据库系统中，用户共享包含帐户信息、产品描述、航班预订和座位分配的表。 在计算机内，处理器核心共享高速缓存行和物理内存。</p>
<p>In addition to sharing, computer systems increasingly replicate data within and across components. In distributed systems, each site may hold a local replica of files, network names, blobs, or system metadata— these replicas, called caches, increase performance of the system. Database systems also replicate rows or tables for speed or to tolerate disasters. Within a computer, parts of memory are replicated at various points in the cache hierarchy (l1, l2, l3 caches), again for speed. We use the term replica broadly to mean any copies of the data maintained by the system.</p>
<p>除了共享之外，计算机系统还越来越多地在组件内部和组件之间复制数据。 在分布式系统中，每个站点都可以保存文件、网络名称、blob 或系统元数据的本地副本 - 这些副本（称为缓存）可以提高系统的性能。 数据库系统还复制行或表以提高速度或容忍灾难。 在计算机内，部分内存会在缓存层次结构（l1、l2、l3 缓存）中的各个点进行复制，同样是为了提高速度。 我们广泛使用术语“副本”来表示系统维护的数据的任何副本。</p>
<p>In all these systems, data sharing and replication raise a fundamental question: what should happen if a client modifies some data items and simultaneously, or within a short time, another client reads or modifies the same items, possibly at a different replica?</p>
<p>在所有这些系统中，数据共享和复制提出了一个基本问题：如果一个客户端修改了某些数据项，并且同时或在短时间内，另一个客户端可能在不同的副本上读取或修改了相同的项目，那么会发生什么情况？</p>
<p>This question does not have a single answer that is right in every context. A consistency property governs the possible outcomes by limiting how data can change or what clients can observe in each case. For example, with DNS, a change to a domain may not be visible for hours; the only guarantee is that updates will be seen eventually—an example of a property called eventual consistency [23]. But with flight seat assignments, updates must be immediate and mutually exclusive, to ensure that no two passengers receive the same seat—an example of a strong type of consistency provided by serializability [5]. Other consistency properties include causal consis- tency [13], read-my-writes [21], bounded staleness [1], continuous consistency [1, 25], release consistency [10], fork consistency [16], epsilon serializability [18], and more.</p>
<p>这个问题没有一个在所有情况下都正确的答案。 一致性属性通过限制数据的更改方式或客户端在每种情况下可以观察到的内容来控制可能的结果。 例如，对于 DNS，对域的更改可能在数小时内不可见； 唯一的保证是最终会看到更新——一个称为最终一致性的属性的例子[23]。 但对于航班座位分配，更新必须是立即且互斥的，以确保没有两名乘客获得相同的座位——这是可串行性提供的强一致性的一个例子 [5]。 其他一致性属性包括因果一致性 [13]、读我的写 [21]、有界陈旧性 [1]、连续一致性 [1, 25]、发布一致性 [10]、分叉一致性 [16]、epsilon 可串行性 [ 18]等等。</p>
<p>Consistency is important because developers must understand the answer to the above fundamental question. This is especially true when the clients interacting with the system are not humans but other computer programs that must be coded to deal with all possible outcomes.</p>
<p>一致性很重要，因为开发人员必须理解上述基本问题的答案。 当与系统交互的客户端不是人类而是必须编码以处理所有可能结果的其他计算机程序时尤其如此。</p>
<p>In this article, we examine many examples of how consistency is used across three computer science disci- plines: distributed systems, database systems, and computer architecture. We find that the use of consistency varies significantly across these disciplines. To bring some clarity, we identify two fundamentally different types of consistency: <font color="red">state consistency and operation consistency</font>&gt;. State consistency concerns the state of the system and establishes constraints on the allowable relationships between different data items or different replicas of the same items. For instance, state consistency might require that two replicas store the same value when updates are not outstanding. Operation consistency concerns operations on the system and establishes constraints on what results they may return. For instance, operation consistency might require that a read of a file reflects the contents of the most recent write on that file. State consistency tends to be simpler and application dependent, while operation consistency tends to be more complex and application agnostic. Both types of consistency are important and, in our opinion, our communities should more clearly disentangle them.</p>
<p>在本文中，我们研究了如何在三个计算机科学学科（分布式系统、数据库系统和计算机体系结构）中使用一致性的许多示例。 我们发现，在这些学科中，一致性的使用存在显着差异。 为了清楚起见，我们确定了两种根本不同类型的一致性：状态一致性和操作一致性。 状态一致性涉及系统的状态，并对不同数据项或相同项的不同副本之间允许的关系建立约束。 例如，状态一致性可能要求两个副本在更新未完成时存储相同的值。 操作一致性涉及系统上的操作，并对它们可能返回的结果建立约束。 例如，操作一致性可能要求文件的读取反映该文件最近写入的内容。 状态一致性往往更简单且依赖于应用程序，而操作一致性往往更复杂且与应用程序无关。 这两种类型的一致性都很重要，我们认为，我们的社区应该更清楚地理清它们。</p>
<p>While this article discusses different forms of consistency, it focuses on the <em>semantics</em> of consistency rather than the <em>mechanisms</em> of consistency. Semantics refer to what consistency properties the system provides, while mechanisms refer to how the system enforces those properties. Semantics and mechanisms are closely related but it is important to understand the former without needing to understand the latter.</p>
<p>虽然本文讨论了不同形式的一致性，但它重点关注一致性的语义而不是一致性的机制。 语义是指系统提供哪些一致性属性，而机制是指系统如何强制执行这些属性。 语义和机制密切相关，但重要的是理解前者而不需要理解后者。</p>
<p>The rest of this article is organized as follows. We first explain the abstract system model and terminology used throughout the article in Section 2. We present the two types of consistency and their various embodiments in Section 3. We indicate how these consistency types occur across different disciplines in Section 4.</p>
<p>本文的其余部分组织如下。 我们首先在第 2 节中解释整篇文章中使用的抽象系统模型和术语。我们在第 3 节中介绍了两种类型的一致性及其各种实施例。我们在第 4 节中指出了这些一致性类型如何在不同学科中发生。</p>
<h2 id="2-Abstract-model"><a href="#2-Abstract-model" class="headerlink" title="2 Abstract model"></a>2 Abstract model</h2><p>We consider a setting with multiple <em>clients</em> that submit <em>operations</em> to be executed by the system. Clients could be human users, computer programs, or other systems that do not concern us. Operations might include simple read and write, read-modify-write, start and commit a transaction, and range queries. Operations typically act on data items, which could be blocks, files, key-value pairs, DNS entries, rows of tables, memory locations, and so on.</p>
<p>我们考虑具有多个客户端的设置，这些客户端提交要由系统执行的操作。 客户端可以是人类用户、计算机程序或与我们无关的其他系统。 操作可能包括简单的读取和写入、读取-修改-写入、启动和提交事务以及范围查询。 操作通常作用于数据项，这些数据项可以是块、文件、键值对、DNS 条目、表行、内存位置等。</p>
<p>The system has a <em>state</em>, which includes the current values of the data items. In some cases, we are interested in the consistency of client caches and other replicas. In these cases, the caches and other replicas are considered to be part of the system and the system state includes their contents.</p>
<p>系统具有状态，其中包括数据项的当前值。 在某些情况下，我们对客户端缓存和其他副本的一致性感兴趣。 在这些情况下，缓存和其他副本被视为系统的一部分，并且系统状态包括它们的内容。</p>
<p>An operation execution is not instantaneous; rather, it <em>starts</em> when a client submits the operation, and it <em>finishes</em> when the client obtains its response from the system. If the operation execution returns no response, then it finishes when the system is no longer actively processing it.</p>
<p>操作执行不是即时的； 相反，它在客户端提交操作时开始，在客户端从系统获得响应时结束。 如果操作执行没有返回响应，则当系统不再主动处理它时，操作就会完成。</p>
<p>Operations are distinct from operation executions. Operations are static and a system has relatively few of them, such as read and write. Operation executions, on the other hand, are dynamic and numerous. A client can execute the same operation many times, but each operation execution is unique. While technically we should separate operations from operation executions, we often blur the distinction when it is clear from the context (e.g., we might say that the read operation finishes, rather than the execution of the read operation finishes).</p>
<p>操作与操作执行不同。 操作是静态的，系统中的操作相对较少，例如读和写。 另一方面，操作执行是动态的且数量众多。 一个客户端可以多次执行相同的操作，但每次操作的执行都是唯一的。 虽然从技术上讲，我们应该将操作与操作执行分开，但当上下文清楚时，我们经常会模糊区别（例如，我们可能会说读操作完成，而不是读操作的执行完成）。</p>
<h2 id="3-Two-types-of-consistency"><a href="#3-Two-types-of-consistency" class="headerlink" title="3 Two types of consistency"></a>3 Two types of consistency</h2><p>We are interested in what happens when shared and replicated data is accessed concurrently or nearly con- currently by many clients. Generally speaking, consistency places constraints on the allowable outcomes of operations, according to the needs of the application. We now define two broad types of consistency. One places constraints on the state, the other on the results of operations.</p>
<p>我们感兴趣的是当许多客户端同时或几乎同时访问共享和复制的数据时会发生什么。 一般来说，一致性根据应用程序的需要对操作的允许结果施加限制。 我们现在定义两种广泛的一致性类型。 一是对 状态施加限制，二是对操作结果施加限制。</p>
<h3 id="3-1-State-consistency"><a href="#3-1-State-consistency" class="headerlink" title="3.1 State consistency"></a>3.1 State consistency</h3><p>State consistency pertains to the state of the system; it consists of properties that users expect the state to satisfy despite concurrent access and the existence of multiple replicas. State consistency is also applicable when data can be corrupted by errors (crashes, bit flips, bugs, etc), but this is not the focus of this article.</p>
<p>状态一致性与系统的状态有关； 它由用户期望状态满足的属性组成，尽管存在并发访问和存在多个副本。 当数据可能因错误（崩溃、位翻转、错误等）而损坏时，状态一致性也适用，但这不是本文的重点。</p>
<p>State consistency can be of many subcategories, based on how the properties of state are expressed. We explain these subcategories next.</p>
<p>根据状态属性的表达方式，状态一致性可以分为许多子类别。 接下来我们将解释这些子类别。</p>
<h4 id="3-1-1-Invariants-不变量"><a href="#3-1-1-Invariants-不变量" class="headerlink" title="3.1.1 Invariants 不变量"></a>3.1.1 Invariants 不变量</h4><p>The simplest subcategory of state consistency is one defined by an invariant—a predicate on the state that must evaluate to true. For instance, in a concurrent program, a singly linked list must not contain cycles. In a multiprocessor system, if the local caches of two processors keep a value for some address, it must be the same value. In a social network, if user x is a friend of user y then y is a friend of x. In a photo sharing application, if a photo album includes an image then the image’s owner is the album.</p>
<p>状态一致性最简单的子类别是由不变量定义的，即状态上必须评估为 true 的谓词。 例如，在并发程序中，单链表不能包含循环。 在多处理器系统中，如果两个处理器的本地缓存保存某个地址的值，则该值必须是相同的值。 在社交网络中，如果用户 x 是用户 y 的朋友，则 y 也是 x 的朋友。 在照片共享应用程序中，如果相册包含图像，则该图像的所有者就是相册。</p>
<p>In database systems, two important examples are uniqueness constraints and referential integrity. A <em>unique- ness constraint</em> on a column of a table requires that each value appearing in that column must occur in at most one row. This property is crucial for the primary keys of tables.</p>
<p>在数据库系统中，两个重要的例子是唯一性约束和引用完整性。 表的列的唯一性约束要求该列中出现的每个值必须最多出现在一行中。 该属性对于表的主键至关重要。</p>
<p><em>Referential integrity</em> concerns a table that refers to keys of another table. Databases may store relations between tables by including keys of a table within columns in another table. Referential integrity requires that the included keys are indeed keys in the first table. For instance, in a bank database, suppose that an accounts table includes a column for the account owner, which is a user id; meanwhile, the user id is the primary key in a users table, which has detailed information for each user. A referential integrity constraint requires that user ids in the accounts table must indeed exist in the users table.</p>
<p>参照完整性涉及引用另一个表的键的表。 数据库可以通过将一个表的键包含在另一个表的列中来存储表之间的关系。 参照完整性要求包含的键确实是第一个表中的键。 例如，在银行数据库中，假设帐户表包含帐户所有者的列，即用户 ID； 同时，用户id是用户表中的主键，该表包含每个用户的详细信息。 参照完整性约束要求帐户表中的用户 ID 必须确实存在于用户表中。</p>
<p>Another example of state consistency based on invariants is <em>mutual consistency</em>, used in distributed systems that are replicated using techniques such as primary-backup [2]. Mutual consistency requires that replicas have the same state when there are no outstanding updates. During updates, replicas may diverge temporarily since the updates are not applied simultaneously at all replicas.</p>
<p>基于不变量的状态一致性的另一个例子是相互一致性，用于使用主备份等技术进行复制的分布式系统[2]。 相互一致性要求副本在没有未完成的更新时具有相同的状态。 在更新期间，副本可能会暂时出现分歧，因为更新不会同时应用于所有副本。</p>
<h4 id="3-1-2-Error-bounds-误差范围"><a href="#3-1-2-Error-bounds-误差范围" class="headerlink" title="3.1.2 Error bounds 误差范围"></a>3.1.2 Error bounds 误差范围</h4><p>If the state contains numerical data, the consistency property could indicate a maximum deviation or error from the expected. For instance, the values at two replicas may diverge by at most ε. In an internet-of-things system, the reported value of a sensor, such as a thermometer, must be within ε from the actual value being measured. This example relates the state of the system to the state of the world. Error bounds were first proposed within the database community [1] and the basic idea was later revived in the distributed systems community [25].</p>
<p>如果状态包含数值数据，则一致性属性可能表示与预期的最大偏差或错误。 例如，两个副本上的值最多可能会差异ε。 在图Internet系统中，传感器的报告值（例如温度计）必须在ε内，从实际的值中。 这个示例将系统状态与世界状态联系起来。 误差界最初是在数据库社区中提出的[1]，后来在分布式系统社区中恢复了基本思想[25]。</p>
<h4 id="3-1-3-Limits-on-proportion-of-violations-违规比例限制"><a href="#3-1-3-Limits-on-proportion-of-violations-违规比例限制" class="headerlink" title="3.1.3 Limits on proportion of violations 违规比例限制"></a>3.1.3 Limits on proportion of violations 违规比例限制</h4><p>If there are many properties or invariants, it may be unrealistic to expect all of them to hold, but rather just a high percentage. For instance, the system may require that at most one user’s invariants are violated in a pool of a million users; this could make sense if the system can compensate a small fraction of users for inconsistencies in their data.</p>
<p>如果存在许多属性或不变量，则期望它们全部成立而只是高百分比成立可能是不现实的。 例如，系统可能要求在一百万个用户的池中最多违反一个用户的不变量； 如果系统可以补偿一小部分用户的数据不一致，那么这可能是有意义的。</p>
<h4 id="3-1-4-Importance-重要性"><a href="#3-1-4-Importance-重要性" class="headerlink" title="3.1.4 Importance 重要性"></a>3.1.4 Importance 重要性</h4><p>Properties or invariants may be critical, important, advisable, desirable, or optional, where users expect only the critical properties to hold at all times. Developers can use more expensive and effective mechanisms for the more important invariants. For instance, when a user changes her password at a web site, the system might require all replicas of the user account to have the same password before acknowledging the change to the user. This property is implemented by contacting all replicas and waiting for replies, which can be an overly expensive mechanism for less important properties.</p>
<p>属性或不变量可能是关键的、重要的、可取的、理想的或可选的，其中用户期望始终只保留关键属性。 开发人员可以针对更重要的不变量使用更昂贵和更有效的机制。 例如，当用户在网站上更改密码时，系统可能要求该用户帐户的所有副本都具有相同的密码，然后再向用户确认更改。 该属性是通过联系所有副本并等待回复来实现的，对于不太重要的属性来说，这可能是一种过于昂贵的机制。</p>
<h4 id="3-1-5-Eventual-invariants-最终不变量"><a href="#3-1-5-Eventual-invariants-最终不变量" class="headerlink" title="3.1.5 Eventual invariants 最终不变量"></a>3.1.5 Eventual invariants 最终不变量</h4><p>An invariant may need to hold only after some time has passed. For example, under eventual consistency, replicas need not be the same at all times, as long as they <em>eventually</em> become the same when updates stop occurring. This eventual property is appropriate because replicas may be updated in the background or using some anti-entropy mechanism, where it takes an indeterminate amount of time for a replica to receive and process an update. Eventual consistency was coined by the distributed systems community [23], though the database community previously proposed the idea of reconciling replicas that diverge during partitions [9].</p>
<p>不变量可能只需要在经过一段时间后才保持不变。 例如，在最终一致性下，副本不需要始终相同，只要当更新停止发生时它们最终变得相同即可。 此最终属性是合适的，因为副本可以在后台或使用某种反熵机制进行更新，其中副本接收和处理更新需要不确定的时间。 最终一致性是由分布式系统社区[23]创造的，尽管数据库社区之前提出了协调分区期间分歧的副本的想法[9]。</p>
<p>State consistency is limited to properties on state, but in many cases clients care less about the state and more about the results that they obtain from the system. In other words, what matters is the behavior that clients observe from interacting with the system. These cases call for a different form of consistency, which we discuss next.</p>
<p>状态一致性仅限于状态的属性，但在许多情况下，客户端不太关心状态，而更关心他们从系统获得的结果。 换句话说，重要的是客户通过与系统交互观察到的行为。 这些情况需要不同形式的一致性，我们接下来讨论。</p>
<h3 id="3-2-Operation-consistency"><a href="#3-2-Operation-consistency" class="headerlink" title="3.2 Operation consistency"></a>3.2 Operation consistency</h3><p>Operation consistency pertains to the operation executions by clients; it consists of properties that indicate whether operations return acceptable results. These properties can tie together many operation executions, as shown in the examples below.</p>
<p>操作一致性涉及客户端执行的操作； 它由指示操作是否返回可接受的结果的属性组成。 这些属性可以将许多操作执行联系在一起，如下面的示例所示。</p>
<p>Operation consistency has subcategories, with different ways to define the consistency property. We explain these subcategories next.</p>
<p>操作一致性有不同的子类别，有不同的方法来定义一致性属性。 接下来我们将解释这些子类别。</p>
<h4 id="3-2-1-Sequential-equivalence-顺序等价"><a href="#3-2-1-Sequential-equivalence-顺序等价" class="headerlink" title="3.2.1 Sequential equivalence 顺序等价"></a>3.2.1 Sequential equivalence 顺序等价</h4><p>This subcategory defines the permitted operation results of a concurrent execution in terms of the permitted oper- ation results in a sequential execution—one in which operations are executed one at a time, without concurrency. More specifically, there must be a way to take the execution of all operations submitted by any subset of clients, and then <em>reduce</em> them to a sequential execution that is correct. The exact nature of the reduction depends on the specific consistency property. Technically, the notion of a correct sequential execution is system dependent, so it needs to be specified as well, but it is often obvious and therefore omitted.</p>
<p>该子类别根据顺序执行（一次执行一个操作，无并发）中允许的操作结果来定义并发执行的允许操作结果。 更具体地说，必须有一种方法来执行任何客户端子集提交的所有操作，然后将它们简化为正确的顺序执行。 减少的确切性质取决于特定的一致性属性。 从技术上讲，正确顺序执行的概念取决于系统，因此也需要指定它，但它通常是显而易见的，因此被省略。</p>
<p>We now give some examples of sequential equivalence. 现在我们给出一些顺序等价的例子。</p>
<p><font color="red"><em>Linearizability [12]</em> is a strong form of consistency.</font> Intuitively, the constraint is that each operation must appear to occur at an instantaneous point between its start and finish times, where execution at these instanta- neous points form a valid sequential execution. More precisely, we define a partial order &lt; from the concurrent execution, as follows: <em>op</em>1 &lt; <em>op</em>2 iff <em>op</em>1 finishes before <em>op</em>2 starts. There must exist a legal total order <em>T</em> of all operations with their results, such that (1) <em>T</em> is consistent with &lt;, meaning that if <em>op</em>1 &lt; <em>op</em>2 then <em>op</em>1 appears before <em>op</em>2 in <em>T</em>, and (2) <em>T</em> defines a correct sequential execution. Linearizability has been traditionally used to define the correct behavior of concurrent data structures; more recently, it has also been used in distributed systems.</p>
<p>线性化[12]是一致性的一种强形式。 直观上，约束是每个操作必须出现在其开始时间和结束时间之间的瞬时点，其中在这些瞬时点的执行形成有效的顺序执行。 更准确地说，我们从并发执行中定义一个偏序 &lt; ，如下所示： op1 &lt; op2 iff op1 在 op2 开始之前完成。 所有操作及其结果必须存在合法的全序 T，使得 (1) T 与 &lt; 一致，这意味着如果 op1 &lt; op2 则 op1 在 T 中出现在 op2 之前，并且 (2) T 定义了正确的顺序执行 。 线性化传统上被用来定义并发数据结构的正确行为。 最近，它也被用于分布式系统。</p>
<p><em>Sequential consistency [14]</em> is also a strong form of consistency, albeit weaker than linearizability. Intuitively, it requires that operations execute as if they were totally ordered in a way that respects the order in which each client issues operations. More precisely, we define a partial order &lt; as follows: <em>op</em>1 &lt; <em>op</em>2 iff both operations are executed by the same client and <em>op</em>1 finishes before <em>op</em>2 starts. There must exist a total order <em>T</em> such that (1) <em>T</em> is consistent with &lt;, and (2) <em>T</em> defines a correct sequential execution. These conditions are similar to linearizability, except that &lt; reflects just the local order of operations at each client. Sequential consistency is used to define a strongly consistent memory model of a computer, but it could also be used in the context of concurrent data structures.</p>
<p>顺序一致性[14]也是一致性的一种强形式，尽管比线性化弱。 直观上，它要求操作执行时就好像它们是完全有序的，并且尊重每个客户端发出操作的顺序。 更准确地说，我们定义一个偏序 &lt; 如下： op1 &lt; op2 当且仅当两个操作都由同一客户端执行并且 op1 在 op2 开始之前完成。 必须存在一个全序 T，使得 (1) T 与 &lt; 一致，并且 (2) T 定义了正确的顺序执行。 这些条件类似于线性化能力，除了 &lt; 只反映每个客户端的本地操作顺序。 顺序一致性用于定义计算机的强一致性内存模型，但它也可以用在并发数据结构的上下文中。</p>
<p>The next examples pertain to systems that support <em>transactions</em>. Intuitively, a transaction is a bundle of one or more operations that must be executed as a whole. More precisely, there are special operations to start, commit, and abort transactions; and operations on data items are associated with a transaction. The system provides an isolation property, which ensures that transactions do not significantly interfere with one another. There are many isolation properties: serializability, strong session serializability, order-preserving serializability, snapshot isolation, read committed, repeatable reads, etc. All of these are forms of operation consistency, and several of them are of the sequential equivalence subcategory. Here are some examples, all of which are used in the context of database systems.</p>
<p>下一个示例涉及支持事务的系统。 直观上，事务是一组必须作为一个整体执行的一个或多个操作。 更准确地说，有一些特殊的操作来启动、提交和中止事务； 对数据项的操作与事务相关联。 该系统提供了一种隔离属性，可确保事务不会严重干扰彼此。 隔离属性有很多：可序列化、强会话可序列化、保序可序列化、快照隔离、已提交读、可重复读等。所有这些都是操作一致性的形式，其中一些属于顺序等效子类别。 以下是一些示例，所有这些示例都在数据库系统的上下文中使用。</p>
<p><em>Serializability [5]</em> intuitively guarantees that each transaction appears to execute in series. More precisely, serializability imposes a constraint on the operations in a system: the schedule corresponding to those operations must be equivalent to a serial schedule of transactions. The serial schedule is called a serialization of the schedule.</p>
<p>可串行性[5]直观地保证每个事务看起来都是串行执行的。 更准确地说，可串行性对系统中的操作施加了约束：与这些操作相对应的调度必须等于事务的串行调度。 串行调度称为调度的串行化。</p>
<p><em>Strong session serializability [8]</em> addresses an issue with serializability. Serializability allows transactions of the same client to be reordered, which can be undesirable at times. Strong session serializability imposes additional constraints on top of serializability. More precisely, each transaction is associated with a session, and the constraint is that serializability must hold (as defined above) and the serialization must respect the order of transactions within every session: if transaction <em>T</em>1 occurs before <em>T</em>2 in the same session, then <em>T</em>2 is not serialized before <em>T</em>1.</p>
<p>强大的会话可串行性[8]解决了可串行性的问题。 可串行性允许对同一客户端的事务进行重新排序，这有时是不可取的。 强大的会话可串行性在可串行性之上施加了额外的约束。 更准确地说，每个事务都与一个会话关联，并且约束是可串行性必须保持（如上面所定义），并且序列化必须遵守每个会话中事务的顺序：如果事务 T1 在同一会话中发生在 T2 之前，则 T2 在 T1 之前未序列化。</p>
<p><em>Order-preserving serializability [24]</em>, also called strict serializability [6, 17] or strong serializability [7], requires that the serialization order respect the real-time ordering of transactions. More precisely, the constraint is that serializability must hold and the serialization must satisfy the requirement that, if transaction <em>T</em>1 commits before <em>T</em>2 starts, then <em>T</em>2 is not serialized before <em>T</em>1.</p>
<p>保序序列化[24]，也称为严格序列化[6, 17]或强序列化[7]，要求序列化顺序尊重事务的实时排序。 更准确地说，约束是可串行性必须成立，并且串行化必须满足以下要求：如果事务 T1 在 T2 开始之前提交，则 T2 不会在 T1 之前序列化。</p>
<h4 id="3-2-2-Reference-equivalence-参考等效"><a href="#3-2-2-Reference-equivalence-参考等效" class="headerlink" title="3.2.2 Reference equivalence 参考等效"></a>3.2.2 Reference equivalence 参考等效</h4><p>Reference equivalence is a generalization of sequential equivalence. It defines the permitted operation results by requiring the concurrent execution to be equivalent to a given reference, where the notion of equivalence and the reference depend on the consistency property. We now give some examples for systems with transactions. These examples occur often in the context of database systems.</p>
<p>参考等价是顺序等价的推广。 它通过要求并发执行等效于给定的引用来定义允许的操作结果，其中等效的概念和引用取决于一致性属性。 我们现在给出一些具有事务的系统的示例。 这些例子经常出现在数据库系统的环境中。</p>
<p><em>Snapshot isolation [4]</em> requires that transactions behave identically to a certain reference implementation, that is, transactions must have the same outcome as in the reference implementation, and operations must return the same results. The reference implementation is as follows. When a transaction starts, it gets assigned a monotonic start timestamp. When the transaction reads data, it reads from a snapshot of the system as of the start timestamp. When a transaction <em>T</em>1 wishes to commit, the system obtains a monotonic commit timestamp and verifies whether there is some other transaction <em>T</em>2 such that (1) <em>T</em>2 updates some item that <em>T</em>1 also updates, and (2) <em>T</em>2 has committed with a commit timestamp between <em>T</em>1’s start and commit timestamp. If so, then <em>T</em>1 is aborted; otherwise, <em>T</em>1 is committed and all its updates are applied instantaneously as of the time of <em>T</em>1’s commit timestamp.</p>
<p>快照隔离[4]要求事务的行为与某个参考实现相同，即事务必须具有与参考实现相同的结果，并且操作必须返回相同的结果。 参考实现如下。 当事务开始时，它会被分配一个单调的开始时间戳。 当事务读取数据时，它从截至开始时间戳的系统快照中读取。 当事务 T1 希望提交时，系统获取单调提交时间戳并验证是否存在其他事务 T2，以便 (1) T2 更新 T1 也更新的某些项目，以及 (2) T2 已提交，且提交时间戳介于 T1 的开始和提交时间戳。 如果是，则 T1 中止； 否则，T1 将被提交，并且自 T1 的提交时间戳记起立即应用其所有更新。</p>
<p>Interestingly, the next two properties are examples of reference equivalence where the reference is itself defined by another consistency property. This other property is in the serial equivalence subcategory in the first example, and it is in the reference equivalence subcategory in the second example.</p>
<p>有趣的是，接下来的两个属性是引用等效的示例，其中引用本身是由另一个一致性属性定义的。 该其他属性在第一个示例中位于序列等效子类别中，在第二个示例中位于参考等效子类别中。</p>
<p><em>One-copy serializability [5]</em> pertains to a replicated database system. The replicated system must behave like a reference system, which is a system that is not replicated and provides serializability.</p>
<p>单副本可串行化[5]适用于复制数据库系统。 复制系统的行为必须类似于参考系统，参考系统是一个不复制并提供可串行性的系统。</p>
<p><em>One-copy snapshot isolation [15]</em> also pertains to a replicated system. The requirement is that it must behave like a system that is not replicated and that provides snapshot isolation.</p>
<p>单副本快照隔离[15]也适用于复制系统。 要求是它的行为必须像一个不复制且提供快照隔离的系统。</p>
<h4 id="3-2-3-Read-write-centric-以读写为中心"><a href="#3-2-3-Read-write-centric-以读写为中心" class="headerlink" title="3.2.3 Read-write centric 以读写为中心"></a>3.2.3 Read-write centric 以读写为中心</h4><p>The above subcategories of operation consistency apply to systems with arbitrary operations. The read-write centric subcategory applies to systems with two very specific operations: read and write. These systems are important because they include many types of storage systems, such as block storage systems, key value storage systems, and processors accessing memory. By focusing on the two operations, this subcategory permits prop- erties that directly evoke the semantics of the operations. In particular, a write operation returns no information other than an acknowledgment or error status, which has no consistency implications. Thus, the consistency properties focus on the results of reads. Common to these properties is the notion of a read <em>seeing</em> the values of a set of writes, as we now explain. Each read is affected by some writes in the system; if every write covers the entire data item, then writes overwrite each other and the read returns the value written by one of them. But if the writes update just part of a data item, the read returns a combination of the written values in some appropriate order. In either case, the crucial consideration is the set of writes that <em>could</em> have potentially affected the read, irrespective of whether the writes are partial or not; we say that the read <em>sees</em> those writes. This notion is used to define several known consistency properties, as we now exemplify.</p>
<p>上述操作一致性的子类别适用于具有任意操作的系统。 以读写为中心的子类别适用于具有两个非常具体的操作的系统：读和写。 这些系统很重要，因为它们包括许多类型的存储系统，例如块存储系统、键值存储系统和访问内存的处理器。 通过关注这两个操作，该子类别允许直接唤起操作语义的属性。 特别是，写操作除了确认或错误状态之外不返回任何信息，这没有一致性影响。 因此，一致性属性集中于读取的结果。 正如我们现在所解释的，这些属性的共同点是读取看到一组写入的值的概念。 每次读都会受到系统中一些写的影响； 如果每次写入都覆盖整个数据项，则写入会相互覆盖，并且读取会返回其中之一写入的值。 但是，如果写入仅更新数据项的一部分，则读取将按某种适当的顺序返回写入值的组合。 无论哪种情况，关键的考虑因素是可能影响读取的一组写入，无论写入是否部分； 我们说读可以看到那些写。 正如我们现在举例的那样，这个概念用于定义几个已知的一致性属性。</p>
<p><em>Read-my-writes [21]</em> requires that a read by a client sees at least all writes previously executed by the same client, in the order in which they were executed. This property is relevant when clients expect to observe their own writes, but can tolerate delays before observing the writes of others. Typically, read-my-writes is combined with another read-write consistency property, such as bounded staleness or operational eventual consistency, defined below. By combined we mean that the system must provide both read-my-writes and the other prop- erty. Read-my-writes was originally defined in the context of distributed systems [21], then used in computer architecture to define memory models [19].</p>
<p>Read-my-writes [21] 要求客户端的读取至少可以看到同一客户端先前执行的所有写入（按执行顺序排列）。 当客户端期望观察自己的写入，但在观察其他人的写入之前可以容忍延迟时，此属性是相关的。 通常，read-my-writes 与另一个读写一致性属性相结合，例如下面定义的有限陈旧性或操作最终一致性。 通过组合，我们的意思是系统必须提供“读我的写”和其他属性。 Read-my-writes 最初是在分布式系统的上下文中定义的 [21]，然后在计算机体系结构中用于定义内存模型 [19]。</p>
<p><em>Bounded staleness [1]</em>, intuitively, bounds the time it takes for writes to be seen by reads. More precisely, the property has a parameter δ, such that a read must see at least all writes that complete δ time before the read started. This property is relevant when inconsistencies are tolerable in the short term as defined by δ, or when time intervals smaller than δ are imperceptible by clients (e.g., δ is in the tens of milliseconds and clients are humans). Bounded staleness was originally defined in the context of database systems [1] and has been used more recently in the context of cloud distributed systems [20].</p>
<p>直观上讲，有界陈旧性 [1] 限制了读取看到写入所需的时间。 更准确地说，该属性有一个参数 δ，这样读取必须至少看到在读取开始之前完成 δ 时间的所有写入。 当 δ 定义的短期内可以容忍不一致时，或者当客户无法察觉小于 δ 的时间间隔时（例如，δ 为数十毫秒且客户是人类），此属性是相关的。 有界过时性最初是在数据库系统 [1] 的上下文中定义的，最近已在云分布式系统 [20] 的上下文中使用。</p>
<p><em>Operational eventual consistency</em> is a variant of eventual consistency (a form of state consistency) defined using operation consistency. The requirement is that each write be eventually seen by all reads, and if clients stop executing writes then eventually every read returns the same latest value [22].</p>
<p>操作最终一致性是使用操作一致性定义的最终一致性（状态一致性的一种形式）的变体。 要求是每次写入最终都会被所有读取看到，并且如果客户端停止执行写入，则最终每次读取都会返回相同的最新值[22]。</p>
<p><em>Cache coherence</em> originates from computer architecture to define the correct behavior of a memory cache. Intuitively, cache coherence requires that reads and writes to an individual data item (a memory location) satisfy some properties. The properties vary across the literature. One possibility [11] is to require that, for each data item: (1) a read by some client returns the value of the previous write by that client, unless another client has written in between, (2) a read returns the value of a write by another client if the write and read are sufficiently separated in time and if no other write occurred in between, and (3) writes are serialized.</p>
<p>缓存一致性源自计算机体系结构，用于定义内存缓存的正确行为。 直观上，缓存一致性要求对单个数据项（内存位置）的读取和写入满足某些属性。 各个文献的特性各不相同。 一种可能性[11]是要求，对于每个数据项：（1）某个客户端的读取返回该客户端先前写入的值，除非另一个客户端在其间写入，（2）读取返回该值 如果写入和读取在时间上充分分离并且中间没有发生其他写入，则由另一个客户端进行写入，并且 (3) 写入被串行化。</p>
<h3 id="3-3-Discussion"><a href="#3-3-Discussion" class="headerlink" title="3.3 Discussion"></a>3.3 Discussion</h3><p>We now compare state consistency and operation consistency in terms of their level of abstraction, complexity, power, and application dependence.</p>
<p>现在，我们从抽象级别、复杂性、功能和应用程序依赖性方面比较状态一致性和操作一致性。</p>
<h4 id="3-3-1-Level-of-abstraction"><a href="#3-3-1-Level-of-abstraction" class="headerlink" title="3.3.1 Level of abstraction"></a>3.3.1 Level of abstraction</h4><p>Operation consistency is an end-to-end property, because it deals with results that clients can observe directly. This is in contrast to state consistency, which deals with system state that clients observe indirectly by executing operations. In other words, operation consistency is at a higher level of abstraction than state consistency. As a result, a system might have significant state inconsistencies, but hide these inconsistencies externally to provide a strong form of operation consistency.</p>
<p>操作一致性是一个端到端的属性，因为它处理的是客户端可以直接观察到的结果。 这与状态一致性相反，状态一致性处理客户端通过执行操作间接观察到的系统状态。 换句话说，操作一致性比状态一致性处于更高的抽象级别。 因此，系统可能存在严重的状态不一致，但将这些不一致隐藏在外部以提供强大的操作一致性形式。</p>
<p>An interesting example is a storage system with three servers replicated using majority quorums [3], where (1) to write data, the system attaches a monotonic timestamp and stores the data at two (a majority of) servers, and (2) to read, the system fetches the data from two servers; if the servers return the same data, the system returns the data to the client; otherwise, the system picks the data with the highest timestamp, stores that data and its timestamp in another server (to ensure that two servers have the data), and returns the data to the client. This system violates mutual consistency, because when there are no outstanding operations, one of the servers deviates from the other two. However, this inconsistency is not observable in the results returned by reads, since a read filters out the inconsistent server by querying a majority. In fact, this storage system satisfies linearizability, one of the strongest forms of operation consistency.</p>
<p>一个有趣的例子是一个具有使用多数仲裁[3]复制的三台服务器的存储系统，其中（1）写入数据，系统附加单调时间戳并将数据存储在两台（大多数）服务器上，以及（2） read，系统从两台服务器获取数据； 如果服务器端返回的数据相同，则系统将数据返回给客户端； 否则，系统选择具有最高时间戳的数据，将该数据及其时间戳存储在另一台服务器中（以确保两台服务器都有该数据），并将数据返回给客户端。 该系统违反了相互一致性，因为当没有未完成的操作时，其中一台服务器会偏离其他两台服务器。 然而，这种不一致在读取返回的结果中是观察不到的，因为读取通过查询多数来过滤掉不一致的服务器。 事实上，该存储系统满足线性化，这是操作一致性的最强形式之一。</p>
<h4 id="3-3-2-Complexity"><a href="#3-3-2-Complexity" class="headerlink" title="3.3.2 Complexity"></a>3.3.2 Complexity</h4><p>Operation consistency is more complex than state consistency. With state consistency, developers gain a direct understanding of what states they can expect from the system. Each property concerns specific data items that do not depend on the execution. As a result, state consistency is intuitive and simple to express and under- stand. Moreover, state consistency can be checked by analyzing a snapshot of the system state, which facilitates debugging.</p>
<p>操作一致性比状态一致性更复杂。 通过状态一致性，开发人员可以直接了解他们可以从系统中获得哪些状态。 每个属性都涉及不依赖于执行的特定数据项。 因此，状态一致性直观且易于表达和理解。 而且，可以通过分析系统状态快照来检查状态一致性，从而方便调试。</p>
<p>By contrast, operation consistency properties establish relations between operations that are spread over time and possibly over many clients, which creates complexity. This complexity makes operation consistency less intuitive and harder to understand, as can be observed from the examples in Section 3.2. Moreover, checking operation consistency requires analyzing an execution log, which complicates debugging.</p>
<p>相比之下，操作一致性属性在随时间分布并且可能分布在许多客户端的操作之间建立关系，这会产生复杂性。 这种复杂性使得操作一致性变得不那么直观并且更难以理解，从 3.2 节中的示例可以看出。 而且，检查操作一致性需要分析执行日志，这使得调试变得复杂。</p>
<h4 id="3-3-3-Power"><a href="#3-3-3-Power" class="headerlink" title="3.3.3 Power"></a>3.3.3 Power</h4><p>Operation consistency and state consistency have different powers. Operation consistency can see all operations in the system, which permits constraining the ordering and results of operations. If the system is deterministic, operation consistency properties can reconstruct the state of the system from the operations, and thereby indi- rectly constrain the state much like state consistency. But doing so is not generally possible when the system is non-deterministic (e.g., due to concurrency, timing, or external events).</p>
<p>操作一致性和状态一致性具有不同的权力。 操作一致性可以看到系统中的所有操作，这允许约束操作的顺序和结果。 如果系统是确定性的，操作一致性属性可以从操作中重建系统的状态，从而像状态一致性一样间接约束状态。 但当系统不确定时（例如，由于并发、计时或外部事件），这样做通常是不可能的。</p>
<p>State consistency, on the other hand, can see the entire state of the system, which permits constraining operations that might break the state. If the system records all its operations in its state, then state consistency can indirectly constrain the results of operations much like operation consistency.1 However, it is often prohibitive to record all operations so this is only a theoretical capability.</p>
<p>另一方面，状态一致性可以看到系统的整个状态，这允许限制可能破坏状态的操作。 如果系统在其状态中记录其所有操作，则状态一致性可以间接约束操作的结果，就像操作一致性一样。[^1] 但是，记录所有操作通常是禁止的，因此这只是一种理论上的能力。</p>
<p>[^1]: It is even possible to constrain all operations of the entire execution, though enforcing such constraints would be hard. 甚至可以约束整个执行的所有操作，尽管强制执行此类约束会很困难。</p>
<h4 id="3-3-4-Application-dependence"><a href="#3-3-4-Application-dependence" class="headerlink" title="3.3.4 Application dependence"></a>3.3.4 Application dependence</h4><p>State consistency tends to be application dependent, because the properties concern state, and the correct state of a system varies significantly from application to application. As a result, developers need to figure out the right properties for each system, which takes time and effort. Moreover, in some cases there are no general mechanisms to enforce state consistency and developers must write application code that is closely tied to each property. There are two noteworthy exceptions: mutual consistency and eventual consistency. These properties apply broadly to any replicated system, by referring to the replicated state irrespective of the application, and there are general replication mechanisms to enforce such properties.</p>
<p>状态一致性往往依赖于应用程序，因为属性涉及状态，并且系统的正确状态因应用程序而异。 因此，开发人员需要为每个系统找出正确的属性，这需要时间和精力。 此外，在某些情况下，没有通用机制来强制状态一致性，开发人员必须编写与每个属性紧密相关的应用程序代码。 有两个值得注意的例外：相互一致性和最终一致性。 这些属性通过引用复制状态而广泛适用于任何复制系统，而与应用程序无关，并且存在通用复制机制来强制执行这些属性。</p>
<p>Operation consistency is often application independent. It achieves application independence in two ways. First, some properties factor out the application-specific behavior, by reducing the behavior of the system under concurrent operations to behavior under sequential operations (as in the sequential equivalence subcategory), or behavior under a reference (as in the reference equivalence subcategory). Second, some properties focus on specific operations, such as read and write, that apply to many systems (as in the read-write centric subcategory). Theoretically, operation consistency <em>can</em> be highly application dependent, but this is not common. An example might be an email system accessible by many devices, where each operation (read, delete, move) might have different constraints on their response according to their semantics and the expectations of users.</p>
<p>操作一致性通常与应用程序无关。 它通过两种方式实现应用程序独立性。 首先，一些属性通过将并发操作下的系统行为减少为顺序操作下的行为（如顺序等效子类别中的行为）或引用下的行为（如引用等效子类别中的行为）来分解特定于应用程序的行为。 其次，某些属性专注于适用于许多系统的特定操作，例如读取和写入（如在以读写为中心的子类别中）。 理论上，操作一致性可能高度依赖于应用程序，但这并不常见。 一个示例可能是可由许多设备访问的电子邮件系统，其中每个操作（读取、删除、移动）可能根据其语义和用户的期望对其响应有不同的限制。</p>
<h4 id="3-3-5-Which-type-to-use"><a href="#3-3-5-Which-type-to-use" class="headerlink" title="3.3.5 Which type to use?"></a>3.3.5 Which type to use?</h4><p>To decide what type of consistency to use, we suggest taking a few things into consideration. First, think about the negation of consistency: what are the inconsistencies that must be avoided? If the answer is most easily described by an undesirable state (e.g., two replicas diverge), then use state consistency. If the answer is most easily described by an incorrect result to an operation (e.g., a read returns stale data), then use operation consistency.</p>
<p>要决定使用哪种类型的一致性，我们建议考虑一些事项。 首先，思考一致性的否定：哪些是必须避免的不一致？ 如果答案最容易通过不良状态来描述（例如，两个副本存在分歧），则使用状态一致性。 如果答案最容易通过操作的错误结果来描述（例如，读取返回过时的数据），则使用操作一致性。</p>
<p>A second important consideration is application dependency. Many operation consistency and some state consistency properties are application independent (e.g., serializability, linearizability, mutual consistency, even- tual consistency). We recommend trying to use such properties, before defining an application-specific one, because the mechanisms to enforce them are well understood. If the system requires an application specific property, and state and operation consistency are both natural choices, then we recommend using state consis- tency due to its simplicity.</p>
<p>第二个重要的考虑因素是应用程序依赖性。 许多操作一致性和一些状态一致性属性是独立于应用程序的（例如，可串行性、可线性性、相互一致性、最终一致性）。 我们建议在定义特定于应用程序的属性之前尝试使用此类属性，因为强制执行它们的机制很容易理解。 如果系统需要应用程序特定的属性，并且状态和操作一致性都是自然的选择，那么我们建议使用状态一致性，因为它很简单。</p>
<h2 id="4-Consistency-in-different-disciplines"><a href="#4-Consistency-in-different-disciplines" class="headerlink" title="4 Consistency in different disciplines"></a>4 Consistency in different disciplines</h2><p>不同学科的一致性</p>
<p>We now discuss what consistency means in each discipline, why it is relevant in that discipline, and how it relates to the two types of consistency in Section 3. We also point out concepts that are considered to be consistency in one discipline but not in another.</p>
<p>我们现在讨论一致性在每个学科中的含义，为什么它与该学科相关，以及它与第 3 节中的两种类型的一致性有何关系。我们还指出了在一个学科中被认为是一致性但在另一个学科中则不然的概念 。</p>
<h3 id="4-1-Distributed-systems"><a href="#4-1-Distributed-systems" class="headerlink" title="4.1 Distributed systems"></a>4.1 Distributed systems</h3><p>In distributed systems, consistency refers to either state or operation consistency. Early replication protocols focused on providing mutual consistency while many cloud distributed systems provide eventual consistency. These are examples of state consistency. Some systems aim at providing linearizability or various flavors of read-write centric consistency. These are examples of operation consistency.</p>
<p>在分布式系统中，一致性指的是状态一致性或操作一致性。 早期的复制协议侧重于提供相互一致性，而许多云分布式系统提供最终一致性。 这些是状态一致性的例子。 一些系统旨在提供线性化或各种以读写为中心的一致性。 这些都是操作一致性的例子。</p>
<p>Consistency is an important consideration in distributed systems because such systems face many concerns that preclude or hinder consistency: clients separated by a slow network, machines that fail, clients that discon- nect from each other, scalability of the system to a large number of clients, and high availability. These concerns can make it hard to provide strong levels of consistency, because consistency requires client coordination that may not be possible. As a result, distributed systems may adopt weaker levels of consistency, chosen according to the needs of applications.</p>
<p>一致性是分布式系统中的一个重要考虑因素，因为此类系统面临许多阻碍或阻碍一致性的问题：客户端被慢速网络分隔、机器发生故障、客户端彼此断开连接、系统对大量客户端的可扩展性 和高可用性。 这些问题可能会使提供强级别的一致性变得困难，因为一致性需要客户端协调，而这可能是不可能的。 因此，分布式系统可能会采用根据应用程序的需要选择的较弱的一致性级别。</p>
<p>Cloud systems, an interesting type of distributed system, face all of the above concerns with intensity: the systems are geo-distributed (distributed around the globe) with significant latency separating data centers; ma- chines fail often because there are many of them; clients disconnect from remote data centers due to problems or congestion in wide-area links; many clients are active and the system must serve all of them well; and the system must be available whenever possible since businesses lose money during downtime. Because of these challenges, cloud systems often resort to weak levels of consistency.</p>
<p>云系统是一种有趣的分布式系统，它面临着上述所有严重问题：系统是地理分布式的（分布在全球各地），数据中心之间存在显着的延迟； 机器经常出故障，因为数量太多； 由于广域链路出现问题或拥塞，客户端与远程数据中心断开连接； 许多客户都是活跃的，系统必须为所有客户提供良好的服务； 并且系统必须尽可能可用，因为企业在停机期间会损失资金。 由于这些挑战，云系统通常采用弱一致性级别。</p>
<h3 id="4-2-Database-systems"><a href="#4-2-Database-systems" class="headerlink" title="4.2 Database systems"></a>4.2 Database systems</h3><p>In database systems, consistency refers to state consistency. For example, consider the ACID acronym that de- scribes the guarantees of transactions. The “C” stands for consistency, which in this case means that the database is always in a state that developers consider valid: the system must preserve invariants such as uniqueness con- straints, referential integrity, and application-specific properties (e.g., x is a friend of y iff y is a friend of x). These are flavors of state consistency.</p>
<p>在数据库系统中，一致性是指状态一致性。 例如，考虑描述事务保证的 ACID 缩写。 “C”代表一致性，在这种情况下意味着数据库始终处于开发人员认为有效的状态：系统必须保留诸如唯一性约束、引用完整性和特定于应用程序的属性（例如，x 是 y 的朋友当且仅当 y 是 x 的朋友）。 这些都是状态一致性的体现。</p>
<p>The “A” stands for atomicity and the “I” stands for isolation. Interestingly, atomicity and isolation are examples of operation consistency. Atomicity requires that a transaction either executes in its entirety or does not execute at all, while isolation requires that transactions appear to execute by themselves without much interference. There are many different levels of isolation (serializability, snapshot isolation, read committed, repeatable reads, etc), but they all constrain the behavior of operations.</p>
<p>“A”代表原子性，“I”代表隔离。 有趣的是，原子性和隔离性是操作一致性的例子。 原子性要求事务要么完整执行，要么根本不执行，而隔离性则要求事务看起来自己执行而没有太多干扰。 有许多不同级别的隔离（可串行性、快照隔离、已提交读、可重复读等），但它们都限制了操作的行为。</p>
<p>Although the database systems community separates transaction isolation from consistency and atomicity, in the distributed systems community, transaction isolation is seen as a form of consistency, while in the computer architecture community, a concept analogous to isolation is called atomicity. We do not know exactly why these terms have acquired different meanings across communities. But we suspect that a reason is that there are intertwined ideas across these concepts, which is something we try to identify and clarify in this article.</p>
<p>尽管数据库系统社区将事务隔离与一致性和原子性分开，但在分布式系统社区中，事务隔离被视为一致性的一种形式，而在计算机体系结构社区中，类似于隔离的概念称为原子性。 我们并不确切知道为什么这些术语在不同社区中具有不同的含义。 但我们怀疑其中一个原因是这些概念之间存在相互交织的想法，这是我们在本文中试图识别和澄清的内容。</p>
<p>Consistency is important in database systems because data is of primary concern; in fact, data could be even more important than the result of operations in such systems (e.g., operations can fail as long as data is not destroyed). Different types of consistency arise because of the different classes of invariants that exist in the database, each with its own enforcement mechanism. For example, uniqueness constraints are enforced by an index and checks in the execution engine; application-specific constraints are enforced by the application logic; and mutual consistency is enforced by the replication manager.</p>
<p>一致性在数据库系统中很重要，因为数据是首要关注的； 事实上，在此类系统中，数据可能比操作结果更重要（例如，只要数据不被破坏，操作就可能失败）。 不同类型的一致性是由于数据库中存在不同类别的不变量而产生的，每个不变量都有自己的执行机制。 例如，唯一性约束由执行引擎中的索引和检查强制执行； 应用程序特定的约束由应用程序逻辑强制执行； 相互一致性由复制管理器强制执行。</p>
<h3 id="4-3-Computer-architecture"><a href="#4-3-Computer-architecture" class="headerlink" title="4.3 Computer architecture"></a>4.3 Computer architecture</h3><p>In computer architecture, consistency refers to operation consistency. A similar concept called coherence is also a form of operation consistency. Consistency and coherence have a subtle difference. Consistency concerns the entire memory system; it constrains the behavior of reads and writes—called loads and stores—across all the memory locations; an example is the sequential consistency property. Coherence concerns the cache subsystem; it can be seen as consistency of the operation of the various caches responsible for a given memory location. Thus, coherence constrains the behavior of loads and stores to an individual memory location.</p>
<p>在计算机体系结构中，一致性是指操作的一致性。 称为一致性的类似概念也是操作一致性的一种形式。 一致性和连贯性有细微的差别。 一致性涉及整个内存系统； 它限制所有内存位置的读取和写入行为（称为加载和存储）； 一个例子是顺序一致性属性。 一致性涉及缓存子系统； 它可以被视为负责给定内存位置的各种高速缓存的操作的一致性。 因此，一致性将加载和存储的行为限制在单个内存位置。</p>
<p>Coherence and consistency are separated to permit a modular architecture of the system: a cache coherence protocol ensures the correct behavior of the caching subsystem, while the rest of the system ensures consistency across memory accesses without worrying about the cache subsystem.</p>
<p>一致性和一致性是分开的，以允许系统的模块化架构：缓存一致性协议确保缓存子系统的正确行为，而系统的其余部分确保内存访问之间的一致性，而无需担心缓存子系统。</p>
<p>Consistency and coherence arise as issues in computer architecture because increasingly computer systems have many cores or processors sharing access to a common memory: in such systems, there are concurrent operations on memory locations and data replication across many caches, which lead to problems of data sharing.</p>
<p>一致性和连贯性成为计算机体系结构中的问题，因为越来越多的计算机系统有许多核心或处理器共享对公共内存的访问：在这样的系统中，内存位置上存在并发操作，并且跨多个缓存进行数据复制，这会导致数据共享问题 。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>Consistency is a concern that spans many disciplines, as we briefly described here. This concern stems from the rise of concurrency and replication across these disciplines, a trend that we expect to continue. Unfortunately, consistency is subtle and hard to grasp, and to make matters worse, it has different names and meanings across communities. We hope to have shed some light on this subject by identifying two broad and very different types of consistency—state consistency and operation consistency—that can be seen across the disciplines.</p>
<p>正如我们在这里简要描述的那样，一致性是一个跨越许多学科的问题。 这种担忧源于这些学科的并发和复制的兴起，我们预计这一趋势将持续下去。 不幸的是，一致性是微妙且难以掌握的，更糟糕的是，它在不同的社区中有不同的名称和含义。 我们希望通过确定跨学科可以看到的两种广泛且截然不同的一致性类型（状态一致性和操作一致性）来阐明这个主题。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>oracle 层级查询</title>
    <url>/2023/02/01/2-%E6%95%B0%E6%8D%AE%E5%BA%93/2-oracle/oracle%E7%B3%BB%E5%88%97%E4%B9%8B%2002-%E5%B1%82%E7%BA%A7%E6%9F%A5%E8%AF%A2/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="1-Hierarchical-Queries"><a href="#1-Hierarchical-Queries" class="headerlink" title="1. Hierarchical Queries"></a>1. Hierarchical Queries</h2><p>如果嫌描述啰嗦，直接 看 例子<a href="#info"> prior 例子</a></p>
<h3 id="1-语法"><a href="#1-语法" class="headerlink" title="1. 语法"></a>1. <strong>语法</strong></h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">connect</span> <span class="keyword">by</span> [nocycle] <span class="keyword">condition</span> [<span class="keyword">start</span> <span class="keyword">with</span> <span class="keyword">condition</span>]</span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> <span class="keyword">condition</span> <span class="keyword">connect</span> <span class="keyword">by</span> [nocycle] <span class="keyword">condition</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p><code>condition</code> </p>
<p><code>start with</code> 指定层次查询的 root row</p>
<p><code>connect by</code> 指定层次查询中 parent rows 和 child rows 的关系</p>
<ul>
<li>NOCYCLE 参数指示 Oracle 数据库从查询中返回行，即使数据中存在 CONNECT BY 循环。 将此参数与 CONNECT_BY_ISCYCLE 伪列一起使用以查看哪些行包含循环。 有关详细信息，请参阅 CONNECT_BY_ISCYCLE 伪列。</li>
<li>在分层查询中，条件中的一个表达式必须使用 PRIOR 运算符限定以引用父行。 例如，</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">... PRIOR expr <span class="operator">=</span> expr</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">... expr <span class="operator">=</span> PRIOR expr</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果 CONNECT BY 条件是复合条件，则只有一个条件需要 PRIOR 运算符，尽管您可以有多个 PRIOR 条件。 例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CONNECT</span> <span class="keyword">BY</span> last_name <span class="operator">!=</span> <span class="string">&#x27;King&#x27;</span> <span class="keyword">AND</span> PRIOR employee_id <span class="operator">=</span> manager_id ...</span><br><span class="line"><span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id <span class="keyword">and</span> </span><br><span class="line">           PRIOR account_mgr_id <span class="operator">=</span> customer_id ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>PRIOR 是一元运算符，与一元 + 和 - 算术运算符具有相同的优先级。 它为分层查询中当前行的父行计算紧随其后的表达式。</p>
<p>PRIOR 最常用于使用相等运算符比较列值时。 （PRIOR 关键字可以在运算符的任一侧。） PRIOR 使 Oracle 使用列中父行的值。 在 CONNECT BY 子句中理论上可以使用除等号 (&#x3D;) 以外的运算符。 但是，这些其他运算符创建的条件可能会导致通过可能的组合的无限循环。 在这种情况下，Oracle 在运行时检测到循环并返回错误。</p>
<p>CONNECT BY 条件和 PRIOR 表达式都可以采用不相关子查询的形式。 但是，CURRVAL 和 NEXTVAL 不是有效的 PRIOR 表达式，因此 PRIOR 表达式不能引用序列。</p>
<p>您可以通过使用 CONNECT_BY_ROOT 运算符来进一步细化层次查询，以限定选择列表中的列。 此运算符不仅返回直接父行，而且返回层次结构中的所有祖先行，从而扩展了层次查询的 CONNECT BY [PRIOR] 条件的功能。</p>
<h3 id="2-执行过程"><a href="#2-执行过程" class="headerlink" title="2. 执行过程"></a>2. 执行过程</h3><p>Oracle 按如下方式处理分层查询：</p>
<ul>
<li>如果存在连接，则首先评估连接，无论连接是在 FROM 子句中指定还是使用 WHERE 子句谓词。</li>
<li>评估 CONNECT BY 条件。</li>
<li>评估任何剩余的 WHERE 子句谓词。</li>
</ul>
<p>然后，Oracle 使用来自这些评估的信息通过以下步骤形成层次结构：</p>
<ol>
<li>Oracle 选择层次结构的根行——那些满足 START WITH 条件的行。</li>
<li>Oracle 选择每个根行的子行。每个子行必须满足关于其中一个根行的 CONNECT BY 条件的条件。</li>
<li>Oracle 选择连续几代的子行。 Oracle 首先选择步骤 2 中返回的行的子代，然后选择这些子代的子代，以此类推。 Oracle 总是通过评估与当前父行相关的 CONNECT BY 条件来选择子行。</li>
<li>如果查询包含没有连接的 WHERE 子句，则 Oracle 从层次结构中删除所有不满足 WHERE 子句条件的行。 Oracle 对每一行单独评估此条件，而不是删除不满足条件的行的所有子行。</li>
<li>Oracle 按图 9-1 所示的顺序返回行。在图中，孩子出现在父母的下方。有关分层树的说明，请参见图 3-1。</li>
</ol>
<p><img src="https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/img/sqlrf002.gif" alt="Description of Figure 9-1 follows"></p>
<p>为了找到父行的子行，Oracle 计算父行的 CONNECT BY 条件的 PRIOR 表达式和表中每一行的另一个表达式。 条件为真的行是父项的子项。 CONNECT BY 条件可以包含其他条件以进一步过滤查询选择的行。</p>
<p>如果 CONNECT BY 条件导致层次结构中出现循环，则 Oracle 返回错误。 如果一行既是另一行的父（或祖父母或直接祖先）又是子（或孙子或直接后代），则发生循环。</p>
<blockquote>
<p>注意：在分层查询中，不要指定 ORDER BY 或 GROUP BY，因为它们会覆盖 CONNECT BY 结果的分层顺序。 如果要对同一父级的兄弟行进行排序，请使用 ORDER SIBLINGS BY 子句。 请参见 order_by_clause。</p>
</blockquote>
<h3 id="3-Hierarchical-例子"><a href="#3-Hierarchical-例子" class="headerlink" title="3. Hierarchical  例子"></a>3. Hierarchical  例子</h3><h4 id="3-1CONNECT-BY-Example"><a href="#3-1CONNECT-BY-Example" class="headerlink" title="3.1CONNECT BY Example"></a>3.1<strong>CONNECT BY Example</strong></h4><p>以下分层查询使用 CONNECT BY 子句来定义员工和经理之间的关系：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, manager_id</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id;</span><br><span class="line"></span><br><span class="line">EMPLOYEE_ID LAST_NAME                 MANAGER_ID</span><br><span class="line"><span class="comment">----------- ------------------------- ----------</span></span><br><span class="line">        <span class="number">101</span> Kochhar                          <span class="number">100</span></span><br><span class="line">        <span class="number">108</span> Greenberg                        <span class="number">101</span></span><br><span class="line">        <span class="number">109</span> Faviet                           <span class="number">108</span></span><br><span class="line">        <span class="number">110</span> Chen                             <span class="number">108</span></span><br><span class="line">        <span class="number">111</span> Sciarra                          <span class="number">108</span></span><br><span class="line">        <span class="number">112</span> Urman                            <span class="number">108</span></span><br><span class="line">        <span class="number">113</span> Popp                             <span class="number">108</span></span><br><span class="line">        <span class="number">200</span> Whalen                           <span class="number">101</span></span><br><span class="line">        <span class="number">203</span> Mavris                           <span class="number">101</span></span><br><span class="line">        <span class="number">204</span> Baer                             <span class="number">101</span></span><br><span class="line">. . .</span><br></pre></td></tr></table></figure>

<h4 id="3-2-LEVEL-Example"><a href="#3-2-LEVEL-Example" class="headerlink" title="3.2 LEVEL Example"></a>3.2 <strong>LEVEL Example</strong></h4><p>下一个示例与前面的示例类似，但使用 LEVEL 伪列来显示父行和子行：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, manager_id, LEVEL</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id;</span><br><span class="line"></span><br><span class="line">EMPLOYEE_ID LAST_NAME                 MANAGER_ID      LEVEL</span><br><span class="line"><span class="comment">----------- ------------------------- ---------- ----------</span></span><br><span class="line">        <span class="number">101</span> Kochhar                          <span class="number">100</span>          <span class="number">1</span></span><br><span class="line">        <span class="number">108</span> Greenberg                        <span class="number">101</span>          <span class="number">2</span></span><br><span class="line">        <span class="number">109</span> Faviet                           <span class="number">108</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">110</span> Chen                             <span class="number">108</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">111</span> Sciarra                          <span class="number">108</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">112</span> Urman                            <span class="number">108</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">113</span> Popp                             <span class="number">108</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">200</span> Whalen                           <span class="number">101</span>          <span class="number">2</span></span><br><span class="line">        <span class="number">203</span> Mavris                           <span class="number">101</span>          <span class="number">2</span></span><br><span class="line">        <span class="number">204</span> Baer                             <span class="number">101</span>          <span class="number">2</span></span><br><span class="line">        <span class="number">205</span> Higgins                          <span class="number">101</span>          <span class="number">2</span></span><br><span class="line">        <span class="number">206</span> Gietz                            <span class="number">205</span>          <span class="number">3</span></span><br><span class="line">        <span class="number">102</span> De Haan                          <span class="number">100</span>          <span class="number">1</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="3-3-START-WITH-Examples"><a href="#3-3-START-WITH-Examples" class="headerlink" title="3.3 START WITH Examples"></a>3.3 <strong>START WITH Examples</strong></h4><p>下一个示例添加一个 START WITH 子句来指定层次结构的根行，并使用 SIBLINGS 关键字添加一个 ORDER BY 子句来保持层次结构内的顺序：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name, employee_id, manager_id, LEVEL</span><br><span class="line">      <span class="keyword">FROM</span> employees</span><br><span class="line">      <span class="keyword">START</span> <span class="keyword">WITH</span> employee_id <span class="operator">=</span> <span class="number">100</span></span><br><span class="line">      <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id</span><br><span class="line">      <span class="keyword">ORDER</span> SIBLINGS <span class="keyword">BY</span> last_name;</span><br><span class="line"></span><br><span class="line">LAST_NAME                 EMPLOYEE_ID MANAGER_ID      LEVEL</span><br><span class="line"><span class="comment">------------------------- ----------- ---------- ----------</span></span><br><span class="line">King                              <span class="number">100</span>                     <span class="number">1</span></span><br><span class="line">Cambrault                         <span class="number">148</span>        <span class="number">100</span>          <span class="number">2</span></span><br><span class="line">Bates                             <span class="number">172</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">Bloom                             <span class="number">169</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">Fox                               <span class="number">170</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">Kumar                             <span class="number">173</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">Ozer                              <span class="number">168</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">Smith                             <span class="number">171</span>        <span class="number">148</span>          <span class="number">3</span></span><br><span class="line">De Haan                           <span class="number">102</span>        <span class="number">100</span>          <span class="number">2</span></span><br><span class="line">Hunold                            <span class="number">103</span>        <span class="number">102</span>          <span class="number">3</span></span><br><span class="line">Austin                            <span class="number">105</span>        <span class="number">103</span>          <span class="number">4</span></span><br><span class="line">Ernst                             <span class="number">104</span>        <span class="number">103</span>          <span class="number">4</span></span><br><span class="line">Lorentz                           <span class="number">107</span>        <span class="number">103</span>          <span class="number">4</span></span><br><span class="line">Pataballa                         <span class="number">106</span>        <span class="number">103</span>          <span class="number">4</span></span><br><span class="line">Errazuriz                         <span class="number">147</span>        <span class="number">100</span>          <span class="number">2</span></span><br><span class="line">Ande                              <span class="number">166</span>        <span class="number">147</span>          <span class="number">3</span></span><br><span class="line">Banda                             <span class="number">167</span>        <span class="number">147</span>          <span class="number">3</span></span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在 hr.employees 表中，员工 Steven King 是公司的负责人，没有经理。 他的员工中有 John Russell，他是部门 80 的经理。如果您更新 employees 表以将 Russell 设置为 King 的经理，您会在数据中创建一个循环：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> employees <span class="keyword">SET</span> manager_id <span class="operator">=</span> <span class="number">145</span></span><br><span class="line">   <span class="keyword">WHERE</span> employee_id <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> last_name &quot;Employee&quot;, </span><br><span class="line">   LEVEL, SYS_CONNECT_BY_PATH(last_name, <span class="string">&#x27;/&#x27;</span>) &quot;Path&quot;</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> level <span class="operator">&lt;=</span> <span class="number">3</span> <span class="keyword">AND</span> department_id <span class="operator">=</span> <span class="number">80</span></span><br><span class="line">   <span class="keyword">START</span> <span class="keyword">WITH</span> last_name <span class="operator">=</span> <span class="string">&#x27;King&#x27;</span></span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id <span class="keyword">AND</span> LEVEL <span class="operator">&lt;=</span> <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">ERROR:</span><br><span class="line">ORA<span class="number">-01436</span>: <span class="keyword">CONNECT</span> <span class="keyword">BY</span> loop <span class="keyword">in</span> <span class="keyword">user</span> data</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>CONNECT BY 条件中的 NOCYCLE 参数使 Oracle 尽管有循环仍返回行。 CONNECT_BY_ISCYCLE 伪列显示哪些行包含循环：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name &quot;Employee&quot;, CONNECT_BY_ISCYCLE &quot;Cycle&quot;,</span><br><span class="line">   LEVEL, SYS_CONNECT_BY_PATH(last_name, <span class="string">&#x27;/&#x27;</span>) &quot;Path&quot;</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> level <span class="operator">&lt;=</span> <span class="number">3</span> <span class="keyword">AND</span> department_id <span class="operator">=</span> <span class="number">80</span></span><br><span class="line">   <span class="keyword">START</span> <span class="keyword">WITH</span> last_name <span class="operator">=</span> <span class="string">&#x27;King&#x27;</span></span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> NOCYCLE PRIOR employee_id <span class="operator">=</span> manager_id <span class="keyword">AND</span> LEVEL <span class="operator">&lt;=</span> <span class="number">4</span></span><br><span class="line">   <span class="keyword">ORDER</span> <span class="keyword">BY</span> &quot;Employee&quot;, &quot;Cycle&quot;, LEVEL, &quot;Path&quot;;</span><br><span class="line"></span><br><span class="line">Employee                       <span class="keyword">Cycle</span>      LEVEL Path</span><br><span class="line"><span class="comment">------------------------- ---------- ---------- -------------------------</span></span><br><span class="line">Abel                               <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Zlotkey<span class="operator">/</span>Abel</span><br><span class="line">Ande                               <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz<span class="operator">/</span>Ande</span><br><span class="line">Banda                              <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz<span class="operator">/</span>Banda</span><br><span class="line">Bates                              <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Bates</span><br><span class="line">Bernstein                          <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Russell<span class="operator">/</span>Bernstein</span><br><span class="line">Bloom                              <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Bloom</span><br><span class="line">Cambrault                          <span class="number">0</span>          <span class="number">2</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault</span><br><span class="line">Cambrault                          <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Russell<span class="operator">/</span>Cambrault</span><br><span class="line">Doran                              <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Partners<span class="operator">/</span>Doran</span><br><span class="line">Errazuriz                          <span class="number">0</span>          <span class="number">2</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz</span><br><span class="line">Fox                                <span class="number">0</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Fox</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="3-4-CONNECT-BY-ISLEAF-Example"><a href="#3-4-CONNECT-BY-ISLEAF-Example" class="headerlink" title="3.4 CONNECT_BY_ISLEAF Example"></a>3.4 <strong>CONNECT_BY_ISLEAF Example</strong></h4><p>以下语句显示了如何使用分层查询将列中的值转换为逗号分隔的列表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> LTRIM(SYS_CONNECT_BY_PATH (warehouse_id,<span class="string">&#x27;,&#x27;</span>),<span class="string">&#x27;,&#x27;</span>) <span class="keyword">FROM</span></span><br><span class="line">   (<span class="keyword">SELECT</span> ROWNUM r, warehouse_id <span class="keyword">FROM</span> warehouses)</span><br><span class="line">   <span class="keyword">WHERE</span> CONNECT_BY_ISLEAF <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">START</span> <span class="keyword">WITH</span> r <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> r <span class="operator">=</span> PRIOR r <span class="operator">+</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">ORDER</span> <span class="keyword">BY</span> warehouse_id; </span><br><span class="line"> </span><br><span class="line">LTRIM(SYS_CONNECT_BY_PATH(WAREHOUSE_ID,<span class="string">&#x27;,&#x27;</span>),<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"><span class="comment">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span></span><br></pre></td></tr></table></figure>

<h4 id="3-5-CONNECT-BY-ROOT-Examples"><a href="#3-5-CONNECT-BY-ROOT-Examples" class="headerlink" title="3.5 CONNECT_BY_ROOT Examples"></a>3.5 <strong>CONNECT_BY_ROOT Examples</strong></h4><p>以下示例返回部门 110 中每个员工的姓氏、层次结构中该员工上方最高级别的每个经理、经理和员工之间的级别数以及两者之间的路径：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name &quot;Employee&quot;, CONNECT_BY_ROOT last_name &quot;Manager&quot;,</span><br><span class="line">   LEVEL<span class="number">-1</span> &quot;Pathlen&quot;, SYS_CONNECT_BY_PATH(last_name, <span class="string">&#x27;/&#x27;</span>) &quot;Path&quot;</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> LEVEL <span class="operator">&gt;</span> <span class="number">1</span> <span class="keyword">and</span> department_id <span class="operator">=</span> <span class="number">110</span></span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id</span><br><span class="line">   <span class="keyword">ORDER</span> <span class="keyword">BY</span> &quot;Employee&quot;, &quot;Manager&quot;, &quot;Pathlen&quot;, &quot;Path&quot;;</span><br><span class="line"></span><br><span class="line">Employee        Manager            Pathlen Path</span><br><span class="line"><span class="comment">--------------- --------------- ---------- ------------------------------</span></span><br><span class="line">Gietz           Higgins                  <span class="number">1</span> <span class="operator">/</span>Higgins<span class="operator">/</span>Gietz</span><br><span class="line">Gietz           King                     <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins<span class="operator">/</span>Gietz</span><br><span class="line">Gietz           Kochhar                  <span class="number">2</span> <span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins<span class="operator">/</span>Gietz</span><br><span class="line">Higgins         King                     <span class="number">2</span> <span class="operator">/</span>King<span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins</span><br><span class="line">Higgins         Kochhar                  <span class="number">1</span> <span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以下示例使用 GROUP BY 子句返回部门 110 中每个员工的总工资以及层次结构中该员工之上的所有员工：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">SUM</span>(salary) &quot;Total_Salary&quot; <span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> CONNECT_BY_ROOT last_name <span class="keyword">as</span> name, Salary</span><br><span class="line">      <span class="keyword">FROM</span> employees</span><br><span class="line">      <span class="keyword">WHERE</span> department_id <span class="operator">=</span> <span class="number">110</span></span><br><span class="line">      <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id)</span><br><span class="line">      <span class="keyword">GROUP</span> <span class="keyword">BY</span> name</span><br><span class="line">   <span class="keyword">ORDER</span> <span class="keyword">BY</span> name, &quot;Total_Salary&quot;;</span><br><span class="line"></span><br><span class="line">NAME                      Total_Salary</span><br><span class="line"><span class="comment">------------------------- ------------</span></span><br><span class="line">Gietz                             <span class="number">8300</span></span><br><span class="line">Higgins                          <span class="number">20300</span></span><br><span class="line">King                             <span class="number">20300</span></span><br><span class="line">Kochhar                          <span class="number">20300</span></span><br></pre></td></tr></table></figure>







<h2 id="2-Hierarchical-Operators"><a href="#2-Hierarchical-Operators" class="headerlink" title="2. Hierarchical  Operators"></a>2. Hierarchical  Operators</h2><p>两个运算符 PRIOR 和 CONNECT_BY_ROOT 仅在分层查询中有效</p>
<h3 id="1-PRIOR"><a href="#1-PRIOR" class="headerlink" title="1. PRIOR"></a>1. PRIOR</h3><p>在分层查询中，CONNECT BY 条件中的一个表达式必须由 PRIOR 运算符限定。 如果 CONNECT BY 条件是复合条件，则只有一个条件需要 PRIOR 运算符，尽管您可以有多个 PRIOR 条件。 PRIOR 计算层次查询中当前行的父行的紧随其后的表达式。</p>
<p>PRIOR 最常用于使用相等运算符比较列值时。 （PRIOR 关键字可以在运算符的任一侧。） PRIOR 使 Oracle 使用列中父行的值。 在 CONNECT BY 子句中理论上可以使用除等号 (&#x3D;) 以外的运算符。 但是，这些其他运算符创建的条件可能会导致通过可能的组合的无限循环。 在这种情况下，Oracle 在运行时检测到循环并返回错误。 有关此运算符的更多信息（包括示例），请参阅分层查询。</p>
<p>如果还不理解 prior，见后面的例子<a href="#info"> prior 例子</a></p>
<h3 id="2-CONNECT-BY-ROOT"><a href="#2-CONNECT-BY-ROOT" class="headerlink" title="2. CONNECT_BY_ROOT"></a>2. CONNECT_BY_ROOT</h3><p>CONNECT_BY_ROOT 是一元运算符，仅在分层查询中有效。 当您使用此运算符限定列时，Oracle 使用根行中的数据返回列值。 此运算符扩展了分层查询的 CONNECT BY [PRIOR] 条件的功能。</p>
<p>对 CONNECT_BY_ROOT 的限制</p>
<p>您不能在 START WITH 条件或 CONNECT BY 条件中指定此运算符。</p>
<h2 id="3-Hierarchical-伪列"><a href="#3-Hierarchical-伪列" class="headerlink" title="3. Hierarchical  伪列"></a>3. Hierarchical  伪列</h2><p>分层查询伪列仅 (Pseudocolumns) 在分层查询中有效。 分层查询伪列是：</p>
<ul>
<li>[CONNECT_BY_ISCYCLE Pseudocolumn]</li>
<li>[CONNECT_BY_ISLEAF Pseudocolumn]</li>
<li>[LEVEL Pseudocolumn]</li>
</ul>
<p>要在查询中定义层次关系，您必须使用 CONNECT BY 子句。</p>
<h3 id="3-1-CONNECT-BY-ISCYCLE"><a href="#3-1-CONNECT-BY-ISCYCLE" class="headerlink" title="3.1 CONNECT_BY_ISCYCLE"></a>3.1 CONNECT_BY_ISCYCLE</h3><p>如果当前行有一个也是其祖先的子项，则 CONNECT_BY_ISCYCLE 伪列返回 1。 否则返回 0。</p>
<p>仅当您已指定 CONNECT BY 子句的 NOCYCLE 参数时，您才能指定 CONNECT_BY_ISCYCLE。 NOCYCLE 使 Oracle 能够返回查询的结果，否则该查询会因数据中的 CONNECT BY 循环而失败。</p>
<h3 id="3-2-CONNECT-BY-ISLEAF"><a href="#3-2-CONNECT-BY-ISLEAF" class="headerlink" title="3.2 CONNECT_BY_ISLEAF"></a>3.2 CONNECT_BY_ISLEAF</h3><p>如果当前行是由 CONNECT BY 条件定义的树的叶子，则 CONNECT_BY_ISLEAF 伪列返回 1。 否则返回 0。此信息指示是否可以进一步扩展给定行以显示更多层次结构。</p>
<p><strong>CONNECT_BY_ISLEAF Example</strong></p>
<p>以下示例显示了 hr.employees 表的前三个级别，为每一行指示它是叶行（在 IsLeaf 列中用 1 表示）还是有子行（在 IsLeaf 列中用 0 表示）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name &quot;Employee&quot;, CONNECT_BY_ISLEAF &quot;IsLeaf&quot;,</span><br><span class="line">       LEVEL, SYS_CONNECT_BY_PATH(last_name, <span class="string">&#x27;/&#x27;</span>) &quot;Path&quot;</span><br><span class="line">  <span class="keyword">FROM</span> employees</span><br><span class="line">  <span class="keyword">WHERE</span> LEVEL <span class="operator">&lt;=</span> <span class="number">3</span> <span class="keyword">AND</span> department_id <span class="operator">=</span> <span class="number">80</span></span><br><span class="line">  <span class="keyword">START</span> <span class="keyword">WITH</span> employee_id <span class="operator">=</span> <span class="number">100</span></span><br><span class="line">  <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id <span class="keyword">AND</span> LEVEL <span class="operator">&lt;=</span> <span class="number">4</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> &quot;Employee&quot;, &quot;IsLeaf&quot;;</span><br><span class="line"></span><br><span class="line">Employee                      IsLeaf      LEVEL Path</span><br><span class="line"><span class="comment">------------------------- ---------- ---------- -------------------------</span></span><br><span class="line">Abel                               <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Zlotkey<span class="operator">/</span>Abel</span><br><span class="line">Ande                               <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz<span class="operator">/</span>Ande</span><br><span class="line">Banda                              <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz<span class="operator">/</span>Banda</span><br><span class="line">Bates                              <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Bates</span><br><span class="line">Bernstein                          <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Russell<span class="operator">/</span>Bernstein</span><br><span class="line">Bloom                              <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Bloom</span><br><span class="line">Cambrault                          <span class="number">0</span>          <span class="number">2</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault</span><br><span class="line">Cambrault                          <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Russell<span class="operator">/</span>Cambrault</span><br><span class="line">Doran                              <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Partners<span class="operator">/</span>Doran</span><br><span class="line">Errazuriz                          <span class="number">0</span>          <span class="number">2</span> <span class="operator">/</span>King<span class="operator">/</span>Errazuriz</span><br><span class="line">Fox                                <span class="number">1</span>          <span class="number">3</span> <span class="operator">/</span>King<span class="operator">/</span>Cambrault<span class="operator">/</span>Fox</span><br><span class="line">. . . </span><br></pre></td></tr></table></figure>

<h3 id="3-3-LEVEL"><a href="#3-3-LEVEL" class="headerlink" title="3.3 LEVEL"></a>3.3 LEVEL</h3><p>对于分层查询返回的每一行，LEVEL 伪列为根行返回 1，为根的子行返回 2，依此类推。 根行是倒排树中的最高行。 子行是任何非根行。 父行是具有子行的任何行。 叶行是任何没有子行的行。 图 3-1 显示了倒排树的节点及其 LEVEL 值。</p>
<p><img src="https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/img/sqlrf001.gif" alt="Description of Figure 3-1 follows"></p>
<h2 id="4-SYS-CONNECT-BY-PATH"><a href="#4-SYS-CONNECT-BY-PATH" class="headerlink" title="4. SYS_CONNECT_BY_PATH"></a>4. SYS_CONNECT_BY_PATH</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SYS_CONNECT_BY_PATH (<span class="keyword">column</span>, <span class="type">char</span>) </span><br></pre></td></tr></table></figure>

<h3 id="4-1-功能"><a href="#4-1-功能" class="headerlink" title="4.1 功能"></a>4.1 功能</h3><p>SYS_CONNECT_BY_PATH 仅在分层查询中有效。 它返回列值从根到节点的路径，对于 CONNECT BY 条件返回的每一行，列值由 char 分隔。</p>
<p>column 和 char 都可以是任何数据类型 CHAR、VARCHAR2、NCHAR 或 NVARCHAR2。 返回的字符串是 VARCHAR2 数据类型，并且与列在同一字符集中。</p>
<h3 id="4-2-例子"><a href="#4-2-例子" class="headerlink" title="4.2 例子"></a>4.2 <strong>例子</strong></h3><p>以下示例返回从员工 Kochhar 到 Kochhar 的所有员工（及其员工）的员工姓名路径：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> LPAD(<span class="string">&#x27; &#x27;</span>, <span class="number">2</span><span class="operator">*</span>level<span class="number">-1</span>)<span class="operator">||</span>SYS_CONNECT_BY_PATH(last_name, <span class="string">&#x27;/&#x27;</span>) &quot;Path&quot;</span><br><span class="line">   <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">START</span> <span class="keyword">WITH</span> last_name <span class="operator">=</span> <span class="string">&#x27;Kochhar&#x27;</span></span><br><span class="line">   <span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR employee_id <span class="operator">=</span> manager_id;</span><br><span class="line"></span><br><span class="line">Path</span><br><span class="line"><span class="comment">------------------------------</span></span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg<span class="operator">/</span>Chen</span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg<span class="operator">/</span>Faviet</span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg<span class="operator">/</span>Popp</span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg<span class="operator">/</span>Sciarra</span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg<span class="operator">/</span>Urman</span><br><span class="line">     <span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins<span class="operator">/</span>Gietz</span><br><span class="line">   <span class="operator">/</span>Kochhar<span class="operator">/</span>Baer</span><br><span class="line">   <span class="operator">/</span>Kochhar<span class="operator">/</span>Greenberg</span><br><span class="line">   <span class="operator">/</span>Kochhar<span class="operator">/</span>Higgins</span><br><span class="line">   <span class="operator">/</span>Kochhar<span class="operator">/</span>Mavris</span><br><span class="line">   <span class="operator">/</span>Kochhar<span class="operator">/</span>Whalen</span><br><span class="line"> <span class="operator">/</span>Kochhar</span><br></pre></td></tr></table></figure>



<h2 id="5-例子"><a href="#5-例子" class="headerlink" title="5. 例子"></a>5. 例子</h2><h3 id="5-1-构造数据"><a href="#5-1-构造数据" class="headerlink" title="5.1 构造数据"></a>5.1 构造数据</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_tree (</span><br><span class="line">  test_id   <span class="type">INT</span>  <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  pid       <span class="type">INT</span>,</span><br><span class="line">  test_val  <span class="type">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (test_id)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">1</span>, <span class="number">0</span>,   <span class="string">&#x27;.NET&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">2</span>, <span class="number">1</span>,      <span class="string">&#x27;C#&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">3</span>, <span class="number">1</span>,      <span class="string">&#x27;J#&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">4</span>, <span class="number">1</span>,      <span class="string">&#x27;ASP.NET&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">5</span>, <span class="number">1</span>,      <span class="string">&#x27;VB.NET&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">6</span>, <span class="number">0</span>,   <span class="string">&#x27;J2EE&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">7</span>, <span class="number">6</span>,      <span class="string">&#x27;EJB&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">8</span>, <span class="number">6</span>,      <span class="string">&#x27;Servlet&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">9</span>, <span class="number">6</span>,      <span class="string">&#x27;JSP&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">10</span>, <span class="number">0</span>,  <span class="string">&#x27;Database&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">11</span>, <span class="number">10</span>,    <span class="string">&#x27;DB2&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">12</span>, <span class="number">10</span>,    <span class="string">&#x27;MySQL&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">13</span>, <span class="number">10</span>,    <span class="string">&#x27;Oracle&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">14</span>, <span class="number">10</span>,    <span class="string">&#x27;SQL Server&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">15</span>, <span class="number">13</span>,    <span class="string">&#x27;PL/SQL&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">16</span>, <span class="number">15</span>,    <span class="string">&#x27;Function&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">17</span>, <span class="number">15</span>,    <span class="string">&#x27;Procedure&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">18</span>, <span class="number">15</span>,    <span class="string">&#x27;Package&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">19</span>, <span class="number">15</span>,    <span class="string">&#x27;Cursor&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_tree <span class="keyword">VALUES</span>(<span class="number">20</span>, <span class="number">14</span>,    <span class="string">&#x27;T-SQL&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  LEVEL,</span><br><span class="line">  test_id,</span><br><span class="line">  test_val,</span><br><span class="line">  SYS_CONNECT_BY_PATH(test_val, <span class="string">&#x27;\&#x27;</span>) <span class="keyword">AS</span> &quot;FullPath&quot;</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  test_tree</span><br><span class="line"><span class="keyword">START</span> <span class="keyword">WITH</span></span><br><span class="line">  pid <span class="operator">=</span><span class="number">0</span></span><br><span class="line"><span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR test_id <span class="operator">=</span> pid</span><br><span class="line"><span class="keyword">ORDER</span> SIBLINGS <span class="keyword">BY</span> test_val;</span><br></pre></td></tr></table></figure>



<h3 id="5-2-执行结果解释"><a href="#5-2-执行结果解释" class="headerlink" title="5.2 执行结果解释"></a>5.2 执行结果解释</h3><p> start with 配合 level 解释比较好理解，如果不指定 start with， 那么所有数据都会作为 根行，也就是 level 1。</p>
<p>如果指定了 start with ，被指定的行为根行。 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 不指定 start with</span></span><br><span class="line"><span class="keyword">SELECT</span>  level ,test_id, pid, test_val <span class="keyword">from</span> test_tree   <span class="keyword">CONNECT</span> <span class="keyword">BY</span>  prior test_id<span class="operator">=</span> pid </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>	<span class="number">1</span>	  <span class="number">0</span> 	.NET</span><br><span class="line"><span class="number">1</span>	<span class="number">2</span>  	<span class="number">1</span>  	C#</span><br><span class="line"><span class="number">1</span>	<span class="number">3</span>  	<span class="number">1</span>  	J#</span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	  <span class="number">1</span>  	ASP.NET</span><br><span class="line"><span class="number">1</span>	<span class="number">5</span>	  <span class="number">1</span>  	VB.NET</span><br><span class="line"><span class="number">1</span>	<span class="number">6</span>  	<span class="number">0</span>  	J2EE</span><br><span class="line"><span class="number">1</span>	<span class="number">7</span>	  <span class="number">6</span>  	EJB</span><br><span class="line"><span class="number">1</span>	<span class="number">8</span>	  <span class="number">6</span>  	Servlet</span><br><span class="line"><span class="number">1</span>	<span class="number">9</span>	  <span class="number">6</span>  	JSP</span><br><span class="line"><span class="number">1</span>	<span class="number">10</span>	<span class="number">0</span>	  Database</span><br><span class="line"><span class="number">1</span>	<span class="number">11</span>	<span class="number">10</span>	DB2</span><br><span class="line"><span class="number">1</span>	<span class="number">12</span>	<span class="number">10</span>	MySQL</span><br><span class="line"><span class="number">1</span>	<span class="number">13</span>	<span class="number">10</span>	Oracle</span><br><span class="line"><span class="number">1</span>	<span class="number">14</span>	<span class="number">10</span>	<span class="keyword">SQL</span> Server</span><br><span class="line"><span class="number">1</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">1</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">1</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">1</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">1</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br><span class="line"><span class="number">1</span>	<span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">2</span>	<span class="number">2</span>	  <span class="number">1</span>	  C#</span><br><span class="line"><span class="number">2</span>	<span class="number">3</span>  	<span class="number">1</span>	  J#</span><br><span class="line"><span class="number">2</span>	<span class="number">4</span>  	<span class="number">1</span>  	ASP.NET</span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>  	<span class="number">1</span>  	VB.NET</span><br><span class="line"><span class="number">2</span>	<span class="number">7</span>	  <span class="number">6</span>  	EJB</span><br><span class="line"><span class="number">2</span>	<span class="number">8</span>	  <span class="number">6</span>  	Servlet</span><br><span class="line"><span class="number">2</span>	<span class="number">9</span>	  <span class="number">6</span>  	JSP</span><br><span class="line"><span class="number">2</span>	<span class="number">11</span>	<span class="number">10</span>	DB2</span><br><span class="line"><span class="number">2</span>	<span class="number">12</span>	<span class="number">10</span>	MySQL</span><br><span class="line"><span class="number">2</span>	<span class="number">13</span>	<span class="number">10</span>	Oracle</span><br><span class="line"><span class="number">2</span>	<span class="number">14</span>	<span class="number">10</span>	<span class="keyword">SQL</span> Server</span><br><span class="line"><span class="number">2</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">2</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">2</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">2</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">2</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br><span class="line"><span class="number">2</span>	<span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">3</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">3</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">3</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">3</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">3</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br><span class="line"><span class="number">3</span>	<span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">4</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">4</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">4</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">4</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 指定 start with</span></span><br><span class="line"><span class="keyword">SELECT</span>  level ,test_id, pid, test_val <span class="keyword">from</span> test_tree  </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> test_id<span class="operator">=</span><span class="number">10</span> <span class="keyword">CONNECT</span> <span class="keyword">BY</span>  prior test_id<span class="operator">=</span> pid <span class="keyword">order</span> <span class="keyword">by</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>	<span class="number">10</span>	<span class="number">0</span>	  Database</span><br><span class="line"><span class="number">2</span>	<span class="number">11</span>	<span class="number">10</span>	DB2</span><br><span class="line"><span class="number">2</span>	<span class="number">12</span>	<span class="number">10</span>	MySQL</span><br><span class="line"><span class="number">2</span>	<span class="number">13</span>	<span class="number">10</span>	Oracle</span><br><span class="line"><span class="number">2</span>	<span class="number">14</span>	<span class="number">10</span>	<span class="keyword">SQL</span> Server</span><br><span class="line"><span class="number">3</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">3</span>	<span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">4</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">4</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">4</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">4</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br></pre></td></tr></table></figure>



<p><a id="info"> prior例子 </a> 由于prior 可能会不大好理解，这里再详细解释一下</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  level ,test_id, pid, test_val <span class="keyword">from</span> test_tree  </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> test_id<span class="operator">=</span><span class="number">10</span> <span class="keyword">CONNECT</span> <span class="keyword">BY</span>  prior test_id<span class="operator">=</span> pid <span class="keyword">order</span> <span class="keyword">by</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>	<span class="number">10</span>	<span class="number">0</span>	  Database</span><br><span class="line"><span class="number">2</span>	<span class="number">11</span>	<span class="number">10</span>	DB2</span><br><span class="line"><span class="number">2</span>	<span class="number">12</span>	<span class="number">10</span>	MySQL</span><br><span class="line"><span class="number">2</span>	<span class="number">13</span>	<span class="number">10</span>	Oracle</span><br><span class="line"><span class="number">2</span>	<span class="number">14</span>	<span class="number">10</span>	<span class="keyword">SQL</span> Server</span><br><span class="line"><span class="number">3</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">3</span>	<span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">4</span>	<span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span></span><br><span class="line"><span class="number">4</span>	<span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span></span><br><span class="line"><span class="number">4</span>	<span class="number">18</span>	<span class="number">15</span>	Package</span><br><span class="line"><span class="number">4</span>	<span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span></span><br><span class="line"></span><br><span class="line">                                此时 形成的 树形结构为</span><br><span class="line">         level <span class="number">1</span>                    <span class="number">10</span></span><br><span class="line">                                    ｜</span><br><span class="line">                         ————————<span class="comment">----------————————</span></span><br><span class="line">                        ｜      ｜       ｜        ｜</span><br><span class="line">         level <span class="number">2</span>        <span class="number">11</span>      <span class="number">12</span>      <span class="number">13</span>        <span class="number">14</span>     <span class="comment">-----&gt; prior 指定 父行</span></span><br><span class="line">                        ｜      ｜       ｜        ｜</span><br><span class="line">         level <span class="number">3</span>                        <span class="number">15</span>        <span class="number">20</span>     <span class="comment">-----&gt; prior 指定</span></span><br><span class="line">                                          <span class="operator">|</span></span><br><span class="line">                                  <span class="comment">---------------</span></span><br><span class="line">                                  <span class="operator">|</span>    <span class="operator">|</span>    <span class="operator">|</span>    <span class="operator">|</span></span><br><span class="line">                                  <span class="number">16</span>   <span class="number">17</span>  <span class="number">18</span>    <span class="number">19</span></span><br><span class="line">         level <span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- prior 在另一侧，修改一下 start with 的条件。</span></span><br><span class="line"><span class="keyword">SELECT</span>  level ,test_id, pid, test_val <span class="keyword">from</span> test_tree  </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> test_id <span class="operator">=</span> <span class="number">15</span> <span class="keyword">CONNECT</span> <span class="keyword">BY</span> test_id <span class="operator">=</span> prior pid <span class="keyword">order</span> <span class="keyword">by</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>	<span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span></span><br><span class="line"><span class="number">2</span>	<span class="number">13</span>	<span class="number">10</span>	Oracle</span><br><span class="line"><span class="number">3</span>	<span class="number">10</span>	<span class="number">0</span>		Database</span><br><span class="line"></span><br><span class="line">                         此时 形成的 树形结构为</span><br><span class="line">         level <span class="number">1</span>             <span class="number">13</span>      <span class="comment">----&gt; prior 指定的父行pid = 13</span></span><br><span class="line">                             <span class="operator">|</span></span><br><span class="line">         level <span class="number">2</span>            <span class="number">10</span></span><br><span class="line">                             <span class="operator">|</span></span><br><span class="line">       	 level <span class="number">3</span>             <span class="number">0</span></span><br><span class="line"></span><br><span class="line">     </span><br></pre></td></tr></table></figure>



<p>完整功能</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 完整的功能有 10 个。</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">test_id, pid, test_val,</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 1. 操作符</span></span><br><span class="line">connect_by_root test_id ,</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 函数</span></span><br><span class="line">SYS_CONNECT_BY_PATH(pid, <span class="string">&#x27;/&#x27;</span>),</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3. 伪列(三个)</span></span><br><span class="line">CONNECT_BY_ISCYCLE,</span><br><span class="line"><span class="comment">-- 4.</span></span><br><span class="line">CONNECT_BY_ISLEAF,</span><br><span class="line"><span class="comment">-- 5.</span></span><br><span class="line">level</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> test_tree  </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 6. 根行</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> test_id<span class="operator">=</span><span class="number">10</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 7. 父行和子行的关系</span></span><br><span class="line"><span class="keyword">CONNECT</span> <span class="keyword">BY</span> nocycle <span class="comment">/* 8. ( cycle ) */</span> prior <span class="comment">/* 9. 操作符 */</span> test_id<span class="operator">=</span> pid </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 10. 排序</span></span><br><span class="line"><span class="keyword">ORDER</span> SIBLINGS <span class="keyword">BY</span> test_id;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">10</span>	<span class="number">0</span>		Database		<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span>					<span class="number">0</span>	<span class="number">0</span>	<span class="number">1</span></span><br><span class="line"><span class="number">11</span>	<span class="number">10</span>	DB2					<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span>				<span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span></span><br><span class="line"><span class="number">12</span>	<span class="number">10</span>	MySQL				<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span>				<span class="number">0</span>	<span class="number">1</span>	<span class="number">2</span></span><br><span class="line"><span class="number">13</span>	<span class="number">10</span>	Oracle			<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span>				<span class="number">0</span>	<span class="number">0</span>	<span class="number">2</span></span><br><span class="line"><span class="number">15</span>	<span class="number">13</span>	PL<span class="operator">/</span><span class="keyword">SQL</span>			<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">13</span>		<span class="number">0</span>	<span class="number">0</span>	<span class="number">3</span></span><br><span class="line"><span class="number">16</span>	<span class="number">15</span>	<span class="keyword">Function</span>		<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">13</span><span class="operator">/</span><span class="number">15</span>	<span class="number">0</span>	<span class="number">1</span>	<span class="number">4</span></span><br><span class="line"><span class="number">17</span>	<span class="number">15</span>	<span class="keyword">Procedure</span>		<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">13</span><span class="operator">/</span><span class="number">15</span>	<span class="number">0</span>	<span class="number">1</span>	<span class="number">4</span></span><br><span class="line"><span class="number">18</span>	<span class="number">15</span>	Package			<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">13</span><span class="operator">/</span><span class="number">15</span>	<span class="number">0</span>	<span class="number">1</span>	<span class="number">4</span></span><br><span class="line"><span class="number">19</span>	<span class="number">15</span>	<span class="keyword">Cursor</span>			<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">13</span><span class="operator">/</span><span class="number">15</span>	<span class="number">0</span>	<span class="number">1</span>	<span class="number">4</span></span><br><span class="line"><span class="number">14</span>	<span class="number">10</span>	<span class="keyword">SQL</span> Server	<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span>				<span class="number">0</span>	<span class="number">0</span>	<span class="number">2</span></span><br><span class="line"><span class="number">20</span>	<span class="number">14</span>	T<span class="operator">-</span><span class="keyword">SQL</span>				<span class="number">10</span>	<span class="operator">/</span><span class="number">0</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">14</span>		<span class="number">0</span>	<span class="number">1</span>	<span class="number">3</span></span><br></pre></td></tr></table></figure>





<h2 id="6-SQL-标准-CTE"><a href="#6-SQL-标准-CTE" class="headerlink" title="6. SQL 标准 CTE"></a>6. SQL 标准 CTE</h2><h3 id="6-1-CTE-描述"><a href="#6-1-CTE-描述" class="headerlink" title="6.1 CTE 描述"></a>6.1 CTE 描述</h3><p> **common table expression **或 CTE 是从简单的 SELECT 语句创建的临时命名结果集，可用于后续的 SELECT 语句。 每个 SQL CTE 就像一个命名查询，其结果存储在一个虚拟表 (CTE) 中，以便稍后在主查询中引用。</p>
<h3 id="6-2-CTE实现-connect-by"><a href="#6-2-CTE实现-connect-by" class="headerlink" title="6.2 CTE实现 connect by"></a>6.2 CTE实现 connect by</h3><p>我们之间将如何使用 CTE 来实现 oracle 的层级查询功能，直接看例子。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- connect by</span></span><br><span class="line"><span class="keyword">SELECT</span>  level ,test_id, pid, test_val <span class="keyword">from</span> test_tree  </span><br><span class="line"><span class="keyword">start</span> <span class="keyword">with</span> test_id<span class="operator">=</span><span class="number">10</span> <span class="keyword">CONNECT</span> <span class="keyword">BY</span>  prior test_id<span class="operator">=</span> pid <span class="keyword">order</span> <span class="keyword">by</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> LEVEL, empno, ename, mgr, sal</span><br><span class="line"><span class="keyword">FROM</span> emp_</span><br><span class="line"><span class="keyword">CONNECT</span> <span class="keyword">BY</span> PRIOR empno <span class="operator">=</span> mgr</span><br><span class="line"><span class="keyword">START</span> <span class="keyword">WITH</span> ename <span class="operator">=</span> <span class="string">&#x27;BLAKE&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- ctes 实现上述 层次查询</span></span><br><span class="line"><span class="keyword">with</span> <span class="keyword">recursive</span> cte_tab(level, test_id, pid, test_val) &#123;</span><br><span class="line">	<span class="keyword">select</span> <span class="number">1</span> <span class="keyword">AS</span> level,test_id, pid, test_val <span class="keyword">from</span> test_tree <span class="keyword">where</span> test_id<span class="operator">=</span><span class="number">10</span> </span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">	<span class="keyword">select</span>  cte_tab.level<span class="operator">+</span><span class="number">1</span> ,test_treetest_.id, test_tree.pid, test_tree.test_val <span class="keyword">from</span> test_tree, cte_tab </span><br><span class="line">	<span class="keyword">where</span> cte_tab.test_id<span class="operator">=</span> test_val.pid</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> cte_tab;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h2 id="7-connect-by-算法"><a href="#7-connect-by-算法" class="headerlink" title="7. connect by 算法"></a>7. connect by 算法</h2><p>是一个查找算法。看完补充</p>
<h3 id="7-1-基本算法描述"><a href="#7-1-基本算法描述" class="headerlink" title="7.1 基本算法描述"></a>7.1 基本算法描述</h3><p>TODO </p>
<h3 id="7-2-算法伪代码"><a href="#7-2-算法伪代码" class="headerlink" title="7.2 算法伪代码"></a>7.2 算法伪代码</h3><p>后续补充。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>15445</title>
    <url>/2023/10/08/2-%E6%95%B0%E6%8D%AE%E5%BA%93/5-%E7%AC%94%E8%AE%B0/15445-01/</url>
    <content><![CDATA[<p>15445 是 一门 数据库管理系统的设计与实现 的课程。</p>
<p>课程大纲：</p>
<ul>
<li>Relational Databases</li>
<li>Storage</li>
<li>Execution</li>
<li>Concurrency Control</li>
<li>Recovery</li>
<li>Distributed Databases</li>
<li>Potpourri</li>
</ul>
<span id="more"></span>

<h2 id="关系模型关系代数"><a href="#关系模型关系代数" class="headerlink" title="关系模型关系代数"></a>关系模型关系代数</h2><h2 id="存储1"><a href="#存储1" class="headerlink" title="存储1"></a>存储1</h2><p>面向磁盘的数据库，所有数据都存储在磁盘中。</p>
<p>mmap 不要用在数据库存储种。</p>
<p>如何将数据存储在磁盘上 ？</p>
<p>需要暴露那些 API ？</p>
<hr>
<ol>
<li><p>如何表示磁盘上文件中的数据 ？</p>
</li>
<li><p>如果管理内存和磁盘之间的数据移动。（buffer pools）</p>
</li>
</ol>
<p>第一个问题</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">File Storage</span><br><span class="line">Page Layout</span><br><span class="line">Tuple Layout</span><br></pre></td></tr></table></figure>



<p>首先 如何将 数据库组织在 一系列 page 中。</p>
<p>然后 如何将 这些 page 保存在文件中</p>
<p>然后 这些 page 中的 tuple 是什么样的</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Storage Manager 负责维护磁盘上的文件。</span><br><span class="line">这些文件为 page 的 集合。</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">database PAGES</span><br><span class="line"></span><br><span class="line">一个 page 是一个固定大小的数据块</span><br><span class="line">page 可以包含 tuples，meta-data，indexes，<span class="built_in">log</span> records。任何东西。</span><br><span class="line"></span><br><span class="line">有的数据库要求 page 元数据和数据保存在一起，这样即使丢了一个page 也不会影响其他page(oracle)</span><br><span class="line">indirection 层 运行将 page <span class="built_in">id</span> 映射到文件中的某一个位置。</span><br><span class="line">page directory 记录page 在哪里</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hardware page（4k） 执行原子写入存储设备的最低底层的东西. 对磁盘进行 write和flush操作，</span><br><span class="line">										存储设备只能保证每次写入4kb时是原子的</span><br><span class="line">OS page （4k）</span><br><span class="line">database page （512b - 16 K）</span><br><span class="line"></span><br><span class="line">对磁盘进行 write 和 flush 只能保证 4kb 是原子的。</span><br></pre></td></tr></table></figure>





<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">page storage architecture</span><br><span class="line">不同数据库管理 page 文件使用不同的方式</span><br><span class="line"></span><br><span class="line">HEAP FILE</span><br><span class="line">Sequential/Sorted FILE</span><br><span class="line">hashing File</span><br></pre></td></tr></table></figure>



<p>HEAP FILE 无序的</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">提供操作page 的函数</span><br><span class="line">create</span><br><span class="line">get</span><br><span class="line">write</span><br><span class="line">delete</span><br><span class="line">  </span><br><span class="line">迭代page   </span><br><span class="line">iterating</span><br><span class="line">  </span><br><span class="line">元数据表示 page 是否有空间，有那些 page</span><br><span class="line">  </span><br><span class="line">两种方式表示 heap file</span><br><span class="line">  链表（没人用）、page目录</span><br></pre></td></tr></table></figure>





<h4 id="该如何表示page-存储架构"><a href="#该如何表示page-存储架构" class="headerlink" title="该如何表示page 存储架构?"></a>该如何表示page 存储架构?</h4><p>最常⻅的方式是使用Heap File Organization.</p>
<p>数据库中的 heap 文件是一个无序的 page 集合。</p>
<p>用 page 目录来表示page 对应的位置。</p>
<p><strong>更新数据和 page 目录的时候，有可能会崩溃，需要确保重建数据库的时候，能准确无误。</strong></p>
<p>每个 page 的  header里面有 page 大小，checksum，数据库版本之类的东西。</p>
<p>当新版本发布时候，可以判断根据不同版本来解析page。</p>
<h4 id="在一个page中，我们可以通过两种不同的方式来表示数据"><a href="#在一个page中，我们可以通过两种不同的方式来表示数据" class="headerlink" title="在一个page中，我们可以通过两种不同的方式来表示数据"></a>在一个page中，我们可以通过两种不同的方式来表示数据</h4><p>面向 tuple 的方式 和 log-structured 的策略</p>
<p>page header 后面有个 slot 数组 ，表明 tuple 数据。</p>
<p>page header 里面记录有条 tuple （没人这么干）</p>
<p>一个page 里都是相同 格式的 tuple。如果有碎片，需要 vaccum 进行整理碎片</p>
<p>tuple id 一般由 page id 加 slot 组成。</p>
<p>会有元数据记录 tuple 的组成。为了读取。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> r(id <span class="type">int</span> <span class="keyword">primary</span> key, val <span class="type">varchar</span>(<span class="number">6</span>));</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> r <span class="keyword">values</span>(<span class="number">101</span>,<span class="string">&#x27;aaa&#x27;</span>),(<span class="number">102</span>,<span class="string">&#x27;bbb&#x27;</span>),(<span class="number">103</span>,<span class="string">&#x27;ccc&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> r.ctid, r.<span class="operator">*</span> <span class="keyword">from</span> r;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- protgres ctid 表示了数据的位置。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> r <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">102</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> r <span class="keyword">values</span>(<span class="number">104</span>, <span class="string">&#x27;ddd&#x27;</span>); 会在后面追加。</span><br><span class="line"></span><br><span class="line"><span class="comment">--  pg 的 vacuum 会整理page。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="存储2"><a href="#存储2" class="headerlink" title="存储2"></a>存储2</h2><p>lsm tree 型数据库</p>
<p>优势，速度快。追加</p>
<p>劣势，查找慢。(压缩，建索引 提高查找速度)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Data Representation</span><br><span class="line">System Catalogs</span><br><span class="line">Storage Models</span><br></pre></td></tr></table></figure>

<p>数据类型，decimal。大多数情况下，page的大小是固定的。</p>
<p>如果一个 page 放不下数据的话。使用 overflow page。 通过一个指针，指向存数据的位置。</p>
<p>postgres 中叫做 TOAST。 还有一种是放在 外部文件中。</p>
<p>区分 olap 和 oltp</p>
<h2 id="buffer-pool-Manager"><a href="#buffer-pool-Manager" class="headerlink" title="buffer pool Manager"></a>buffer pool Manager</h2><p><strong>如何将磁盘中的数据库文件 或者 page 放到内存中</strong></p>
<p>什么时候将 page 读入内存，什么时候写出到磁盘。</p>
<p>malloc 一块内存，分成固定大小的 chunk，叫做  frame。</p>
<p>page table 用来跟踪内存(pool)中有哪些 page。必须是线程安全的.</p>
<p>通过page table 和 page id 就可以定位到 内存中 的 特定page</p>
<p>每个 page 都需要 额外的 元数据 来表示 page 发生的变化。page修改前， 需要写日志。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Dirty Flag   (page 是否被修改)</span><br><span class="line">Pin/Reference Counter  (引用该 page 的线程数，这样就不想写到磁盘中)</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Locks:</span><br><span class="line">	保护数据库中的逻辑内容，</span><br><span class="line">Latches:</span><br><span class="line">	保护内部数据或者线程</span><br></pre></td></tr></table></figure>



<p>page catalog 记录page 在数据库的文件的什么位置。需要持久化。</p>
<p>page table 记录 buffer pool （内存）中 page 的位置，不需要持久化。</p>
<p><strong>buffer pool optimizations</strong></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Multiple Buffers Pools</span><br><span class="line">Pre-Fetching</span><br><span class="line">Scan Sharing</span><br><span class="line">Buffer Pool Bypass</span><br></pre></td></tr></table></figure>

<p>多种策略：</p>
<p>每个表一个 buffer pool。例如一个 buffer pool 处理索引，一个处理表。这样可以减少冲突。</p>
<p>预取，提前把可能要读的数据放到内存中。</p>
<p>查询共享，跟结果缓存不一样，结果缓存要求查询必须一样。两个查询要扫描相同的数据。</p>
<p>不使用 操作系统的 缓存，一般使用 direct_io。posgtresql 是唯一一个依赖操作系统缓存的。</p>
<p>fwrite 不是真正的写到磁盘，只有调用 sync 才会写。</p>
<h3 id="替换策略"><a href="#替换策略" class="headerlink" title="替换策略"></a>替换策略</h3><p>LRU 算法 LRU 的变体clock；</p>
<p>不跟踪时间戳，跟踪每个 page 的标志位。(在检查过后，这个page 是否被访问过。)</p>
<p>环形 buffer，然后有个指针可以转，来检查标志位是 1还是 0。</p>
<p>如果标志位是 0.那么自从上次检查过后，page没有被访问过。可以移除。</p>
<p>优化 ：</p>
<ol>
<li><p>LRU-K，访问达到 k 次的才会被移除。</p>
</li>
<li><p>多个buff er池</p>
</li>
<li><p>priority hints(优先级提示)</p>
</li>
</ol>
<p><strong>如何处理 dirty pages ？</strong></p>
<p>page 上有一个 dirty bit 标志是否有 query 对他进行了修改。</p>
<p>其他内存池：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sorting + <span class="built_in">join</span></span><br><span class="line">query caches</span><br><span class="line">maintenance buffers</span><br><span class="line"><span class="built_in">log</span> buffers</span><br><span class="line">dictionary buffers</span><br></pre></td></tr></table></figure>



<p>project 1s</p>
<h2 id="hash-table"><a href="#hash-table" class="headerlink" title="hash table"></a>hash table</h2><ul>
<li>hash functions</li>
<li>Static Hashing Schemes</li>
<li>Dynamic Hashing Schemes</li>
</ul>
<p>hash functions</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">CRC-64 (1975)</span><br><span class="line">MurmurHash (2008)</span><br><span class="line">Google CityHash (2011)</span><br><span class="line">FaceBook XXHash (2012)   ---- 最好</span><br><span class="line">Google FarmHash (2014)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

























]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Consistency Models</title>
    <url>/2023/10/26/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/Consistency%20Models/</url>
    <content><![CDATA[<p>原文链接：<a href="https://jepsen.io/consistency">https://jepsen.io/consistency</a></p>
<p><em>This clickable map (adapted from</em> <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Bailis, Davidson, Fekete et al</a> <em>and</em> <a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti &amp; Vukolic</a><em>) shows the relationships between common consistency models for concurrent systems. Arrows show the relationship between consistency models. For instance, strict serializable implies both serializability and linearizability, linearizability implies sequential consistency, and so on. Colors show how available each model is, for a distributed system on an asynchronous network.</em></p>
<p>这个可点击的地图（改编自 Bailis、Davidson、Fekete 等人和 Viotti &amp; Vukolic）显示了并发系统的常见一致性模型之间的关系。 箭头显示一致性模型之间的关系。 例如，严格可串行化意味着可串行化和可线性化，可线性化意味着顺序一致性，等等。 颜色显示每个模型对于异步网络上的分布式系统的可用性。</p>
<span id="more"></span>



<p>​													<a href="https://jepsen.io/consistency/models/strict-serializable">Strict Serializability</a></p>
<p><strong>Unavailable</strong></p>
<p>Not available during some types of network failures. Some or all nodes must pause operations in order to ensure safety.</p>
<p>在某些类型的网络故障期间不可用。 为了确保安全，部分或全部节点必须暂停操作。</p>
<p><strong>Sticky Available</strong></p>
<p>Available on every non-faulty node, so long as clients only talk to the same servers, instead of switching to new ones.</p>
<p>只要客户端仅与相同的服务器通信，而不是切换到新的服务器，就可以在每个非故障节点上使用。</p>
<p><strong>Total Available</strong> </p>
<p>Available on every non-faulty node, even when the network is completely down.</p>
<p>即使网络完全关闭，也可在每个无故障节点上使用。</p>
<h2 id="Fundamental-Concepts"><a href="#Fundamental-Concepts" class="headerlink" title="Fundamental Concepts"></a>Fundamental Concepts</h2><p>Jepsen analyses the safety properties of distributed systems–most notably, identifying violations of consistency models. But what are consistency models? What phenomena do they allow? What kind of consistency does a given program really need?</p>
<p>Jepsen 分析分布式系统的安全属性，最值得注意的是，识别一致性模型的违规行为。 但什么是一致性模型？ 它们允许哪些现象发生？ 给定的程序真正需要什么样的一致性？</p>
<p>In this reference guide, we provide basic definitions, intuitive explanations, and theoretical underpinnings of various consistency models for engineers and academics alike.</p>
<p>在本参考指南中，我们为工程师和学者提供各种一致性模型的基本定义、直观解释和理论基础。</p>
<h3 id="Systems"><a href="#Systems" class="headerlink" title="Systems"></a>Systems</h3><p><em>Distributed</em> systems are a type of <em>concurrent</em> system, and much of the literature on concurrency control applies directly to distributed systems. Indeed, most of the concepts we’re going to discuss were originally formulated for single-node concurrent systems. There are, however, some important differences in <em>availability</em> and <em>performance</em>.</p>
<p>分布式系统是一种并发系统，许多有关并发控制的文献直接适用于分布式系统。 事实上，我们要讨论的大多数概念最初都是为单节点并发系统制定的。 然而，在可用性和性能方面存在一些重要差异。</p>
<p>Systems have a logical <em>state</em> which changes over time. For instance, a simple system could be a single integer variable, with states like <code>0</code>, <code>3</code>, and <code>42</code>. A mutex has only two states: locked or unlocked. The states of a key-value store might be maps of keys to values, for instance: <code>&#123;cat: 1, dog: 1&#125;</code>, or <code>&#123;cat: 4&#125;</code>.</p>
<p>系统具有随时间变化的逻辑状态。 例如，一个简单的系统可以是一个整数变量，具有 0、3 和 42 等状态。互斥锁只有两种状态：锁定或解锁。 键值存储的状态可能是键到值的映射，例如：{cat：1，dog：1}或{cat：4}。</p>
<h3 id="Processes"><a href="#Processes" class="headerlink" title="Processes"></a>Processes</h3><p>A <em>process</em><a href="https://jepsen.io/consistency#fn-1">1</a> is a logically single-threaded program which performs computation and runs operations. Processes are never asynchronous—we model asynchronous computation via independent processes. We say “logically single-threaded” to emphasize that while a process can only do one thing at a time, its implementation may be spread across multiple threads, operating system processes, or even physical nodes—just so long as those components provide the illusion of a coherent singlethreaded program.</p>
<p>进程 是一个逻辑上的单线程程序，用于执行计算和运行操作。 进程从来都不是异步的——我们通过独立进程对异步计算进行建模。 我们说“逻辑上单线程”是为了强调，虽然一个进程一次只能做一件事，但它的实现可以分布在多个线程、操作系统进程、甚至物理节点上——只要这些组件提供了假象 一个连贯的单线程程序。</p>
<h3 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h3><p>An operation is a transition from state to state. For instance, a single-variable system might have operations like <code>read</code> and <code>write</code>, which get and set the value of that variable, respectively. A counter might have operations like <em>increments</em>, <em>decrements</em>, and <em>reads</em>. An SQL store might have operations like <em>selects</em> and <em>updates</em>.</p>
<p>操作是从状态到状态的转换。 例如，单变量系统可能具有读取和写入等操作，分别获取和设置该变量的值。 计数器可能具有递增、递减和读取等操作。 SQL 存储可能具有选择和更新等操作。</p>
<h4 id="Functions-Arguments-amp-Return-Values"><a href="#Functions-Arguments-amp-Return-Values" class="headerlink" title="Functions, Arguments &amp; Return Values"></a>Functions, Arguments &amp; Return Values</h4><p>In theory, we could give every state transition a unique name. A lock has exactly two transition: <code>lock</code> and <code>unlock</code>. An integer register has an infinite number of reads and writes: <code>read-the-value-1</code>, <code>read-the-value-2</code>, …, and <code>write-1</code>, <code>write-2</code>, ….</p>
<p>理论上，我们可以给每个状态转换一个唯一的名称。 锁只有两个转换：“lock”和“unlock”。 整数寄存器具有无限次数的读取和写入：“read-the-value-1”、“read-the-value-2”、…和“write-1”、“write-2”、…。</p>
<p>To make this more tractable, we break up these transitions into <em>functions</em> like <code>read</code>, <code>write</code>, <code>cas</code>, <code>increment</code>, etc., and <em>values</em> that parameterize those functions. In a single register system, a write of 1 could be written:</p>
<p>为了使这个过程更容易处理，我们将这些转换分解为“函数”，如“read”、“write”、“cas”、“increment”等，以及参数化这些函数的“值”。 在单寄存器系统中，可以写入 1：</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:write</span><span class="punctuation">,</span> <span class="symbol">:value</span> <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>Given a key-value store, we might increment the value of key “a” by 3 like so:</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:increment</span><span class="punctuation">,</span> <span class="symbol">:value</span> [<span class="string">&quot;a&quot;</span> <span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>In a transactional store, the value could be a complex transaction. Here we read the current value of <code>a</code>, finding 2, and set <code>b</code> to <code>3</code>, in a single state transition:</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:txn</span><span class="punctuation">,</span> <span class="symbol">:value</span> [[<span class="symbol">:read</span> <span class="string">&quot;a&quot;</span> <span class="number">2</span>] [<span class="symbol">:write</span> <span class="string">&quot;b&quot;</span> <span class="number">3</span>]]&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Invocation-amp-Completion-Times"><a href="#Invocation-amp-Completion-Times" class="headerlink" title="Invocation &amp; Completion Times"></a>Invocation &amp; Completion Times</h4><p>Operations, in general, take time. In a multithreaded program, an operation might be a function call. In distributed systems, an operation might mean sending a request to a server, and receiving a response.</p>
<p>一般来说，操作需要时间。 在多线程程序中，操作可能是函数调用。 在分布式系统中，操作可能意味着向服务器发送请求并接收响应。</p>
<p>To model this, we say that each operation has an <em>invocation time</em> and, should it complete, a strictly greater <em>completion time</em>, both given by an imaginary<a href="https://jepsen.io/consistency#fn-2">2</a>, perfectly synchronized, globally accessible clock.<a href="https://jepsen.io/consistency#fn-3">3</a> We refer to these clocks as providing a <em>real-time</em> order, as opposed to clocks that only track causal ordering.<a href="https://jepsen.io/consistency#fn-4">4</a></p>
<p>为了对此进行建模，我们说每个操作都有一个<em>调用时间</em>，并且如果完成，则有一个严格更大的<em>完成时间</em>，两者都由一个虚构的[2](<a href="https://jepsen.io/consistency#fn-">https://jepsen.io/consistency#fn-</a> 2)、完全同步、全局可访问的时钟。<a href="https://jepsen.io/consistency#fn-3">3</a> 我们将这些时钟称为提供<em>实时</em>顺序，而不是仅跟踪的时钟 因果排序。<a href="https://jepsen.io/consistency#fn-4">4</a></p>
<h4 id="Concurrency"><a href="#Concurrency" class="headerlink" title="Concurrency"></a>Concurrency</h4><p>Since operations take time, two operations might overlap in time. For instance, given two operations A and B, A could begin, B could begin, A could complete, and then B could complete. We say that two operations A and B are <em>concurrent</em> if there is some time during which A and B are both executing.</p>
<p>Processes are single-threaded, which implies that no two operations executed by the same process are ever concurrent.</p>
<h4 id="Crashes"><a href="#Crashes" class="headerlink" title="Crashes"></a>Crashes</h4><p>If an operation does not complete for some reason (perhaps because it timed out or a critical component crashed) that operation <em>has no completion time</em>, and must, in general, be considered concurrent with every operation after its invocation. It may or may not execute.</p>
<p>A process with an operation is in this state is effectively stuck, and can never invoke another operation again. If it <em>were</em> to invoke another operation, it would violate our single-threaded constraint: processes only do one thing at a time.</p>
<h3 id="Histories"><a href="#Histories" class="headerlink" title="Histories"></a>Histories</h3><p>A <em>history</em> is a collection of operations, including their concurrent structure.</p>
<p>Some papers represent this as a set of operations, where each operation includes two numbers, representing their invocation and completion time; concurrent structure is inferred by comparing the time windows between processes.</p>
<p>Jepsen represents a history as an ordered list of invocation and completion operations, effectively splitting each operation in two. This representation is more convenient for algorithms which iterate over the history, keeping a representation of concurrent operations and possible states.</p>
<h3 id="Consistency-Models"><a href="#Consistency-Models" class="headerlink" title="Consistency Models"></a>Consistency Models</h3><p>A <em>consistency model</em> is a set of histories. We use consistency models to define which histories are “good”, or “legal” in a system. When we say a history “violates serializability” or “is not serializable”, we mean that the history is not in the set of serializable histories.</p>
<p>We say that consistency model A implies model B if A is a subset of B. For example, linearizability implies sequential consistency because every history which is linearizable is also sequentially consistent. This allows us to relate consistency models in a hierarchy.</p>
<p>Speaking informally, we refer to smaller, more restrictive consistency models as “stronger”, and larger, more permissive consistency models as “weaker”.</p>
<p>Not all consistency models are directly comparable. Often, two models allow different behavior, but neither contains the other.</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>Real transactions are serializable</title>
    <url>/2023/10/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/01-Real%20transactions%20are%20serializable/</url>
    <content><![CDATA[<p>Most databases offer a choice of several transaction isolation levels, offering a tradeoff between correctness and performance. However, that performance comes at a price, as developers must study their transactional interactions carefully or risk introducing subtle bugs. CockroachDB provides strong (“<code>SERIALIZABLE</code>”) isolation by default to ensure that your application always sees the data it expects. In this post I’ll explain what this means and how insufficient isolation impacts real-world applications.</p>
<p>大多数数据库提供多种事务隔离级别的选择，在正确性和性能之间进行权衡。 然而，这种性能是有代价的，因为开发人员必须仔细研究他们的事务交互，否则就有引入微妙错误的风险。 CockroachDB 默认提供强（“可串行化”）隔离，以确保您的应用程序始终看到它期望的数据。 在这篇文章中，我将解释这意味着什么以及隔离不足如何影响实际应用程序。</p>
<span id="more"></span>

<h2 id="Isolation-in-the-SQL-Standard"><a href="#Isolation-in-the-SQL-Standard" class="headerlink" title="Isolation in the SQL Standard"></a>Isolation in the SQL Standard</h2><p>The SQL standard defines four isolation levels:</p>
<ul>
<li><code>SERIALIZABLE</code></li>
<li><code>REPEATABLE READ</code></li>
<li><code>READ COMMITTED</code></li>
<li><code>READ UNCOMMITTED</code></li>
</ul>
<p><code>SERIALIZABLE</code> transactions run <em>as if</em> only one transaction were running at a time; the other isolation levels allow what the SQL standard euphemistically calls “the three phenomena”: dirty reads, non-repeatable reads, and phantom reads. <a href="https://heoric.github.io/2023/10/27/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/A%20Critique%20of%20ANSI%20SQL%20Isolation%20Levels/#more">Subsequent research</a> has identified additional “phenomena” and isolation levels.</p>
<p>SERIALIZABLE 事务的运行就像一次只有一个事务在运行一样； 其他隔离级别允许 SQL 标准委婉地称为“三种现象”：脏读、不可重复读和幻读。 随后的研究发现了额外的“现象”和隔离级别。</p>
<p>In modern research, these “phenomena” are more commonly called “anomalies”, or more bluntly, <a href="http://hpts.ws/papers//2015/jepsen.pdf">“lies”</a>. When you use a non-<code>SERIALIZABLE</code> isolation level, you’re giving the database permission to return an incorrect answer in the hope that it will be faster than producing the correct one. The SQL standard recognizes that this is dangerous and requires that <code>SERIALIZABLE</code> is the default isolation level. Weaker isolation levels are provided as a potential optimization for applications that can tolerate these anomalies.</p>
<p>在现代研究中，这些“现象”通常被称为“异常”，或者更直白地称为“谎言”。 当您使用非 SERIALIZABLE 隔离级别时，您就授予数据库返回错误答案的权限，希望它比生成正确答案更快。 SQL 标准认识到这是危险的，并要求 SERIALIZABLE 是默认的隔离级别。 提供较弱的隔离级别作为可以容忍这些异常的应用程序的潜在优化。</p>
<h2 id="Isolation-in-Real-Databases"><a href="#Isolation-in-Real-Databases" class="headerlink" title="Isolation in Real Databases"></a>Isolation in Real Databases</h2><p>Most databases ignore the specification that <code>SERIALIZABLE</code> be the default, and instead prioritize performance over safety by defaulting to the weaker <code>READ COMMITTED</code> or <code>REPEATABLE READ</code> isolation levels. More worryingly, some databases (including Oracle, and PostgreSQL prior to version 9.1) do not provide a serializable transaction implementation at all. Oracle’s implementation of the <code>SERIALIZABLE</code> isolation level is actually a weaker mode called “snapshot isolation”.</p>
<p>大多数数据库都会忽略 SERIALIZABLE 为默认值的规范，而是通过默认较弱的 READ COMMITTED 或 REPEATABLE READ 隔离级别来优先考虑性能而非安全性。 更令人担忧的是，一些数据库（包括Oracle和9.1版本之前的PostgreSQL）根本不提供可序列化事务的实现。 Oracle对SERIALIZABLE隔离级别的实现实际上是一种较弱的模式，称为“快照隔离”。</p>
<p>Snapshot isolation was developed after the initial standardization of the SQL language, but has been implemented in multiple database systems because it provides a good balance of performance and consistency. It is stronger than <code>READ COMMITTED</code> but weaker than <code>SERIALIZABLE</code>. It is similar to <code>REPEATABLE READ</code> but not exactly equivalent (<code>REPEATABLE READ</code> permits phantom reads but prevents write skew, while the reverse is true of snapshot isolation). The databases that have implemented snapshot isolation have made different decisions about how to fit it into the four SQL standard levels. Oracle takes the most aggressive stance, calling their snapshot implementation <code>SERIALIZABLE</code>. CockroachDB and Microsoft SQL Server are conservative and treat <code>SNAPSHOT</code> as a separate fifth isolation level. PostgreSQL (since version 9.1) falls in between, using snapshot isolation in place of <code>REPEATABLE READ</code>.</p>
<p>快照隔离是在 SQL 语言最初标准化之后开发的，但由于它提供了性能和一致性的良好平衡，已在多个数据库系统中实现。 它比 READ COMMITTED 强，但比 SERIALIZABLE 弱。 它与 REPEATABLE READ 类似，但并不完全相同（REPEATABLE READ 允许幻读，但防止写入倾斜，而快照隔离则相反）。 已经实现快照隔离的数据库对于如何将其适应四个 SQL 标准级别做出了不同的决定。 Oracle 采取了最激进的立场，称他们的快照实现是可串行化的。 CockroachDB 和 Microsoft SQL Server 比较保守，将 SNAPSHOT 视为单独的第五个隔离级别。 PostgreSQL（自版本 9.1 起）介于两者之间，使用快照隔离代替可重复读取。</p>
<p>Because serializable mode is used less often in databases that default to weaker isolation, it is often less thoroughly tested or optimized. For example, PostgreSQL has a fixed-size memory pool that it uses to track conflicts between serializable transactions, which can be exhausted under heavy load.</p>
<p>由于可序列化模式在默认隔离较弱的数据库中使用较少，因此通常没有经过彻底的测试或优化。 例如，PostgreSQL 有一个固定大小的内存池，用于跟踪可序列化事务之间的冲突，这些冲突在重负载下可能会耗尽。</p>
<p>Most database vendors treat stronger transaction isolation as an exotic option to be enabled by applications with exceptional consistency needs. Most applications, however, are expected to work with the faster but unsafe weak isolation modes. This backwards approach to the problem exposes applications to a variety of subtle bugs. At Cockroach Labs, we like thinking about transactional anomalies so much that we named all our conference rooms after them, but I would have a hard time advising with confidence when it is both safe and beneficial to choose <code>SNAPSHOT</code> isolation instead of <code>SERIALIZABLE</code>. Our philosophy is that it’s better to start with safety and work towards performance than the other way around.</p>
<p>大多数数据库供应商将更强的事务隔离视为一种奇特的选项，由具有特殊一致性需求的应用程序启用。 然而，大多数应用程序都希望使用更快但不安全的弱隔离模式。 这种向后解决问题的方法使应用程序面临各种微妙的错误。 在 Cockroach Labs，我们非常喜欢考虑事务异常，因此我们以它们的名字命名了所有会议室，但当选择快照隔离而不是串行隔离既安全又有益时，我很难充满信心地提出建议。 我们的理念是，最好从安全开始，努力提高性能，而不是相反。</p>
<h2 id="ACIDRain-Finding-Transactional-Bugs"><a href="#ACIDRain-Finding-Transactional-Bugs" class="headerlink" title="ACIDRain: Finding Transactional Bugs"></a>ACIDRain: Finding Transactional Bugs</h2><p><a href="http://www.bailis.org/papers/acidrain-sigmod2017.pdf">Recent research at Stanford</a> has explored the degree to which weak isolation leads to real-world bugs. Todd Warszawski and Peter Bailis examined 12 eCommerce applications and found 22 bugs related to transactions, five of which would have been avoided by running at a higher isolation level. Many of these bugs were simple to exploit and had direct financial implications. For example, in five of the tested applications, adding an item to your cart while checking out in another browser tab could result in the item being added to the order for free. The researchers developed tools to identify these vulnerabilities in a semi-automated way, paving the way for similar attacks (which the researchers dubbed “ACIDRain”) to become more prevalent.</p>
<p>斯坦福大学最近的研究探讨了弱隔离导致现实世界错误的程度。 Todd Warszawski 和 Peter Bailis 检查了 12 个电子商务应用程序，发现了 22 个与事务相关的错误，其中 5 个错误可以通过在更高的隔离级别运行来避免。 其中许多错误很容易被利用，并且会产生直接的财务影响。 例如，在五个测试的应用程序中，在另一个浏览器选项卡中结帐时将商品添加到购物车可能会导致该商品免费添加到订单中。 研究人员开发了工具以半自动方式识别这些漏洞，为类似攻击（研究人员称之为“ACIDRain”）变得更加普遍铺平了道路。</p>
<p>Most databases that default to weak transactional isolation provide workarounds, such as the (non-standard) <code>FOR UPDATE</code> and <code>LOCK IN SHARE MODE</code> modifiers for <code>SELECT</code> statements. When used correctly, these modifiers can make transactions safe even in weaker isolation levels. However, this is easy to get wrong, and even when used consistently these extensions introduce most of the downsides of <code>SERIALIZABLE</code> mode (in fact, overuse of <code>SELECT FOR UPDATE</code> in a <code>READ COMMITTED</code> transaction can perform worse than a <code>SERIALIZABLE</code> transaction, because it uses exclusive locks where serializability may only require shared locks). The ACIDRain research demonstrates the limitations of this technique: only one in three of the applications that attempted to use <code>SELECT FOR UPDATE</code> feature did so correctly; the others remained vulnerable.</p>
<p>大多数默认为弱事务隔离的数据库都提供了解决方法，例如 SELECT 语句的（非标准）FOR UPDATE 和 LOCK IN SHARE MODE 修饰符。 如果正确使用，这些修饰符即使在较弱的隔离级别下也可以使事务安全。 然而，这很容易出错，即使一致使用，这些扩展也会引入 SERIALIZABLE 模式的大部分缺点（事实上，在 READ COMMITTED 事务中过度使用 SELECT FOR UPDATE 的性能可能比 SERIALIZABLE 事务更差，因为它使用独占模式） 可串行化可能只需要共享锁的锁）。 ACIDRain 研究表明了该技术的局限性：尝试使用 SELECT FOR UPDATE 功能的应用程序中只有三分之一正确执行了操作； 其他人仍然很脆弱。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Databases that encourage the use of weaker isolation levels have prioritized performance over the safety of your data, leaving you to study subtle interactions between your transactions and implement error-prone workarounds. CockroachDB provides <code>SERIALIZABLE</code> transactions by default to ensure that you always see the consistency that you expect from a transactional database.</p>
<p>鼓励使用较弱隔离级别的数据库将性能置于数据安全之上，让您可以研究事务之间的微妙交互并实施容易出错的解决方法。 CockroachDB 默认提供 SERIALIZABLE 事务，以确保您始终看到事务数据库所期望的一致性。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>Linearizability versus Serializability</title>
    <url>/2023/10/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/00-Linearizability%20versus%20Serializability/</url>
    <content><![CDATA[<h2 id="Linearizability-versus-Serializability"><a href="#Linearizability-versus-Serializability" class="headerlink" title="[Linearizability versus Serializability]"></a>[Linearizability versus Serializability]</h2><p>线性化与串行化</p>
<p>原文链接</p>
<p>(<a href="http://www.bailis.org/blog/linearizability-versus-serializability/">http://www.bailis.org/blog/linearizability-versus-serializability/</a>)</p>
<span id="more"></span>

<p>Linearizability and serializability are both important properties about interleavings of operations in databases and distributed systems, and it’s easy to get them confused. This post gives a short, simple, and hopefully practical overview of the differences between the two.</p>
<p>线性化和串行化都是数据库和分布式系统中操作交错的重要属性，很容易将它们混淆。 这篇文章对两者之间的差异进行了简短、简单且实用的概述。</p>
<h4 id="Linearizability-single-operation-single-object-real-time-order"><a href="#Linearizability-single-operation-single-object-real-time-order" class="headerlink" title="Linearizability: single-operation, single-object, real-time order"></a>Linearizability: single-operation, single-object, real-time order</h4><p><em><a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">Linearizability</a> is a guarantee about single operations on single objects.</em> It provides a real-time (i.e., wall-clock) guarantee on the behavior of a set of single operations (often reads and writes) on a single object (e.g., distributed register or data item).</p>
<p>线性化是对单个对象上的单个操作的保证。 它为单个对象（例如分布式寄存器或数据项）上的一组单个操作（通常是读取和写入）的行为提供实时（即挂钟）保证。</p>
<p>In plain English, under linearizability, writes should appear to be instantaneous. Imprecisely, once a write completes, all later reads (where “later” is defined by wall-clock start time) should return the value of that write or the value of a later write. Once a read returns a particular value, all later reads should return that value or the value of a later write.</p>
<p>用简单的英语来说，在线性化下，写入应该看起来是瞬时的。 不精确地说，一旦写入完成，所有后续读取（其中“稍后”由挂钟开始时间定义）都应返回该写入的值或稍后写入的值。 一旦读取返回特定值，所有后续读取都应返回该值或后续写入的值。</p>
<p>Linearizability for read and write operations is synonymous with the term “atomic consistency” and is the “C,” or “consistency,” in Gilbert and Lynch’s <a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">proof of the CAP Theorem</a>. We say linearizability is <em>composable</em> (or “local”) because, if operations on each object in a system are linearizable, then all operations in the system are linearizable.</p>
<p>读写操作的线性化与术语“原子一致性”同义，在 Gilbert 和 Lynch 的 CAP 定理证明中是“C”或“一致性”。 我们说线性化是可组合的（或“局部的”），因为如果系统中每个对象的操作都是线性化的，那么系统中的所有操作都是线性化的。</p>
<h4 id="Serializability-multi-operation-multi-object-arbitrary-total-order"><a href="#Serializability-multi-operation-multi-object-arbitrary-total-order" class="headerlink" title="Serializability: multi-operation, multi-object, arbitrary total order"></a>Serializability: multi-operation, multi-object, arbitrary total order</h4><p><em>Serializability is a guarantee about transactions, or groups of one or more operations over one or more objects.</em> It guarantees that the execution of a set of transactions (usually containing read and write operations) over multiple items is equivalent to <em>some</em> serial execution (total ordering) of the transactions.</p>
<p><em>可串行性是对事务或对一个或多个对象的一个或多个操作组的保证。</em>它保证对多个项目执行一组事务（通常包含读取和写入操作）相当于<em>某些</em>串行 事务的执行（总排序）。</p>
<p>Serializability is the traditional “I,” or isolation, in <a href="http://sites.fas.harvard.edu/~cs265/papers/haerder-1983.pdf">ACID</a>. If users’ transactions each preserve application correctness (“C,” or consistency, in ACID), a serializable execution also preserves correctness. Therefore, serializability is a mechanism for guaranteeing database correctness. </p>
<p>可串行性是 ACID 中传统的“I”或隔离。 如果每个用户的事务都保持应用程序的正确性（“C”，即 ACID 中的一致性），则可序列化执行也会保持正确性。 因此，可序列化是保证数据库正确性的一种机制。1</p>
<p>Unlike linearizability, serializability does not—by itself—impose any real-time constraints on the ordering of transactions. Serializability is also not composable. Serializability does not imply any kind of deterministic order—it simply requires that <em>some</em> equivalent serial execution exists.</p>
<p>与线性化不同，可串行化本身并不对事务的排序施加任何实时约束。 可串行化也是不可组合的。 可串行性并不意味着任何类型的确定性顺序 - 它只是需要存在某种等效的串行执行。</p>
<h4 id="Strict-Serializability-Why-don’t-we-have-both"><a href="#Strict-Serializability-Why-don’t-we-have-both" class="headerlink" title="Strict Serializability: Why don’t we have both?"></a>Strict Serializability: Why don’t we have both?</h4><p>Combining serializability and linearizability yields <em>strict serializability</em>: transaction behavior is equivalent to some serial execution, and the serial order corresponds to real time. For example, say I begin and commit transaction T1, which writes to item <em>x</em>, and you later begin and commit transaction T2, which reads from <em>x</em>. A database providing strict serializability for these transactions will place T1 before T2 in the serial ordering, and T2 will read T1’s write. A database providing serializability (but not strict serializability) could order T2 before T1.<a href="http://www.bailis.org/blog/linearizability-versus-serializability/#fn:implementation">2</a></p>
<p>将串行化和线性化相结合会产生严格的串行化：事务行为相当于某种串行执行，并且串行顺序对应于实时。 例如，假设我开始并提交事务 T1，它写入项目 x，而您稍后开始并提交事务 T2，它从 x 读取。 为这些事务提供严格可串行性的数据库将在串行排序中将 T1 置于 T2 之前，并且 T2 将读取 T1 的写入。 提供可序列化（但不是严格可序列化）的数据库可以在 T1 之前排序 T2</p>
<p>As <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">Herlihy and Wing</a> note, “linearizability can be viewed as a special case of strict serializability where transactions are restricted to consist of a single operation applied to a single object.”</p>
<p>正如 Herlihy 和 Wing 所指出的，“线性化可以被视为严格串行化的一种特殊情况，其中事务仅限于由应用于单个对象的单个操作组成。”</p>
<h4 id="Coordination-costs-and-real-world-deployments"><a href="#Coordination-costs-and-real-world-deployments" class="headerlink" title="Coordination costs and real-world deployments"></a>Coordination costs and real-world deployments</h4><p>Neither linearizability nor serializability is achievable without coordination. That is we can’t provide either guarantee with availability (i.e., CAP “AP”) under an asynchronous network.<a href="http://www.bailis.org/blog/linearizability-versus-serializability/#fn:hardness">3</a></p>
<p>如果没有协调，线性化和串行化都无法实现。 也就是说，我们无法在异步网络下提供可用性保证（即 CAP“AP”）。3</p>
<p>In practice, your database is <a href="http://www.bailis.org/blog/when-is-acid-acid-rarely/">unlikely to provide serializability</a>, and your multi-core processor is <a href="http://preshing.com/20120930/weak-vs-strong-memory-models/">unlikely to provide linearizability</a>—at least by default. As the above theory hints, achieving these properties requires a lot of expensive coordination. So, instead, real systems often use cheaper-to-implement and often <a href="http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem/">harder-to-understand</a> models. This trade-off between efficiency and programmability represents a fascinating and challenging design space.</p>
<p>实际上，您的数据库不太可能提供可串行化，并且您的多核处理器也不太可能提供线性化——至少在默认情况下是这样。 正如上述理论所暗示的，实现这些特性需要大量昂贵的协调。 因此，实际系统通常使用实施成本较低且通常较难理解的模型。 效率和可编程性之间的这种权衡代表了一个令人着迷且具有挑战性的设计空间。</p>
<h4 id="A-note-on-terminology-and-more-reading"><a href="#A-note-on-terminology-and-more-reading" class="headerlink" title="A note on terminology, and more reading"></a>A note on terminology, and more reading</h4><p>One of the reasons these definitions are so confusing is that linearizability hails from the distributed systems and concurrent programming communities, and serializability comes from the database community. Today, almost everyone uses <em>both</em> distributed systems and databases, which often leads to overloaded terminology (e.g., “consistency,” “atomicity”).</p>
<p>这些定义如此令人困惑的原因之一是，线性化来自分布式系统和并发编程社区，而串行化来自数据库社区。 如今，几乎每个人都使用分布式系统和数据库，这常常导致术语过多（例如“一致性”、“原子性”）。</p>
<p>There are many more precise treatments of these concepts. I like <a href="http://link.springer.com/book/10.1007%2F978-3-642-15260-3">this book</a>, but there is plenty of free, concise, and (often) accurate material on the internet, such as <a href="https://www.cs.rochester.edu/~scott/458/notes/04-concurrent_data_structures">these notes</a>.</p>
<p>这些概念还有许多更精确的处理方法。 我喜欢这本书，但是互联网上有大量免费、简洁且（通常）准确的材料，例如这些笔记。</p>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ol>
<li><p>But it’s not the only mechanism!</p>
<p>但这不是唯一的机制！</p>
<p>Granted, serializability is (more or less) the most <em>general</em> means of maintaining database correctness. In what’s becoming one of my favorite “underground” (i.e., relatively poorly-cited) references, <a href="http://en.wikipedia.org/wiki/H._T._Kung">H.T. Kung</a> and <a href="http://en.wikipedia.org/wiki/Christos_Papadimitriou">Christos Papadimitriou</a> dropped a paper in SIGMOD 1979 on “<a href="http://www.eecs.harvard.edu/~htk/publication/1979-sigmod-kung-papadimitriou.pdf">An Optimality Theory of Concurrency Control for Databases</a>.” In it, they essentially show that, if all you have are transactions’ syntactic modifications to database state (e.g., read and write) and <em>no</em> information about application logic, serializability is, in some sense, “optimal”: in effect, a schedule that is not serializable might modify the database state in a way that produces inconsistency for some (arbitrary) notion of correctness that is not known to the database.</p>
<p>诚然，可序列化（或多或少）是维护数据库正确性的最通用方法。 在成为我最喜欢的“地下”（即引用相对较少的）参考文献之一中，H.T. Kung 和 Christos Papadimitriou 在 SIGMOD 1979 上发表了一篇关于“数据库并发控制的最优理论”的论文。 在其中，它们本质上表明，如果您拥有的只是事务对数据库状态的语法修改（例如，读取和写入）并且没有有关应用程序逻辑的信息，那么可串行性在某种意义上是“最佳的”：实际上，是一个时间表 不可序列化的可能会修改数据库状态，从而导致数据库未知的某些（任意）正确性概念产生不一致。</p>
<p>However, if <em>do</em> you know more about your user’s notions of correctness (say, you <em>are</em> the user!), you can often do a lot more in terms of concurrency control and can circumvent many of the fundamental overheads imposed by serializability. Recognizing when you don’t need serializability (and subsequently exploiting this fact) is the best way I know to “beat CAP.” <a href="http://www.bailis.org/blog/linearizability-versus-serializability/#fnref:mechanism">↩</a></p>
<p>但是，如果您更多地了解用户的正确性概念（例如，您是用户！），您通常可以在并发控制方面做更多的事情，并且可以规避可串行性带来的许多基本开销。 认识到何时不需要可序列化（并随后利用这一事实）是我所知道的“击败 CAP”的最佳方法。 ↩</p>
</li>
<li><p>Note that some implementations of serializability (such as two-phase locking with long write locks and long read locks) actually provide strict serializability. As <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">Herlihy and Wing</a> point out, other implementations (such as some MVCC implementations) may not.</p>
<p>请注意，某些可串行性的实现（例如具有长写锁和长读锁的两阶段锁定）实际上提供了严格的可串行性。 正如 Herlihy 和 Wing 指出的那样，其他实现（例如某些 MVCC 实现）可能不会。</p>
<p>So, why didn’t the early papers that defined serializability call attention to this real-time ordering? In some sense, real time doesn’t really matter: all serializable schedules are equivalent in terms of their power to preserve database correctness! However, there are some weird edge cases: for example, returning NULL in response to every read-only transaction is serializable (provided we start with an empty database) but rather unhelpful.</p>
<p>那么，为什么早期定义可串行化的论文没有引起人们对这种实时排序的关注呢？ 从某种意义上说，实时并不重要：所有可序列化的调度在保持数据库正确性方面的能力都是相同的！ 然而，有一些奇怪的边缘情况：例如，响应每个只读事务返回 NULL 是可序列化的（假设我们从空数据库开始），但毫无帮助。</p>
<p>One tantalizingly plausible theory for this omission is that, back in the 1970s when serializability theory was being invented, everyone was running on single-site systems anyway, so linearizability essentially “came for free.” However, I believe this theory is unlikely: for example, database pioneer <a href="http://en.wikipedia.org/wiki/Phil_Bernstein">Phil Bernstein</a> was already looking at distributed transaction execution in his SDD-1 system <a href="http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA131789">as early as 1977</a> (and there are <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4567884">older references</a> yet). Even in this early work, Bernstein (and company) are careful to stress that “there may in fact be <em>several</em> such equivalent serial orderings” [emphasis theirs]. To further put this theory to rest, Papadimitriou makes clear in his seminal <a href="https://www.cs.purdue.edu/homes/bb/cs542-06Spr-bb/SCDU-Papa-79.pdf">1979 JACM</a> article that he’s familiar with problems inherent in a distributed setting. (If you ever want to be blown away by the literature, look at how much of the foundational work on concurrency control was done by the early 1980s.) <a href="http://www.bailis.org/blog/linearizability-versus-serializability/#fnref:implementation">↩</a></p>
<p>对于这一遗漏，一个看似合理的理论是，早在 20 世纪 70 年代，当串行化理论被发明时，每个人都在单站点系统上运行，因此线性化本质上是“免费的”。 然而，我认为这个理论不太可能：例如，数据库先驱 Phil Bernstein 早在 1977 年就已经在他的 SDD-1 系统中研究分布式事务执行（并且还有更早的参考资料）。 即使在这项早期工作中，伯恩斯坦（和公司）也小心地强调“实际上可能有几个这样的等效序列顺序”[强调他们的]。 为了进一步证实这一理论，Papadimitriou 在其 1979 年 JACM 的开创性文章中明确表示，他熟悉分布式环境中固有的问题。 （如果您想被文献所震撼，请看看 20 世纪 80 年代初完成了多少并发控制的基础工作。） ↩</p>
</li>
<li><p>For distributed systems nerds: achieving linearizability for reads and writes is, in a formal sense, “easier” to achieve than serializability. This is probably deserving of another post (encouragement appreciated!), but here’s some intuition: terminating atomic register read&#x2F;write operations <a href="http://www.cse.huji.ac.il/course/2004/dist/p124-attiya.pdf">are achievable</a> in a fail-stop model. Yet atomic commitment—which is needed to execute multi-site serializable transactions (think: AC is to 2PC as consensus is to Paxos)—is not: the <a href="http://www.cs.utexas.edu/~lorenzo/corsi/cs380d/past/03F/notes/fischer.pdf">FLP result</a> says consensus is unachievable in a fail-stop model (hence <em>with One Faulty Process</em>), and (non-blocking) atomic commitment is <a href="http://link.springer.com/chapter/10.1007/BFb0022140">“harder” than consensus</a> (<a href="http://infoscience.epfl.ch/record/83471/files/1596162953p115-delporte.pdf">see also</a>). Also, keep in mind that linearizability for read-modify-write <a href="http://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf">is harder than</a> linearizable read&#x2F;write. (linearizable read&#x2F;write《 consensus《 atomic commitment) <a href="http://www.bailis.org/blog/linearizability-versus-serializability/#fnref:hardness">↩</a></p>
<p>对于分布式系统迷来说：从形式上来说，实现读写的线性化比串行化“更容易”实现。 这可能值得另一篇文章（感谢鼓励！），但这里有一些直觉：终止原子寄存器读&#x2F;写操作在故障停止模型中是可以实现的。 然而，原子承诺——执行多站点可序列化事务所需的原子承诺（想想：AC 之于 2PC，共识之于 Paxos）——却并非如此：FLP 结果表明，在故障停止模型中无法达成共识（因此出现了一个错误进程） ），并且（非阻塞）原子承诺比共识“更难”（另请参阅）。 另外，请记住，读-修改-写的线性化比线性化读&#x2F;写更难。 (线性化读&#x2F;写《共识》《原子承诺) ↩</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>How CockroachDB does distributed, atomic transactions</title>
    <url>/2023/10/31/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/02-distributed_atomic%20transactions/</url>
    <content><![CDATA[<p><em>This article was written in 2015 when CockroachDB was pre-beta. The product has evolved significantly since then. We will be updating this post to reflect the current status of CockroachDB. In the meantime, the <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer">transaction section</a> of the Architecture Document provides a more current description of CockroachDB’s transaction model.</em></p>
<p>本文写于 2015 年，当时 CockroachDB 还处于预测试阶段。 从那时起，该产品已经发生了显着的发展。 我们将更新这篇文章以反映 CockroachDB 的当前状态。 同时，架构文档的事务部分提供了 CockroachDB 事务模型的更新描述。</p>
<span id="more"></span>

<p>One of the headline features of CockroachDB is its full support for <a href="https://en.wikipedia.org/wiki/ACID">ACID transactions</a> across arbitrary keys in a distributed database. CockroachDB transactions apply a set of operations to the database while maintaining some key properties: Atomicity, Consistency, Isolation, and Durability (ACID). In this post, we’ll be focusing on how CockroachDB enables <strong>atomic</strong> transactions without using locks.</p>
<p>CockroachDB 的主要功能之一是完全支持分布式数据库中跨任意键的 ACID 事务。 CockroachDB 事务将一组操作应用于数据库，同时维护一些关键属性：原子性、一致性、隔离性和持久性 (ACID)。 在这篇文章中，我们将重点讨论 CockroachDB 如何在不使用锁的情况下实现原子事务。</p>
<p><strong>Atomicity</strong> can be defined as: 原子性可以定义为：</p>
<blockquote>
<p><em>For a group of database operations, either all of the operations are applied or none of them are applied.</em></p>
<p>对于一组数据库操作，要么应用所有操作，要么不应用任何操作。</p>
</blockquote>
<p>Without atomicity, a transaction that is interrupted may only write a portion of the changes it intended to make; this may leave your database in an inconsistent state.</p>
<p>如果没有原子性，被中断的事务可能只会写入其打算进行的部分更改； 这可能会使您的数据库处于不一致的状态。</p>
<h2 id="Strategy-战略"><a href="#Strategy-战略" class="headerlink" title="Strategy 战略"></a>Strategy 战略</h2><p>The strategy CockroachDB uses to provide atomic transactions follows these basic steps:</p>
<p>CockroachDB 用于提供原子事务的策略遵循以下基本步骤：</p>
<ol>
<li><p><strong>Switch</strong>: Before modifying the value of any key, the transaction creates a switch, which is a writeable value distinct from any of the real values being changed in the batch. The switch cannot be concurrently accessed – reads and writes of the switch are strictly ordered. The switch is initially “off,” and it can be switched to “on.”</p>
<p>开关：在修改任何键的值之前，事务会创建一个 switch，它是一个可写的值，与批次中正在更改的任何实际值不同。 开关不能同时访问——开关的读和写是严格顺序的。 开关最初是“关闭”的，可以切换到“打开”。</p>
</li>
<li><p><strong>Stage</strong>: The writer prepares several changes to the database, but does not overwrite any existing values; the new values are instead staged in proximity to the original values.</p>
<p>阶段：编写者准备对数据库进行多项更改，但不会覆盖任何现有值； 相反，新值会在接近原始值的位置上演。</p>
</li>
<li><p><strong>Filter</strong>: For any key with a staged value, reads for that key must check the state of the transaction’s switch before returning a value. If the switch is “off,” the reader returns the original value of the key. If the switch is “on,” the reader returns the staged value. Thus, all reads of a key with a staged value are filtered through the switch’s state.</p>
<p>过滤器：对于任何具有暂存值的键，读取该键必须在返回值之前检查事务开关的状态。 如果开关“关闭”，则读取器返回密钥的原始值。 如果开关处于“打开”状态，则读取器返回暂存值。 因此，对具有阶段值的键的所有读取都会通过开关的状态进行过滤。</p>
</li>
<li><p><strong>Flip</strong>: When the writer has prepared all changes in the transaction, the writer flips the switch to the “on” position. In combination with the filtering, all values staged as part of the transaction are immediately returned by any future reads.</p>
<p>翻转：当写入者准备好事务中的所有更改时，写入者将开关翻转到“打开”位置。 与过滤相结合，作为事务一部分暂存的所有值都将立即由任何未来的读取返回。</p>
</li>
<li><p><strong>Unstage</strong>: Once a transaction is completed (either aborted or committed), the staged values are cleaned up as soon as possible. If the transaction succeeded, then the original values are replaced by the staged values; on failure, the staged values are discarded. Note that unstaging is done asynchronously and does not need to have finished before the transaction is considered committed.</p>
<p>取消暂存：一旦事务完成（中止或提交），暂存的值将尽快清除。 如果交易成功，则原始值将被暂存值替换； 失败时，阶段值将被丢弃。 请注意，取消暂存是异步完成的，不需要在事务被视为已提交之前完成。</p>
</li>
</ol>
<h2 id="The-Detailed-Transaction-Process"><a href="#The-Detailed-Transaction-Process" class="headerlink" title="The Detailed Transaction Process"></a>The Detailed Transaction Process</h2><h3 id="Switch-CockroachDB-Transaction-Record"><a href="#Switch-CockroachDB-Transaction-Record" class="headerlink" title="Switch: CockroachDB Transaction Record"></a>Switch: CockroachDB Transaction Record</h3><p>To begin a transaction, a writer first needs to create a <strong>transaction record</strong>. The transaction record is used by CockroachDB to provide the <strong>switch</strong> in our overall strategy.</p>
<p>要开始事务，writer 首先需要创建事务记录。 CockroachDB 使用事务记录来提供我们整体策略的切换。</p>
<p>Each transaction record has the following fields:</p>
<p>每条事务记录都有以下字段：</p>
<ul>
<li>A <strong>Unique ID</strong> (UUID) which identifies the transaction.</li>
<li>A current <strong>state</strong> of <code>PENDING</code>, <code>ABORTED</code>, or <code>COMMITTED</code>.</li>
<li>A cockroach K&#x2F;V <strong>key</strong>. This determines where the “switch” is located in the distributed data store.</li>
</ul>
<p>The writer generates a transaction record with a new UUID in the <code>PENDING</code> state. The writer then uses a special CockroachDB command <code>BeginTransaction()</code> to store the transaction record. The record is co-located (i.e. on the same nodes in the distributed system) with the key in the transaction record.</p>
<p>writer 生成一条处于 PENDING 状态且具有新 UUID 的事务记录。 然后，writer 使用特殊的 CockroachDB 命令 BeginTransaction() 来存储事务记录。 该记录与交易记录中的密钥位于同一位置（即在分布式系统中的相同节点上）。</p>
<p>Because the record is stored at a single cockroach key, operations on it are strictly ordered (by a combination of raft and our underlying storage engine). The <strong>state</strong> of the transaction is the “on&#x2F;off” state of switch, with states of <code>PENDING</code> or <code>ABORTED</code> representing “off,” and <code>COMMITTED</code> representing “on.” The transaction record thus meets the requirements for our switch.</p>
<p>因为记录存储在单个 cockroach key 中，所以对其的操作是严格排序的（通过 raft 和我们的底层存储引擎的组合）。 事务的状态是 switch 的“开&#x2F;关”状态，其中 PENDING 或 ABORTED 状态代表“关”，COMMITTED 状态代表“开”。 这样事务记录就满足我们切换的要求了。</p>
<p>Note that the transaction state can move from <code>PENDING</code> to either <code>ABORTED</code> or <code>COMMITTED</code>, but cannot change in any other way (i.e. <code>ABORTED</code> and <code>COMMITTED</code> are permanent states).</p>
<p>请注意，事务状态可以从 PENDING 移动到 ABORTED 或 COMMITTED，但不能以任何其他方式更改（即 ABORTED 和 COMMITTED 是永久状态）。</p>
<h3 id="Stage-Write-Intents"><a href="#Stage-Write-Intents" class="headerlink" title="Stage: Write Intents"></a>Stage: Write Intents</h3><p>To <strong>stage</strong> the changes in a transaction, CockroachDB uses a structure called a <strong>write intent</strong>. Any time a value is written to a key as part of a transaction, it is written as a write intent.</p>
<p>为了暂存事务中的更改，CockroachDB 使用称为写入意图的结构。 任何时候作为事务的一部分将值写入键时，都会将其写入为写入意图。</p>
<p>This write intent structure contains the value that will be written if the transaction succeeds.</p>
<p>该写入意图结构包含事务成功时将写入的值。</p>
<p>The write intent also contains the <strong>key</strong> where the transaction record is stored. This is crucial: If a reader encounters a write intent, it uses this key value to locate the transaction record (the switch).</p>
<p>写意图还包含存储交易记录的密钥。 这一点至关重要：如果读取器遇到写入意图，它会使用此键值来定位事务记录（交换机）。</p>
<p>As a final rule, there can only be a single write intent on any key. If there were multiple concurrent transactions, it would be possible for one transaction to try to write to a key which has an active intent from another transaction on it. However, transaction concurrency is a complicated topic which we will cover in a later blog post (on transaction isolation); for now, we will assume that there is only one transaction at a time, and that an existing write intent must be from an abandoned transaction.</p>
<p>作为最终规则，任何键上只能有一个写入意图。 如果存在多个并发事务，则一个事务可能会尝试写入具有来自另一事务的活动意图的键。 然而，事务并发是一个复杂的主题，我们将在后面的博客文章（关于事务隔离）中介绍它； 现在，我们假设一次只有一个事务，并且现有的写入意图必须来自废弃的事务。</p>
<p>When writing to a key which already has a write intent:</p>
<p>当写入已经有写入意图的键时：</p>
<ol>
<li><p>Move the transaction record for the existing intent to the <code>ABORTED</code> state if it is still in the <code>PENDING</code> state. If the earlier transaction was <code>COMMITTED</code> or <code>ABORTED</code>, do nothing.</p>
<p>如果现有 Intent 的事务记录仍处于 PENDING 状态，则将其移至 ABORTED 状态。 如果较早的事务已提交或已中止，则不执行任何操作。</p>
</li>
<li><p>Clean up the existing intent from the earlier transaction, which will remove the intent.</p>
<p>清除先前事务中的现有意图，这将删除该意图。</p>
</li>
<li><p>Add a new intent for the concurrent transaction.</p>
<p>为并发事务添加新意图。</p>
</li>
</ol>
<h3 id="Filter-Reading-an-Intent"><a href="#Filter-Reading-an-Intent" class="headerlink" title="Filter: Reading an Intent"></a>Filter: Reading an Intent</h3><p>When reading a key, we must follow principle 3 of our overall strategy and consult the value of any switch before returning a value.</p>
<p>读取键时，我们必须遵循总体策略的原则 3，并在返回值之前查阅任何开关的值。</p>
<p>If the key contains a plain value (i.e. not a write intent), the reader is assured that there is no transaction in progress that involves this key, and that it contains the most recent committed value. The value is thus returned verbatim.</p>
<p>如果密钥包含纯值（即不是写入意图），则读者可以确信没有正在进行的涉及该密钥的事务，并且它包含最近提交的值。 因此，该值将逐字返回。</p>
<p>However, if the reader encounters a write intent, it means that a previous transaction was abandoned at some point before removing the intent (remember: we are assuming that there is only one transaction at a time). The reader needs to check the state of the transaction’s switch (the transaction record) before proceeding.</p>
<p>但是，如果读者遇到写入意图，则意味着在删除意图之前的某个时刻放弃了先前的事务（请记住：我们假设一次只有一个事务）。 读者在继续之前需要检查交易开关的状态（交易记录）。</p>
<ol>
<li><p>Move the transaction record for the existing intent to the <code>ABORTED</code> state if it is still in the <code>PENDING</code> state.</p>
<p>如果现有意图的事务记录仍处于“PENDING”状态，则将其移至“ABORTED”状态。</p>
</li>
<li><p>Clean up the existing intent from the earlier transaction, which will remove the intent.</p>
<p>清除先前事务中的现有意图，这将删除该意图。</p>
</li>
<li><p>Return the plain value for the key. If the earlier transaction was <code>COMMITTED</code>, the cleanup operation will have upgraded the staged value to the plain value; otherwise, this will return the original value of the key before the transaction.</p>
<p>返回密钥的纯值。 如果较早的事务是“COMMITTED”，则清理操作会将暂存值升级为纯值； 否则，这将返回交易前密钥的原始值。</p>
</li>
</ol>
<h3 id="Flip-Commit-the-Transaction"><a href="#Flip-Commit-the-Transaction" class="headerlink" title="Flip: Commit the Transaction"></a>Flip: Commit the Transaction</h3><p>To commit the transaction, the transaction record is updated to a state of <code>COMMITTED</code>.</p>
<p>要提交事务，事务记录将更新为 COMMITTED 状态。</p>
<p>All write intents written by the transaction are immediately valid: any future reads which encounters a write intent for this transaction will filter through the transaction record, see that it is committed, and return the value that was staged in the intent.</p>
<p>事务写入的所有写意图立即有效：任何遇到此事务的写意图的未来读取都将过滤事务记录，查看它是否已提交，并返回意图中暂存的值。</p>
<h4 id="Aborting-a-Transaction"><a href="#Aborting-a-Transaction" class="headerlink" title="Aborting a Transaction"></a>Aborting a Transaction</h4><p>A transaction can be aborted by updating the state of the transaction record to <code>ABORTED</code>. At this point, the transaction is permanently aborted and future reads will ignore write intents created by this transaction.</p>
<p>可以通过将事务记录的状态更新为 ABORTED 来中止事务。 此时，事务将永久中止，并且将来的读取将忽略该事务创建的写入意图。</p>
<h3 id="Unstage-Cleaning-up-Intents"><a href="#Unstage-Cleaning-up-Intents" class="headerlink" title="Unstage: Cleaning up Intents"></a>Unstage: Cleaning up Intents</h3><p>The system above already provides the property of atomic commits; however, the filtering step is expensive, because it requires writes across the distributed system to filter through a central location (the transaction record). This is undesirable behavior for a distributed system.</p>
<p>上面的系统已经提供了原子提交的属性； 然而，过滤步骤的成本很高，因为它需要跨分布式系统进行写入才能通过中央位置（事务记录）进行过滤。 对于分布式系统来说，这是不受欢迎的行为。</p>
<p>Therefore, after a transaction is completed, we remove the write intents it created as soon as possible: if a key has a plain value without a write intent, read operations do not need to be filtered and thus complete in a properly distributed fashion.</p>
<p>因此，在事务完成后，我们会尽快删除它创建的写意图：如果一个键具有没有写意图的纯值，则读操作不需要被过滤，从而以正确分布的方式完成。</p>
<h4 id="Cleanup-Operation"><a href="#Cleanup-Operation" class="headerlink" title="Cleanup Operation"></a>Cleanup Operation</h4><p>The cleanup operation can be called on a write intent when the associated transaction is no longer pending. It follows these simple steps:</p>
<p>当关联事务不再挂起时，可以根据写入意图调用清理操作。 它遵循以下简单步骤：</p>
<ul>
<li><p>If the transaction is <code>ABORTED</code>, the write intent is removed.</p>
<p>如果事务被中止，则写入意图将被删除。</p>
</li>
<li><p>If the transaction is <code>COMMITTED</code>, the write intent’s staged value is converted into the plain value of the key, and then the write intent is removed.</p>
<p>如果事务已提交，则写入意图的暂存值将转换为密钥的纯值，然后删除写入意图。</p>
</li>
<li><p>The cleanup operation is idempotent; that is, if two processes try to clean up an intent for the same key and transaction, the second operation will be a no-op.</p>
<p>清理操作是幂等的； 也就是说，如果两个进程尝试清理相同密钥和事务的意图，则第二个操作将是无操作。</p>
</li>
</ul>
<p>Cleanup is performed in the following cases:</p>
<ul>
<li><p>After a writer commits or aborts a transaction, it attempts to clean up every intent it wrote immediately.</p>
<p>在写入者提交或中止事务后，它会尝试立即清理其写入的每个意图。</p>
</li>
<li><p>When a write encounters another write intent from an earlier transaction.</p>
<p>当写入遇到来自较早事务的另一个写入意图时。</p>
</li>
<li><p>When a read encounters a write intent from an earlier transaction.</p>
<p>当读取遇到来自较早事务的写入意图时。</p>
</li>
</ul>
<p>By aggressively cleaning up expired write intents through multiple avenues, the necessary performance impact of filtering is minimized.</p>
<p>通过多种途径积极清理过期的写入意图，可以最大限度地减少过滤对性能的影响。</p>
<h2 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h2><p>With that, we have covered CockroachDB’s basic strategy for ensuring the atomicity of its distributed, lockless transactions.</p>
<p>至此，我们已经介绍了 CockroachDB 确保其分布式无锁事务原子性的基本策略。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>CockroachDB&#39;s consistency model</title>
    <url>/2023/10/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/03-CockroachDB&#39;s%20consistency%20model/</url>
    <content><![CDATA[<p>原文链接 <a href="https://www.cockroachlabs.com/blog/consistency-model/">https://www.cockroachlabs.com/blog/consistency-model/</a></p>
<p>A few days ago, prompted by a Hacker News post, my friend Ivo texted me saying “<em>Does your head ever explode when you’re thinking about databases and consistency semantics and whatever models? It just sounds like pointless taxonomy stuff. We are &lt;N, K&gt;-serializable whereas QuinoaDB is only ü-serializable</em>”. The answer is yes — my head does explode. I don’t think it’s pointless, though, although I agree that the discussions are generally unproductive.</p>
<p>几天前，在黑客新闻帖子的推动下，我的朋友 Ivo 给我发短信说：“当你思考数据库、一致性语义和任何模型时，你的头是否曾经爆炸过？ 这听起来像是毫无意义的分类学东西。 我们是&lt;N, K&gt;-可序列化的，而QuinoaDB仅是ü-可序列化的”。 答案是肯定的——我的头确实爆炸了。 不过，我不认为这是毫无意义的，尽管我同意讨论通常没有成果。</p>
<span id="more"></span>

<p>Separately, the other day a colleague told a user that “CockroachDB implements serializability, not linearizability”. While we say this statement often, and it is the best kind of correct, I don’t like it much because I think it doesn’t do us justice and it’s also not particularly helpful for the users — it doesn’t teach them very much about CockroachDB.</p>
<p>另外，有一天，一位同事告诉用户“CockroachDB 实现了可序列化，而不是线性化”。 虽然我们经常说这句话，而且它是最好的正确说法，但我不太喜欢它，因为我认为它对我们不公平，而且对用户也不是特别有帮助——它并没有教会他们太多 关于 CockroachDB 的更多信息。</p>
<p>In this post, I’m attempting to present the guarantees that CockroachDB gives and the ones it doesn’t, and offer my preferred marketing slogan summarizing it all.</p>
<p>The first section provides background and some terminology for consistency models to support the following, CockroachDB-specific section. It’s not formal, rigorous or exhaustive (I link to better sources, though) so readers who are familiar with these things might want to skip it and head straight to the section on CockroachDB’s consistency model.</p>
<p>在这篇文章中，我试图介绍 CockroachDB 提供的保证和它没有提供的保证，并提供我最喜欢的营销口号来总结这一切。</p>
<p>第一部分提供一致性模型的背景和一些术语，以支持以下特定于 CockroachDB 的部分。 它不是正式的、严格的或详尽的（不过，我链接到了更好的资源），因此熟悉这些内容的读者可能想跳过它，直接阅读 CockroachDB 一致性模型的部分。</p>
<h2 id="A-summary-of-database-consistency-models"><a href="#A-summary-of-database-consistency-models" class="headerlink" title="A summary of database consistency models"></a>A summary of database consistency models</h2><p>数据库一致性模型总结</p>
<p>First of all, a brief introduction to what we’re talking about. Databases let many “clients” access data concurrently, and so they need to define the semantics of these concurrent accesses: for example, what happens if two clients read and write the same data “at the same time”. Moreover, distributed and replicated databases generally store multiple copies of the data, usually over a network of machines, and so they need to define what complications can arise from the fact that different machines are involved in serving reads and writes to the same data: e.g. if I tell machine A to write a key, and then immediately after I ask machine B to read it, will machine B return the data that had been just written? Informally speaking, what we’d ideally want from our database is to hide the data distribution and replication from us and to behave as if all transactions were being run one at a time by a single machine. A database that provides this kind of execution is said to implement the “strict serializability” consistency model - that’s the holy grail.</p>
<p>首先，简单介绍一下我们正在谈论的内容。 数据库允许许多“客户端”并发访问数据，因此需要定义这些并发访问的语义：例如，如果两个客户端“同时”读取和写入相同的数据会发生什么。 此外，分布式和复制数据库通常通过机器网络存储数据的多个副本，因此它们需要定义不同机器参与对相同数据的读取和写入服务这一事实可能会产生哪些复杂性：例如 如果我告诉机器A写入一个密钥，然后我让机器B读取它后，机器B会立即返回刚刚写入的数据吗？ 通俗地说，我们理想的情况是对数据库隐藏数据分布和复制，并且表现得好像所有事务都由一台机器一次运行一个。 提供这种执行的数据库据说可以实现“严格可串行化”一致性模型 - 这是圣杯。</p>
<p>But, of course, we also want our database to be resilient to machine failure, and we want the transactions to execute fast, and we want many transactions to execute at the same time, and we want data for European customers to be served from European servers and not cross an ocean network link. All these requirements generally come in conflict with strict serializability. So then databases start relaxing the strict serializability guarantees, basically compromising on that front to get execution speed and other benefits. These compromises need precise language for explaining them. For example, consider a replicated database and a write operation executed by one of the replicas followed quickly by a read operation served by another one. What are admissible results for this read? Under strict serializability, the answer is clear — only the value of the preceding write is acceptable. Under more relaxed models, more values are allowed in addition to this one. But which values exactly? Is a random value coming out of thin air acceptable? Generally, no. Is the value of some other relatively recent write acceptable? Perhaps. To define things precisely, we need specialized vocabulary that’s used by well studied sets of rules (called “consistency models”).</p>
<p>但是，当然，我们也希望我们的数据库能够适应机器故障，我们希望事务能够快速执行，我们希望许多事务同时执行，我们希望欧洲客户的数据能够从欧洲提供 服务器而不是跨越海洋的网络链接。 所有这些要求通常与严格的可串行性相冲突。 因此，数据库开始放松严格的可串行性保证，基本上在这方面做出妥协以获得执行速度和其他好处。 这些妥协需要精确的语言来解释。 例如，考虑一个复制数据库，其中一个副本执行写入操作，然后很快由另一个副本执行读取操作。 这次阅读的可接受结果是什么？ 在严格的可串行性下，答案很明确——只有前面写入的值是可接受的。 在更宽松的模型下，除此之外还允许更多值。 但究竟是哪个值呢？ 凭空产生的随机值可以接受吗？ 一般来说，不会。 其他一些相对较新的写入的值是否可以接受？ 也许。 为了精确地定义事物，我们需要经过深入研究的规则集（称为“一致性模型”）使用的专门词汇。</p>
<p>Historically, both the distributed systems community and the databases community have evolved their own terminology and models for consistency. In more recent years, the communities have joined, driven by the advent of “distributed databases”, and the vocabularies have combined. Things are tricky though, plus different databases try to market themselves the best way they can, and so I think it’s fair to say that there’s a lot of confusion on the topic. I’ve been thinking about these things for a couple of years now in the context of CockroachDB, and I still always struggle to make unequivocal and clear statements on the subject. Additionally, I’ll argue that none of the standard lexicon describes CockroachDB very well. For a more systematic treaty on the different meanings of consistency, see <a href="https://heoric.github.io/2023/10/26/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/The%20Many%20Faces%20of%20Consistency/#more">The many faces of consistency</a> and <a href="https://jepsen.io/consistency">Jepsen’s treatment of the topic</a>.</p>
<p>从历史上看，分布式系统社区和数据库社区都发展了自己的术语和模型以实现一致性。 近年来，在“分布式数据库”出现的推动下，社区加入了，词汇也合并了。 但事情很棘手，加上不同的数据库试图以最好的方式推销自己，所以我认为可以公平地说，这个话题存在很多混乱。 几年来，我一直在 CockroachDB 的背景下思考这些事情，但我仍然很难就这个主题做出明确而清晰的陈述。 此外，我认为没有一个标准词典能够很好地描述 CockroachDB。 有关一致性的不同含义的更系统的条约，请参阅[The many faces of consistency]和[Jepsen’s treatment of the topic]。</p>
<h2 id="Transaction-isolation-levels-and-serializability"><a href="#Transaction-isolation-levels-and-serializability" class="headerlink" title="Transaction isolation levels and serializability"></a>Transaction isolation levels and serializability</h2><p>事务隔离级别和可串行性</p>
<p>The databases community has been describing behavior in terms of <em><em>transactions</em></em>, which are composite operations (made up of SQL queries). Transactions are subject to the ACID properties (<strong>A</strong>tomicity, <strong>C</strong>onsistency, <strong>I</strong>solation, <strong>D</strong>urability). This community was primarily interested in the behavior of concurrent transactions on a single server, not so much in the interactions with data replication — it was thus initially not concerned by the historical issues around distributed consistency. For our discussion, the <strong>I</strong>solation property is the relevant one: we have multiple transactions accessing the same data concurrently and we need them to be isolated from each other. Each one needs to behave, to the greatest extent possible, as if no other transaction was interfering with it. Ironically, the <strong>C</strong>onsistency in ACID refers to a concept that’s only tangentially related to what we’re talking about here — the fact that the database will keep indexes up to date automatically and will enforce foreign key constraints and such.</p>
<p>数据库社区一直用_事务_来描述行为，事务是复合操作（由 SQL 查询组成）。 事务受 ACID 属性（原子性、一致性、隔离性、持久性）的约束。 该社区主要对单个服务器上并发事务的行为感兴趣，而不是与数据复制的交互 - 因此它最初并不关心分布式一致性的历史问题。 对于我们的讨论，隔离属性是相关的：我们有多个事务同时访问相同的数据，并且我们需要它们彼此隔离。 每个交易都需要尽最大可能表现得好像没有其他交易干扰它一样。 讽刺的是，ACID 中的一致性指的是一个与我们这里讨论的内容无关的概念——数据库将自动保持索引最新并强制执行外键约束等事实。</p>
<p>To describe the possible degrees of transaction isolation, the literature and the ANSI standard enumerates a list of possible “anomalies” (examples of imperfect isolation), and, based on those, defines a couple of standard “isolation levels”: Read Uncommitted, Read Committed, Repeatable Read, Serializable. To give a flavor of what these are about, for example the Repeatable Read isolation level says that once a transaction has read some data, reading it again within the same transaction yields the same results. So, concurrent transactions modifying that data have to somehow not affect the execution of our reading transaction. However, this isolation level allows the Phantom Read anomaly. Basically, if a transaction performs a query asking for rows matching a condition twice, the second execution might return more rows than the first. For example, something like <code>select * from orders where value &gt; 1000</code> might return orders (a, b, c) the first time and (a, b, c, d) the second time (which is ironic given Repeatable Read’s name since one might call what just happened a non-repeatable read).</p>
<p>为了描述事务隔离的可能程度，文献和 ANSI 标准列举了一系列可能的“异常”（不完美隔离的示例），并在此基础上定义了几个标准“隔离级别”：读未提交、读 提交、可重复读取、可序列化。 为了说明这些内容的含义，例如可重复读取隔离级别表示，一旦事务读取了某些数据，在同一事务中再次读取它会产生相同的结果。 因此，修改该数据的并发事务必须以某种方式不影响我们读取事务的执行。 然而，此隔离级别允许幻读异常。 基本上，如果事务执行两次查询，要求匹配条件的行，则第二次执行可能会返回比第一次更多的行。 例如，像 select * from orders where value &gt; 1000 这样的东西可能会第一次返回订单 (a, b, c) ，第二次返回 (a, b, c, d) （鉴于可重复读取的名称，这很讽刺，因为可能会 将刚刚发生的事情称为不可重复读取）。</p>
<p>Frankly, the definitions of the ANSI isolation levels are terrible (also see <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">A Critique of ANSI SQL Isolation Levels</a>), arguably with the exception of the Serializable one. They have been defined narrow-mindedly with a couple of database implementations in mind and have not stood the test of time.</p>
<p>坦率地说，ANSI 隔离级别的定义非常糟糕（另请参阅《ANSI SQL 隔离级别批判》），可以说除了 Serialized 隔离级别之外。 它们的定义是狭隘的，只考虑了几种数据库实现，并且没有经受住时间的考验。</p>
<p>The <a href="https://www.cockroachlabs.com/blog/acid-rain/">Serializable isolation</a> level, which, as far as the SQL standard is concerned, is the gold standard, doesn’t allow any of the defined anomalies. In plain terms, it states that the database needs to ensure that transactions need to behave <em>as if</em> the transactions executed sequentially, one by one. The definition allows that database to choose the order of transactions in an equivalent sequential execution. This is less than ideal because it allows for the following scenario:</p>
<p>就 SQL 标准而言，可序列化隔离级别是黄金标准，不允许任何已定义的异常。 简单来说，它指出数据库需要确保事务的行为就像事务逐一按顺序执行一样。 该定义允许数据库选择等效顺序执行中的事务顺序。 这不太理想，因为它允许出现以下情况：</p>
<h3 id="HN1"><a href="#HN1" class="headerlink" title="HN1:"></a>HN1:</h3><p>We consider three transactions. The first one is <code>insert into hacker_news_comments (id, parent_id, text) values (1, NULL, &#39;a root comment&#39;)</code>. The second one is <code>insert into hacker_news_comments (id, parent_id, text) values (2, 1, &#39;OP is wrong&#39;)</code>. The third one is <code>select id, text from comments</code>.</p>
<p>我们考虑三个事务。 </p>
<p>第一个是 <code>insert into hacker_news_comments (id, parent_id, text) values (1, NULL, &#39;a root comment&#39;)</code>. </p>
<p>第二个是 <code>insert into hacker_news_comments (id, parent_id, text) values (2, 1, &#39;OP is wrong&#39;)</code>. </p>
<p>第三个是 <code>select id, text from comments</code>.</p>
<ul>
<li>I run transaction one.  我运行第一个</li>
<li>I yell across the room to my friend Tobi who’s just waiting to reply to my threads. 我在房间的lingg</li>
<li>Tobi runs transaction 2. Tobi 运行第二个</li>
<li>We then tell our friend Nathan to stop what he’s doing and read our thread.</li>
<li>He runs transaction 3 and gets a single result: <code>(2, &#39;OP is wrong&#39;)</code>.</li>
</ul>
<p>So, Nathan is seeing the response, but not the original post. That’s not good. And yet, it is allowed by the Serializable isolation level and, in fact, likely to occur in many distributed databases (spoiler alert: not in CRDB), assuming the actors were quick to yell at each other and run their transactions. The serial order in which the transactions appear to have executed is 2, 3, 1.</p>
<p>因此，Nathan 看到的是回复，而不是原始帖子。 这不好。 然而，它是可序列化隔离级别所允许的，并且事实上，它很可能发生在许多分布式数据库中（剧透警报：不在 CRDB 中），假设参与者很快就会互相大喊大叫并运行他们的事务。 事务执行的顺序是 2、3、1。</p>
<p>What has happened here is that the actors synchronized with each other outside of the database and expected the database’s ordering of transactions to respect “real time”, but the isolation levels don’t talk about “real time” at all. This seems to not have been a concern for the SQL standardization committee at the time, probably since this kind of thing simply wouldn’t happen if the database software runs entirely on one machine (however many database researchers were thinking about the issues of distributed databases as early as the 70s–for example, see <a href="https://www.cs.purdue.edu/homes/bb/cs542-06Spr-bb/SCDU-Papa-79.pdf">Papadimitriou paper on serializability</a>).</p>
<p>这里发生的情况是，参与者在数据库外部相互同步，并期望数据库的事务排序尊重“实时”，但隔离级别根本不谈论“实时”。 这似乎并不是当时 SQL 标准化委员会所关心的问题，可能是因为如果数据库软件完全运行在一台机器上，这种事情根本就不会发生（然而许多数据库研究人员正在考虑分布式数据库的问题） 早在 70 年代，例如，请参阅 Papadimitriou 关于可序列化性的论文）。</p>
<h2 id="Distributed-systems-and-linearizability"><a href="#Distributed-systems-and-linearizability" class="headerlink" title="Distributed systems and linearizability"></a>Distributed systems and linearizability</h2><p>分布式系统和线性化</p>
<p>While database people were concerned with transaction isolation, researchers in distributed and parallel systems were concerned with the effects of having multiple copies of data on the system’s operations. In particular, they were concerned with the semantics of “read” and “write” operations on this replicated data. So, the literature evolved a set of operation “consistency levels”, with names like “read your own writes”, “monotonic reads”, “bounded staleness”, “causal consistency”, and “linearizable” which all give guidance about what values a read operation can return under different circumstances. The original two problems in need of solutions were how to resolve concurrent writes to the same logical address from two writers at separate physical locations using local replicas (CPUs on their local cache, NFS clients on their local copy), and when&#x2F;how a stale copy should be updated (cache invalidation). The spectrum of possible solutions has been explored in different ways by the original communities: designers of memory caches were constrained by much tighter demands of programmers on consistency, whereas networked filesystems were constrained by unreliable networks to err on the side of more availability.</p>
<p>数据库人员关心事务隔离，而分布式并行系统的研究人员则关心拥有多个数据副本对系统操作的影响。 特别是，他们关心对此复制数据的“读”和“写”操作的语义。 因此，文献发展了一套操作“一致性级别”，其名称包括“读取自己的写入”、“单调读取”、“有界陈旧性”、“因果一致性”和“线性化”，这些名称都为什么值提供了指导 读操作可以在不同情况下返回。 最初需要解决的两个问题是如何使用本地副本（本地缓存上的 CPU、本地副本上的 NFS 客户端）解决来自不同物理位置的两个写入器对同一逻辑地址的并发写入，以及何时&#x2F;如何处理过时的问题。 应更新副本（缓存失效）。 最初的社区已经以不同的方式探索了一系列可能的解决方案：内存缓存的设计者受到程序员对一致性的更严格要求的限制，而网络文件系统则受到不可靠网络的限制，从而在更高的可用性方面犯了错误。</p>
<p>Generally speaking, this evolutionary branch of consistency models doesn’t talk about transactions. Instead, systems are modeled as collections of objects, with each object defining a set of operations it supports. For example, assuming we have a key-value store that provides the operations read(k) and write(k,v), the system obeys the “monotonic reads” model if, once a process reads the value of a key k, any successive read operation on k by that process will always return that same value or a more recent value. In other words, reads by any one process don’t “go backwards”.</p>
<p>一般来说，一致性模型的这一演化分支不涉及事务。 相反，系统被建模为对象的集合，每个对象定义它支持的一组操作。 例如，假设我们有一个提供 read(k) 和 write(k,v) 操作的键值存储，则系统遵循“单调读取”模型，如果进程读取键 k 的值，则任何 该进程对 k 的连续读取操作将始终返回相同的值或更新的值。 换句话说，任何一个进程的读取都不会“倒退”。</p>
<p>There’s two things to note about this model’s definition: first of all, it talks about a “process”, so the system has a notion of different threads of control. Understanding this is a burden; the serializable isolation level we discussed in the databases context did not need such a concept<a href="https://www.cockroachlabs.com/blog/consistency-model/#fn:1">1</a> — the user of a system did not need to think about what process is performing what operations. Second, this model is quite relaxed in comparison to others. If one process performs a write(“a”, 1) and later another process performs read(“a”) (and there’s no intervening writes to “a”), then the read might not return 1. The monotonic reads model describes various distributed systems where data is replicated asynchronously and multiple replicas can all serve reads.</p>
<p>这个模型的定义有两点需要注意：首先，它讨论的是“进程”，因此系统有不同控制线程的概念。 理解这一点是一种负担； 我们在数据库上下文中讨论的可序列化隔离级别不需要这样的概念1——系统的用户不需要考虑哪个进程正在执行什么操作。 其次，与其他模型相比，该模型相当宽松。 如果一个进程执行 write(“a”, 1)，然后另一个进程执行 read(“a”)（并且没有中间写入“a”），则读取可能不会返回 1。单调读取模型描述了各种 分布式系统，其中数据异步复制并且多个副本都可以提供读取服务。</p>
<p>The gold standard among these models is linearizability. It was <a href="https://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">formalized by Herlihy and Wing in a delightful paper</a>.</p>
<p>这些模型的黄金标准是线性化。 Herlihy 和 Wing 在一篇令人愉快的论文中将其正式化。</p>
<p>This model aims to describe systems with properties pretty similar to the ones guaranteed for database transactions by the Serializable isolation level. Informally, it says that operations will behave as if they were executed one at a time, and an operation that finished before another one began (according to “real time”) has to execute before the second one. This model, assuming systems can actually implement it efficiently, sounds really good. Let’s definite it more formally.</p>
<p>该模型旨在描述具有与可序列化隔离级别保证数据库事务的属性非常相似的系统。 非正式地，它表示操作的行为就像一次执行一个操作，并且在另一个操作开始之前完成的操作（根据“实时”）必须在第二个操作之前执行。 这个模型，假设系统实际上可以有效地实现它，听起来确实不错。 让我们更正式地确定它。</p>
<p>Usually, linearizability is defined at the level of a single, relatively simple “object” and then expanded to the level of a system comprised of many such objects. So, we have an object that affords a couple of operations, and we want to devise a set of rules for how these operations behave. An operation is modeled as an “invocation” (from a client to the object) followed by a “response” (from the object to the client). We’re talking in a concurrent setting, where many clients are interacting with a single object concurrently. We define a “history” to be a set of invocations and responses.</p>
<p>通常，线性化是在单个相对简单的“对象”级别上定义的，然后扩展到由许多此类对象组成的系统级别。 因此，我们有一个提供几个操作的对象，并且我们希望为这些操作的行为方式设计一组规则。 操作被建模为“调用”（从客户端到对象），然后是“响应”（从对象到客户端）。 我们正在讨论并发环境，其中许多客户端同时与单个对象交互。 我们将“历史”定义为一组调用和响应。</p>
<p>For example, say our object is a FIFO queue (providing the enqueue&#x2F;dequeue operations). Then a history might be something like:</p>
<p>例如，假设我们的对象是一个 FIFO 队列（提供入队&#x2F;出队操作）。 那么历史可能是这样的：</p>
<p><strong>H1:</strong></p>
<p>client 1: enqueue “foo”<br>client 1: ok<br>client 1: dequeue<br>client 1: ok (“foo”)<br>client 1: enqueue “bar”<br>client 2: enqueue “baz”<br>client 1: ok<br>client 2: ok<br>client 1: dequeue<br>client 1: ok (“baz”)</p>
<p>The first event in this history is an invocation by client 1, the second one is the corresponding response from the queue object. Responses for dequeue operations are annotated with the element they return.</p>
<p>该历史记录中的第一个事件是客户端 1 的调用，第二个事件是来自队列对象的相应响应。 出队操作的响应用它们返回的元素进行注释。</p>
<p>We say that a given history is “sequential” if every invocation is immediately followed by a response. H1 is not sequential since it contains, for example, this interleaving of operations:</p>
<p>如果每次调用后都立即有响应，我们就说给定的历史是“顺序的”。 H1 不是连续的，因为它包含例如以下操作的交错：</p>
<p>client 1: enqueue “bar”<br>client 2: enqueue “baz”</p>
<p>Sequential histories are easy to reason about and check for validity (e.g. whether or not our FIFO queue is indeed FIFO). Since H1 is not sequential, it’s a bit hard to say whether the last response client 1 got is copacetic. Here’s where we use linearizability: we say that a history H is linearizable if it is <em>equivalent</em> to some valid sequential history H’, where H’ contains the same events, possibly reorderdered under the constraint that, if a response op1 appears before an invocation op2 in H, then this order is preserved in H’. In other words, a history is linearizable if all the responses are valid according to a sequential reordering that preserves the order of non-overlapping responses.</p>
<p>顺序历史很容易推理和检查有效性（例如，我们的 FIFO 队列是否确实是 FIFO）。 由于 H1 不是连续的，所以很难说客户端 1 最后得到的响应是否一致。 这里是我们使用线性化的地方：我们说历史 H 是线性化的，如果它“等价”于某个有效的顺序历史 H’，其中 H’ 包含相同的事件，可能在以下约束下重新排序：如果响应 op1 出现在 H 中的调用 op2 之前，则 该顺序保留在 H’ 中。 换句话说，如果所有响应根据保留非重叠响应顺序的顺序重新排序都是有效的，则历史是可线性化的。</p>
<p>For example, H1 is in fact linearizable because it’s equivalent to the following sequential history:</p>
<p>例如，H1 实际上是可线性化的，因为它相当于以下顺序历史：</p>
<p>client 1: enqueue “foo”<br>client 1: ok<br>client 1: dequeue<br>client 1: ok (“foo”)<br>client 2: enqueue “baz”<br>client 2: ok<br>client 1: enqueue “bar”<br>client 1: ok<br>client 1: dequeue<br>client 1: ok (“baz”)</p>
<p>Now, an object is said to be linearizable if all the histories it produces are linearizable. In other words, no matter how the clients bombard our queue with requests concurrently, the results need to look as if the requests came one by one. If the queue is to claim linearizability, the implementation should use internal locking, or whatever it needs to do, to make this guarantee. Note that this model does not explicitly talk about replication, but the cases where it is of value are primarily systems with replicated state. If our queue is replicated across many machines, and clients talk to all of them for performing operations, “using internal locking” is not trivial but has to somehow be done if we want linearizability.</p>
<p>现在，如果一个对象产生的所有历史都是可线性化的，则称该对象是可线性化的。 换句话说，无论客户端如何并发地用请求轰炸我们的队列，结果都需要看起来像是请求是一个接一个地到来的。 如果队列要求线性化，则实现应使用内部锁定或任何需要执行的操作来保证这一点。 请注意，该模型没有明确讨论复制，但它有价值的情况主要是具有复制状态的系统。 如果我们的队列在许多机器上复制，并且客户端与所有机器通信以执行操作，那么“使用内部锁定”并不是微不足道的，但如果我们想要线性化，就必须以某种方式完成。</p>
<p>To raise the level of abstraction, a whole system is said to be linearizable if it can be modeled as a set of linearizable objects. Linearizability has this nice “local” property: it can be composed like that. So, for example, a key-value store that offers point reads and point writes can be modeled as a collection of registers, with each register offering a read and write operation. If the registers individually provide linearizability, then the store as a whole also does.</p>
<p>为了提高抽象级别，如果整个系统可以建模为一组可线性化对象，则称其为可线性化的。 线性化具有这个很好的“局部”属性：它可以这样组合。 因此，例如，提供点读取和点写入的键值存储可以建模为寄存器的集合，每个寄存器提供读取和写入操作。 如果寄存器单独提供线性化能力，那么存储作为一个整体也能提供线性化能力。</p>
<p>Two things are of note about the linearizable consistency model:</p>
<p>关于线性化一致性模型有两点值得注意：</p>
<p>First, there is a notion of “real time” used implicitly. Everybody is able to look at one clock on the wall so that it can be judged which operation finishes before another operation begins. The order of operations in our linearizable histories has a relation with the time indicated by this mythical clock.</p>
<p>首先，隐含地使用了“实时”的概念。 每个人都可以看着墙上的一个时钟，从而可以判断哪一个操作在另一操作开始之前完成。 我们线性化历史中的运算顺序与这个神话时钟所指示的时间有关。</p>
<p>Second, concurrent operations are allowed to execute in any order. For example, in our history H1, the last event might have been</p>
<p>其次，允许并发操作以任何顺序执行。 例如，在我们的历史 H1 中，最后一个事件可能是</p>
<p>client 1: ok (“bar”) because a serial history where enqueuing baz finishes before enqueuing baz begins would also have been acceptable.</p>
<p>client 1: ok (“bar”)，因为在 enqueuing baz 开始之前 enqueuing baz 完成的串行历史也是可以接受的。</p>
<p>It’s worth reminding ourselves that linearizability does not talk about transactions, so this model by itself is not well suited to be used by SQL databases. I guess one could shoehorn it by saying that the whole database is one object which provides one transaction operation, but then a definition needs to be provided for the functional specifications of this operation. We’re getting back to the ACID properties and the transaction isolation levels, and I’m not sure how the formalism would work exactly.</p>
<p>值得提醒我们的是，线性化不涉及事务，因此该模型本身不太适合 SQL 数据库使用。 我想人们可以通过说整个数据库是一个提供一个事务操作的对象来硬塞它，但随后需要为该操作的功能规范提供一个定义。 我们回到 ACID 属性和事务隔离级别，我不确定形式主义将如何准确地发挥作用。</p>
<p>What the literature does for advancing a database model to incorporate this relationship that linearizability has with time is to incorporate its ideas into the serializable transaction isolation level.</p>
<p>为了推进数据库模型以纳入线性化与时间的这种关系，文献所做的就是将其思想纳入可序列化事务隔离级别。</p>
<h3 id="A-note-on-clocks"><a href="#A-note-on-clocks" class="headerlink" title="A note on clocks"></a><em>A note on clocks</em></h3><p>关于时钟的注意事项</p>
<p>The mentioning of “real time” and the use of a global clock governing a distributed system are fighting words for some of my colleagues. It’s understandable since, on the one hand, Einstein realized that time itself is relative (different observers can perceive events to take place in different orders relative to each other) and, on the other hand, even if we are to ignore relativistic effects for practical purposes, this one true, shared clock doesn’t quite exist in the context of a distributed system. I’m not qualified to discuss relativistic effects beyond acknowledging that there is such a thing as <em><a href="https://link.springer.com/chapter/10.1007/978-3-662-45174-8_25"><em>relativistic linearizability</em></a></em>. I believe the casual database user can ignore them, but I’ll start blabbering if you ask me exactly why.</p>
<p>对于我的一些同事来说，提到“实时”和使用全局时钟来管理分布式系统都是争论不休的。 这是可以理解的，因为一方面，爱因斯坦意识到时间本身是相对的（不同的观察者可以感知事件以相对于彼此不同的顺序发生），另一方面，即使我们在实际中忽略相对论效应 出于目的，这个真正的共享时钟在分布式系统的上下文中并不完全存在。 除了承认存在“相对论线性化”这样的东西之外，我没有资格讨论相对论效应。 我相信普通的数据库用户可以忽略它们，但如果你问我到底为什么，我会开始喋喋不休。</p>
<p>The fact that there is no shared clock according to which we can decide ordering is a problem all too real for implementers of distributed systems like CockroachDB. The closest we’ve come is <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf">a system called TrueTime built by Google</a>, which provides tightly synchronized clocks and bounded errors brought front and center.</p>
<p>事实上，对于像 CockroachDB 这样的分布式系统的实现者来说，没有共享时钟来决定排序是一个非常现实的问题。 我们最接近的是一个由 Google 构建的名为 TrueTime 的系统，它提供了紧密同步的时钟和有限的误差。</p>
<p>As far as the linearizability model is concerned (which assumes that a shared clock exists), the way I think about it is that the model tells us what to expect if such a clock were to exist. Given that it doesn’t quite exist, then clients of the system can’t actually use it to record their histories perfectly: one can’t simply ask all the clients, or all the CockroachDB replicas, to log their operation invocations and responses and timestamp them using the local clocks, and then centralize all the logs and construct a history out of that. This means that verifying a system that claims to be linearizable isn’t trivial. In other words, Herlihy talks about histories but doesn’t describe how one might actually produce these histories in practice. But that doesn’t mean the model is not useful.</p>
<p>就线性化模型而言（假设存在共享时钟），我的想法是该模型告诉我们如果存在这样的时钟会发生什么。 鉴于它并不完全存在，那么系统的客户端实际上无法使用它来完美地记录其历史记录：不能简单地要求所有客户端或所有 CockroachDB 副本记录其操作调用和响应， 使用本地时钟为它们添加时间戳，然后集中所有日志并从中构建历史记录。 这意味着验证一个声称可线性化的系统并非易事。 换句话说，赫利希谈论历史，但没有描述人们如何在实践中真正产生这些历史。 但这并不意味着该模型没有用。</p>
<p>What a verifier can do is record certain facts like “I know that this invocation happened after this other invocation, because there was a causal relationship between them”. For certain operations for which there was not a causal relationship, the client might not have accurate enough timestamps to put in the history and so such pairs of events can’t be used to verify whether a history is linearlizable or not. Alternatively, another thing a verifier might do is relay all its operations through a singular “timestamp oracle”, whose recording would then be used to produce and validate a history. Whether such a construct is practical is debatable, though, since the mere act of sequencing all operations would probably introduce enough latency in them as to hide imperfections of the system under test.</p>
<p>验证者可以做的是记录某些事实，例如“我知道这次调用发生在另一次调用之后，因为它们之间存在因果关系”。 对于某些不存在因果关系的操作，客户端可能没有足够准确的时间戳来放入历史记录中，因此此类事件对不能用于验证历史记录是否可线性化。 或者，验证者可能做的另一件事是通过单个“时间戳预言机”中继其所有操作，然后使用其记录来生成和验证历史记录。 然而，这样的构造是否实用是有争议的，因为仅仅对所有操作进行排序的行为就可能会在其中引入足够的延迟，从而隐藏被测系统的缺陷。</p>
<h2 id="Bringing-the-worlds-together-strict-serializability"><a href="#Bringing-the-worlds-together-strict-serializability" class="headerlink" title="Bringing the worlds together: strict serializability"></a>Bringing the worlds together: strict serializability</h2><p>将世界结合在一起：严格的可串行性</p>
<p>As I was saying, the ANSI SQL standard defines the serializable transaction isolation as the highest level, but its definition doesn’t consider phenomena present in distributed databases. It admits transaction behavior that is surprising and undesirable because it doesn’t say anything about how some transactions need to be ordered with respect to the time at which the client executed them.</p>
<p>正如我所说，ANSI SQL 标准将可序列化事务隔离定义为最高级别，但其定义并未考虑分布式数据库中存在的现象。 它承认令人惊讶且不受欢迎的事务行为，因为它没有说明某些事务需要如何根据客户端执行它们的时间进行排序。</p>
<p>To cover these gaps, the term “strict serializability” has been introduced for describing (distributed) databases that don’t suffer from these undesirable behaviors.</p>
<p>为了弥补这些差距，引入了术语“严格可串行化”来描述不受这些不良行为影响的（分布式）数据库。</p>
<p>Strict serializability says that transaction behavior is equivalent to some serial execution, and the serial order of transactions corresponds to real time (i.e. a transaction started after another one finished will be ordered after it). Note that strict serializability (like linearizability) still doesn’t say anything about the relative ordering of concurrent transactions (but, of course, those transaction still need to appear to be “isolated” from each other). We’ll come back to this point in the next sections.</p>
<p>严格的可串行性表示事务行为相当于某种串行执行，并且事务的串行顺序对应于实时（即，在另一个事务完成之后开始的事务将在它之后排序）。 请注意，严格的可串行性（如线性化）仍然没有说明并发事务的相对顺序（但是，当然，这些事务仍然需要看起来彼此“隔离”）。 我们将在下一节中回到这一点。</p>
<p>Under strict serializability, the system behavior outlined in the Hacker News posts example from the Serializability section is not permitted. Databases described by the strict serializability model must ensure that the final read, Nathan’s, must return both the root comment and the response. Additionally, the system must ensure that a query like <code>select * from hacker_news_comments</code> never returns the child comment without the parent, regardless of the the time when the query is executed (i.e. depending on the time when it’s executed, it can return an empty set, the root, or both the root and the child). We’ll come back to this point when discussing CRDB’s guarantees.</p>
<p>在严格的可序列化性下，不允许出现“可序列化性”部分中的黑客新闻帖子示例中概述的系统行为。 严格的可序列化模型描述的数据库必须确保 Nathan 的最终读取必须返回根注释和响应。 此外，系统必须确保像 <code>select * from hacker_news_comments </code> 这样的查询永远不会返回没有父评论的子评论，无论查询执行的时间如何（即，根据执行的时间，它可以返回一个空集， 根，或根和孩子）。 当我们讨论 CRDB 的保证时，我们会回到这一点。</p>
<p><a href="https://cloud.google.com/spanner/docs/true-time-external-consistency#2">Google’s Spanner uses the term “external consistency”</a> instead of “strict serializability”. I like that term because it emphasizes the difference between a system that provides “consistency” for transactions known to the database to be causally related and systems that don’t try to infer causality and offer stronger guarantees (or, at least, that’s how me and my buddies interpret the term). For example, remembering the Hacker News example, there are systems that allow Tobi to explicitly tell the database that his transaction has been “caused” by my transaction, and then the system guarantees that the ordering of the two transaction will respect this. Usually this is done through some sort of “causality tokens” that the actors pass around between them. In contrast, Spanner doesn’t require such cooperation from the client in order to prevent the bad outcome previously described: even if the clients coordinated “externally” to the database (e.g, by yelling across the room), they’ll still get the consistency level they expect.</p>
<p><font color="red">谷歌的 Spanner 使用术语“外部一致性”而不是“严格的可序列化性”。</font> 我喜欢这个术语，因为它强调了为数据库已知的因果关系事务提供“一致性”的系统与不尝试推断因果关系并提供更强保证的系统之间的区别（或者，至少，这就是我的方式） 我的朋友解释了这个词）。 例如，记住 Hacker News 的例子，有些系统允许 Tobi 明确地告诉数据库他的交易是由我的交易“引起”的，然后系统保证两个交易的顺序将尊重这一点。 通常这是通过演员之间传递的某种“因果关系令牌”来完成的。 相比之下，Spanner 不需要客户端的这种合作来防止前面描述的不良结果：即使客户端在数据库“外部”进行协调（例如，通过在房间里大喊大叫），他们仍然会得到 他们期望的一致性水平。</p>
<p>Peter Bailis has more words on <a href="http://www.bailis.org/blog/linearizability-versus-serializability/">Linearizability, Serializability and Strict Serializability</a>.</p>
<p>Peter Bailis 对线性化、可串行化和严格可串行化有更多的论述。</p>
<h2 id="CockroachDB’s-consistency-model-more-than-serializable-less-than-strict-serializability"><a href="#CockroachDB’s-consistency-model-more-than-serializable-less-than-strict-serializability" class="headerlink" title="CockroachDB’s consistency model: more than serializable, less than strict serializability"></a>CockroachDB’s consistency model: more than serializable, less than strict serializability</h2><p>Now that we’ve discussed some general concepts, let’s talk about how they apply to CockroachDB. CockroachDB is an open-source, transactional, SQL database and it’s also a distributed system. In my opinion, it comes pretty close to being the Holy Grail of databases: it offers a high degree of “consistency”, it’s very resilient to machine and network failures, it scales well and it performs well. This combination of features already makes it unique enough; the system goes beyond that and brings new concepts that are quite game-changing — good, principled control over data placement and read and write latency versus availability tradeoffs in geographically-distributed clusters. All without ever sacrificing things we informally refer to as “consistency” and “correctness” in common parlance. Also it’s improving every day at a remarkable pace. I’m telling you — <a href="https://www.cockroachlabs.com/get-started-cockroachdb/">you need to try this thing</a>!</p>
<p>现在我们已经讨论了一些一般概念，接下来我们来谈谈它们如何应用于 CockroachDB。 CockroachDB 是一个开源的事务性 SQL 数据库，也是一个分布式系统。 在我看来，它非常接近数据库的圣杯：它提供了高度的“一致性”，它对机器和网络故障具有很强的弹性，它具有良好的扩展性并且性能良好。 这种功能组合已经使其足够独特； 该系统超越了这一点，并带来了完全改变游戏规则的新概念——对数据放置、读写延迟与地理分布式集群中的可用性权衡进行良好的、有原则的控制。 所有这一切都没有牺牲我们非正式地称为“一致性”和“正确性”的东西。 而且它每天都在以惊人的速度进步。 我告诉你——你需要尝试一下这个东西！</p>
<p>But back to the subject at hand — the consistency story. CockroachDB is a complex piece of software; understanding how it all works in detail is not tractable for most users, and indeed it will not even be a good proposition for all the engineers working on it. We therefore need to model it and present a simplified version of reality. The model needs to be as simple as possible and as useful as possible to users, without being misleading (e.g. suggesting that outcomes that one might think are undesirable are not possible when in fact they are). Luckily, because CockroachDB was always developed under a “correctness first” mantra, coming up with such a model is not too hard, as I’ll argue.</p>
<p>但回到我们手头的主题——一致性的故事。 CockroachDB 是一个复杂的软件； 对于大多数用户来说，理解它的详细工作原理并不容易，事实上，对于所有致力于它的工程师来说，这甚至不是一个好建议。 因此，我们需要对其进行建模并呈现现实的简化版本。 该模型需要尽可能简单并且对用户尽可能有用，而不会产生误导（例如，暗示人们可能认为不期望的结果是不可能的，而实际上它们是不可能的）。 幸运的是，因为 CockroachDB 始终是在“正确性第一”的口号下开发的，所以正如我所说，提出这样一个模型并不难。</p>
<p>There’s a standard disclosure that comes with our software: the system assumes that the clocks on the Cockroach nodes are somewhat synchronized with each other. The clocks are allowed to drift away from each other up to a configured “maximum clock offset” (by default 500ms). Operators need to run NTP or other clock synchronization mechanism on their machines. The system detects when the drift approaches the maximum allowed limit and shuts down some nodes, alerting an operator[^2]. Theoretically, I think more arbitrary failures modes are possible if clocks get unsynchronized quickly. More on the topic in Spencer’s post <a href="https://www.cockroachlabs.com/blog/living-without-atomic-clocks">“Living Without Atomic Clocks.”</a></p>
<p>我们的软件附带了一个标准披露：系统假设 Cockroach 节点上的时钟在某种程度上彼此同步。 允许时钟彼此漂移，最多可达配置的“最大时钟偏移”（默认为 500ms）。 运营商需要在他们的机器上运行NTP或其他时钟同步机制。 系统会检测漂移何时接近最大允许限制并关闭一些节点，从而向操作员发出警报[^2]。 从理论上讲，我认为如果时钟快速不同步，则可能会出现更多任意故障模式。 有关该主题的更多信息，请参阅斯宾塞的文章“没有原子钟的生活”。</p>
<p>Back to the consistency. For one, CockroachDB implements the serializable isolation level for transactions, as specified by the SQL standard. In contrast to most other databases which don’t offer this level of isolation as the default (<a href="https://blog.dbi-services.com/oracle-serializable-is-not-serializable/">or at all, for crying out loud!</a>), this is the only isolation level we offer; users can’t opt for a lesser one. We, the CockroachDB authors, collectively think that any lower level is just asking for pain. It’s fair to say that it’s generally extremely hard to reason about the other levels and the consequences of using them in an application (see the <a href="http://www.bailis.org/papers/acidrain-sigmod2017.pdf">ACIDRain paper</a> for what can go wrong when using lower isolation levels). I’m not trying to be condescending; up until the 2.1 version we used to offer another relatively high level of isolation as an option (Snapshot Isolation), but it turned out that it (or, at least, our implementation of it) had complex, subtle consequences that even we hadn’t fully realized for the longest time. Thus, we ripped it out and instead improved the performance of the our implementation ensuring serializability as much as possible. Below serializability be dragons.	</p>
<p>回到一致性。 其一，CockroachDB 实现了 SQL 标准指定的事务的可序列化隔离级别。 与大多数其他不提供这种默认隔离级别（或者根本不提供这种隔离级别）的数据库相比，这是我们提供的唯一隔离级别； 用户不能选择较小的。 我们，CockroachDB 作者，集体认为任何较低的级别都是自找痛苦。 公平地说，通常很难推理其他级别以及在应用程序中使用它们的后果（请参阅 ACIDRain 论文，了解使用较低隔离级别时可能会出现的问题）。 我并不是想表现出居高临下的态度。 在 2.1 版本之前，我们曾经提供另一个相对较高级别的隔离作为选项（快照隔离），但事实证明它（或者至少是我们对它的实现）产生了复杂而微妙的后果，甚至我们也没有意识到” 最长的时间没有完全实现。 因此，我们将其删除，并提高了实现的性能，尽可能确保可序列化。 下面的可序列化是龙。</p>
<p>But simply saying that we’re serializable is selling our system short. We offer more than that. We do not allow the bad outcome in the Hacker News commenting scenario.</p>
<p>但仅仅说我们是可序列化的就低估了我们的系统。 我们提供的远不止这些。 我们不允许黑客新闻评论场景出现不良结果。</p>
<p>CockroachDB doesn’t quite offer strict serializability, but we’re fairly close to it. I’ll spend the rest of the section explaining how exactly we fail strict serializability, what our guarantees actually are, and some gotchas.</p>
<p>CockroachDB 并没有完全提供严格的可序列化性，但我们已经相当接近了。 我将用本节的其余部分来解释我们到底是如何失败的严格的可序列化，我们的保证实际上是什么，以及一些陷阱。</p>
<h3 id="No-stale-reads"><a href="#No-stale-reads" class="headerlink" title="No stale reads"></a>No stale reads</h3><p>If there’s one canned response I wish we’d give users that pop into our chat channels asking about the consistency model, I think it should be “CockroachDB doesn’t allow stale reads”. This should be the start of all further conversations, and in fact I think it will probably preempt many conversations. Stating this addresses a large swath of anomalies that people wonder about (in relation to distributed systems). “No stale reads” means that, once a write transaction committed, every read transaction starting afterwards[^3] will see it.</p>
<p>如果我希望我们能给那些突然进入我们的聊天频道询问一致性模型的用户一个预设的回复，我认为它应该是“CockroachDB 不允许过时的读取”。 这应该是所有进一步对话的开始，事实上我认为它可能会抢占许多对话。 声明这一点解决了人们想知道的大量异常现象（与分布式系统有关）。 “无陈旧读取”意味着，一旦写事务被提交，随后开始的每个读取事务[^3]都会看到它。</p>
<p>Internalizing this is important and useful. It does not come by chance; the system works hard for it and so have we, the builders. In the Hacker News comments example, once I have committed my root comment, a new transaction by Nathan is guaranteed to see it. Yes, our system is distributed and data is replicated. Yes, Nathan might be talking to a different node than I was, maybe a node with a clock that’s trailing behind. In fact, the node I was talking to might have even crashed in the meantime. Doesn’t matter. If Nathan is able to read the respective table, he will be able to read my write.</p>
<p>内化这一点很重要而且有用。 它不是偶然出现的；它是偶然发生的。 系统为此努力工作，我们建设者也是如此。 在黑客新闻评论示例中，一旦我提交了我的根评论，Nathan 的新事务就一定会看到它。 是的，我们的系统是分布式的，数据是复制的。 是的，Nathan 可能正在与一个与我不同的节点通信，也许是一个时钟落后的节点。 事实上，我正在交谈的节点甚至可能在此期间崩溃了。 没关系。 如果Nathan能够读取相应的表，他将能够读取我的写入。</p>
<p>Beyond serializability, saying “no stale reads” smells like linearizability (and, thus, strict serializability) since “staleness” is related to the passing of time. In fact, when people come around asking for linearizability, I conjecture that most will be satisfied by this answer. I think this is what I’d be asking for if I hadn’t educated myself specifically on the topic. Relatedly, this is also what the C(onsistency) in the famous <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> is asking for. And we have it.</p>
<p>除了可序列化性之外，说“没有过时的读取”闻起来像线性化（因此也是严格的可序列化），因为“过时性”与时间的流逝有关。 事实上，当人们询问线性化时，我猜大多数人都会对这个答案感到满意。 我想如果我没有专门针对这个主题进行自我教育的话，这就是我所要求的。 与此相关，这也是著名的 CAP 定理中的 C（一致性）所要求的。 我们有它。</p>
<p>So why exactly don’t we claim strict serializability?</p>
<p>那么我们为什么不要求严格的可序列化呢？</p>
<h3 id="CockroachDB-does-not-offer-strict-serializability"><a href="#CockroachDB-does-not-offer-strict-serializability" class="headerlink" title="CockroachDB does not offer strict serializability"></a>CockroachDB does not offer strict serializability</h3><p>Even though CRDB guarantees (say it with me) “no stale reads”, it still can produce transaction histories that are not linearizable.</p>
<p>尽管 CRDB 保证（跟我说）“没有过时的读取”，但它仍然会产生不可线性化的事务历史记录。</p>
<p>Consider the history HN2 (assume every statement is its own transaction, for simplicity):</p>
<p>考虑历史 HN2（为简单起见，假设每个语句都是它自己的事务）：</p>
<ul>
<li>Nathan runs <code>select * from hacker_news_comments</code>. Doesn’t get a response yet.</li>
<li>I run <code>insert into hacker_news_comments (id, parent_id, text) values (1, NULL, &#39;a root comment&#39;)</code> and commit.</li>
<li>Tobi runs <code>insert into hacker_news_comments (id, parent_id, text) values (2, 1, &#39;OP is wrong&#39;)</code> and commits.</li>
<li>Nathan’s query returns and he gets Tobi’s row but not mine.</li>
</ul>
<p>This is the “anomaly” described in Section 2.5 of <a href="https://jepsen.io/analyses/cockroachdb-beta-20160829">Jepsen’s analysis of CRDB</a> from back in the day.</p>
<p>这就是 Jepsen 当年对 CRDB 的分析第 2.5 节中描述的“异常”。</p>
<p>So what happened? From Nathan’s perspective, Tobi’s transaction appears to have executed before mine. That contradicts strict serializability since, according to “real time”, Tobi ran his transaction after me. This is how CRDB fails strict serializability; we call this anomaly “causal reverse”.</p>
<p>所以发生了什么事？ 从 Nathan 的角度来看，Tobi 的交易似乎是在我的之前执行的。 这与严格的序列化相矛盾，因为根据“实时”，托比在我之后运行他的交易。 这就是 CRDB 无法严格串行化的原因； 我们称这种异常为“因果逆转”。</p>
<p>Before freaking out, let’s analyze the circumstances of the anomaly a bit. Then I’ll explain more technically, for the curious, how such a thing can happen in CRDB.</p>
<p>在惊慌失措之前，我们先来分析一下异常情况。 然后，为了满足好奇心，我将从技术上更详细地解释一下 CRDB 中如何发生这样的事情。</p>
<p>First of all, let’s restate our motto: if Nathan had have started his transaction after Tobi committed (in particular, if Nathan would have started his transaction <em>because</em> Tobi committed his), he would have seen both rows and things would have been good. An element that’s at play, and in fact is key here, is that Nathan’s transaction was concurrent with <strong>both</strong> mine and Tobi’s. According to the definition of strict serializability, Nathan’s transaction can be ordered in a bunch of ways with respect to the other two: it can be ordered before both of them, after both of them, or after mine but before Tobi’s. The only thing that’s required is that my transaction is ordered before Tobi’s. The violation of strict serializability that we detected here is not that Nathan’s transaction was mis-ordered, but that mine and Tobi’s (which are not concurrent) appear to have been reordered. Non-strict serializability allows this just fine.</p>
<p>首先，让我们重申一下我们的座右铭：如果 Nathan 在 Tobi 提交后开始他的事务（特别是，如果 Nathan 因为 Tobi 提交了他的事务而开始他的事务），他就会看到这两行，事情就会很好。 一个起作用的因素，实际上是这里的关键，是Nathan的事务与我和Tobi的事务同时发生。 根据严格可序列化的定义，Nathan 的事务相对于其他两个事务可以通过多种方式排序：它可以排序在它们之前、之后，或者在我的事务之后但在 Tobi 之前。 唯一需要做的是我的事务在 Tobi 的事务之前订购。 我们在这里检测到的违反严格序列化的行为并不是 Nathan 的事务顺序错误，而是我的事务和 Tobi 的事务（不并发）似乎已重新排序。 非严格的可串行性允许这很好。</p>
<p>My opinion is that this anomaly is not particularly bad because Nathan was not particularly expecting to see either of the two writes. But if this was my only argument, I’d probably stay silent.</p>
<p>我的观点是，这种异常现象并不是特别糟糕，因为 Nathan 并没有特别期望看到这两个写入中的任何一个。 但如果这是我唯一的论点，我可能会保持沉默。</p>
<p>There’s another important thing to explain: both my and Tobi’s transactions are, apart from their timing, unrelated: the sets of data they read and write do not overlap. If they were overlapping (e.g. if Tobi read my comment from the DB before inserting his), then serializability would not allow them to be reordered at all (and so CockroachDB wouldn’t do it and the anomaly goes away). In this particular example, if the schema of the hacker<em>news</em>comments table would contain a self-referencing foreign key constraint (asking the database to ensure that child comments reference an existing parent), then the “reading” part would have been ensured by the system.</p>
<p>还有一件重要的事情需要解释：我和 Tobi 的事务除了时间之外都是无关的：它们读取和写入的数据集不重叠。 如果它们重叠（例如，如果 Tobi 在插入他的评论之前从数据库中读取了我的评论），那么可序列化性根本不允许它们重新排序（因此 CockroachDB 不会这样做，异常就会消失）。 在这个特定的示例中，如果 hackernewscomments 表的模式包含自引用外键约束（要求数据库确保子评论引用现有的父评论），那么系统将确保“阅读”部分。</p>
<p>So, for this anomaly to occur, you need three transactions to play. Two of them need to appear to be independent of each other (but not really be, or otherwise we probably wouldn’t have noticed the anomaly) and the third needs to overlap both of them. I’ll let everybody judge for themselves how big of a deal this is. For what it’s worth, I don’t remember hearing a CRDB user complaining about it.</p>
<p>因此，要发生这种异常，您需要进行三笔交易。 其中两个需要看起来彼此独立（但实际上并非如此，否则我们可能不会注意到异常），第三个需要将它们重叠。 我会让每个人自己判断这件事有多大。 无论如何，我不记得听到过 CRDB 用户抱怨过它。</p>
<p>Beyond the theory, there are technical considerations that make producing this anomaly even more unlikely: given CockroachDB’s implementation, the anomaly is avoided not only if the read&#x2F;write sets of my and Tobi’s transactions overlap, but also if the leadership of any of the ranges of data containing hacker<em>news</em>comments rows 1 and 2 happens to be on the same node when these transactions occur, or if Nathan’s database client is talking to the same CockroachDB node as Tobi’s, and also in various other situations. Also, the more synchronized the clocks on the three nodes are, the less likely it is. Overall, this anomaly is pretty hard to produce even if you try explicitly.</p>
<p>除了理论之外，还有一些技术考虑因素使得产生这种异常的可能性更小：考虑到 CockroachDB 的实现，不仅当 my 和 Tobi 的事务的读&#x2F;写集重叠时，而且当任何范围的领导层重叠时，也可以避免异常。 当这些事务发生时，或者 Nathan 的数据库客户端与 Tobi 的数据库客户端与同一个 CockroachDB 节点通信时，以及在各种其他情况下，包含 hackernewscomments 行 1 和 2 的数据恰好位于同一节点上。 此外，三个节点上的时钟越同步，这种情况的可能性就越小。 总的来说，即使你明确地尝试，这种异常也很难产生。</p>
<p>As you might have guessed, I personally am not particularly concerned about this anomaly. Besides everything I’ve said, I’ll add a whataboutist argument and take the discussion back to friendly territory: consider this anomaly in contrast to the “stale reads” family of anomalies present in many other competing products. All these things are commonly bucketed under strict serializability &#x2F; linearizability violations, but don’t be fooled into thinking that they’re all just as bad. Our anomaly needs three transactions doing a specific dance resulting in an outcome that, frankly, is not even that bad. A stale read anomaly can be seen much easier in a product that allows it. Examples are many; a colleague gave a compelling one recently: if your bank was using a database that allows stale reads, someone might deposit a check for you, at which point your bank would text you about it, and you’d go online to see your balance. You might see the non-updated balance and freak out. Banks should be using CockroachDB.</p>
<p>正如您可能已经猜到的那样，我个人并不特别担心这种异常现象。 除了我所说的一切之外，我还将添加一个关于什么的论点，并将讨论带回到友好的领域：将此异常与许多其他竞争产品中存在的“陈旧读取”异常系列进行对比。 所有这些事情通常都受到严格的可序列化&#x2F;线性化违规的影响，但不要误以为它们都一样糟糕。 我们的异常需要三笔交易进行特定的舞蹈，坦率地说，结果并没有那么糟糕。 在允许这种情况的产品中，可以更容易地看到陈旧的读取异常。 例子有很多； 一位同事最近给出了一个令人信服的说法：如果您的银行使用的数据库允许过时读取，有人可能会为您存入一张支票，此时您的银行会向您发送短信，然后您可以上网查看余额。 您可能会看到未更新的余额并感到害怕。 银行应该使用 CockroachDB。</p>
<h3 id="Other-CockroachDB-gotchas"><a href="#Other-CockroachDB-gotchas" class="headerlink" title="Other CockroachDB gotchas"></a>Other CockroachDB gotchas</h3><p>其他 CockroachDB 问题</p>
<p>I’ve discussed the CockroachDB guarantees and violations of strict serializability. Our discussion used, laxly, the SQL language to illustrate things but the discussion used language and concepts from more theoretical literature. We bridged the gap by implying that SQL statements are really reads and writes used by some models. This section discusses some uses of CockroachDB&#x2F;SQL that fall a bit outside the models we’ve used, but are surprising nevertheless. I think these examples will not fall nicely into the models used for the strict serializability definition, at least not without some effort into expanding the model.</p>
<p>我已经讨论了 CockroachDB 的保证和对严格序列化的违反。 我们的讨论宽松地使用了 SQL 语言来说明问题，但讨论使用了更多理论文献中的语言和概念。 我们通过暗示 SQL 语句实际上是某些模型使用的读取和写入来弥补这一差距。 本节讨论 CockroachDB&#x2F;SQL 的一些用法，这些用法有点超出我们使用的模型，但仍然令人惊讶。 我认为这些示例不会很好地落入用于严格可串行性定义的模型中，至少在不努力扩展模型的情况下是这样。</p>
<h4 id="The-SQL-now-function"><a href="#The-SQL-now-function" class="headerlink" title="The SQL now() function"></a><strong>The SQL now() function</strong></h4><p>Consider the following two transactions:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="keyword">insert</span> <span class="keyword">into</span> foo (id, <span class="type">time</span>) <span class="keyword">values</span> (<span class="number">1</span>, now())</span><br><span class="line"><span class="number">2.</span> <span class="keyword">insert</span> <span class="keyword">into</span> foo (id, <span class="type">time</span>) <span class="keyword">values</span> (<span class="number">2</span>, now())</span><br></pre></td></tr></table></figure>

<p>Assuming these two transactions execute in this order, it is possible (and surprising) to read the rows back and see that the time value for row 2 is lower that the one for row one.</p>
<p>假设这两个事务按此顺序执行，则有可能（并且令人惊讶）读回行并看到第 2 行的时间值低于第 1 行的时间值。</p>
<p>Perhaps it’s realistic to think that this happens in other systems too, even single-node systems, if the system clock jumps backwards (as it sometimes does), so perhaps there’s nothing new here.</p>
<p>也许现实的是，如果系统时钟向后跳（有时会发生），这种情况也会发生在其他系统中，甚至是单节点系统中，所以也许这里没有什么新东西。</p>
<h3 id="as-of-system-time-queries-and-backups"><a href="#as-of-system-time-queries-and-backups" class="headerlink" title="as of system time queries and backups"></a><code>as of system time</code> queries and backups</h3><p>CockroachDB supports the (newer) standard SQL system-versioned tables; CockroachDB lets one “time travel” and query the old state of the database with a query like <code>select * from foo as of system time now()-10s</code>. This is a fantastic, really powerful feature. But it also provides another way to observe a “causal reverse” anomaly. Say one ran these two distinct transactions, in this order:</p>
<p>CockroachDB 支持（较新的）标准 SQL 系统版本表； CockroachDB 允许“时间旅行”，并使用 select * from foo 之类的查询来查询截至系统时间 now()-10s 的数据库的旧状态。 这是一个非常棒的、非常强大的功能。 但它也提供了另一种观察“因果逆转”异常的方法。 假设有人按以下顺序运行这两个不同的事务：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="keyword">insert</span> <span class="keyword">into</span>  hacker_news_comments (id, parent_id, text) <span class="keyword">values</span> (<span class="number">1</span>, <span class="keyword">NULL</span>, <span class="string">&#x27;a root comment&#x27;</span>)</span><br><span class="line"><span class="number">2.</span> <span class="keyword">insert</span> <span class="keyword">into</span> hacker_news_comments (id, parent_id, text) <span class="keyword">values</span> (<span class="number">2</span>, <span class="number">1</span>, <span class="string">&#x27;OP is wrong&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>It’s possible for an <code>as of system time</code> query to be executed later and, if it’s unlucky in its choice of a “system time”, to see the second row and not the first.</p>
<p>稍后执行<code>as of system time</code>查询是可能的，如果不幸选择了“系统时间”，则可能会看到第二行而不是第一行。</p>
<p>Again, if the second transaction were to read the data written by the first (e.g. implicitly through a foreign key check), the anomaly would not be possible.</p>
<p>同样，如果第二个事务要读取第一个事务写入的数据（例如，通过外键检查隐式地进行），则不会出现异常。</p>
<p>Relatedly, a backup, taken through the <code>backup database</code> command, is using <code>as of system time</code> queries under the hood, and so a particular backup might contain row 2 but not row 1.</p>
<p>相关地，通过备份数据库命令进行的备份在幕后使用系统时间查询，因此特定备份可能包含第 2 行，但不包含第 1 行。</p>
<h2 id="CockroachDB-implementation-details"><a href="#CockroachDB-implementation-details" class="headerlink" title="CockroachDB implementation details"></a>CockroachDB implementation details</h2><p>The <a href="https://www.cockroachlabs.com/docs/stable/architecture/overview">architecture of CockroachDB</a> is based on a separation between multiple layers (a SQL layer on top down to a storage layer at the bottom). For the subject at hand, the interesting layer is the <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer">Transaction Layer</a>, which is in charge of making sure that a transaction doesn’t miss writes that it’s supposed to be seeing. Each transaction has a timestamp, assigned by the “gateway node” — the node that a client happens to be talking to — when the transaction starts (through a SQL BEGIN statement). As the transaction talks to different other nodes that might be responsible for <em><em>ranges</em></em> of data it wants to read, this timestamp is used to decide what values are visible (because they’ve been written by transactions “in the past”) and which values aren’t visible because they’ve been written “in the future”.</p>
<p>CockroachDB的架构基于多层之间的分离（顶部的SQL层到底部的存储层）。 对于当前的主题，有趣的层是事务层，它负责确保事务不会错过它应该看到的写入。 每个事务都有一个时间戳，由事务开始时（通过 SQL BEGIN 语句）由“网关节点”（客户端恰好与之通信的节点）分配。 当事务与可能负责其想要读取的数据范围的不同其他节点进行通信时，此时间戳用于决定哪些值是可见的（因为它们是由“过去”的事务写入的）以及哪些值是不可见，因为它们已被写为“未来”。</p>
<p>CockroachDB uses multi-version concurrency control (MVCC), which means that the history of each row is available for transactions to look through. The difficulties, with respect to consistency guarantees, stem from the fact that the timestamp recording into MVCC are taken from the clock of the gateway node that wrote it, which generally is not the same one as the gateways assigning transaction timestamp for a reader, and we assume that the clock can be desynchronized up to a limit (we call the phenomenon “clock skew”). So, given transaction timestamp <em>t</em> and value timestamp <em>t’</em>, how does one decide whether the value in question should be visible or not?</p>
<p>CockroachDB使用多版本并发控制（MVCC），这意味着每一行的历史记录都可供事务查看。 一致性保证方面的困难源于以下事实：MVCC 中的时间戳记录取自写入它的网关节点的时钟，该时钟通常与为读取器分配事务时间戳的网关不同，并且 我们假设时钟可以去同步到一定限度（我们将这种现象称为“时钟偏差”）。 那么，给定交易时间戳 t 和值时间戳 t’，如何决定所讨论的值是否应该可见？</p>
<p>The rules are that, if <em>t’ &lt;&#x3D; t</em>, then the transaction will see the respective value (and so we’ll essentially order our transaction after that writer). The reasoning is that either our transaction really started after the other one committed, or, if not, the two were concurrent and so we can order things either way.</p>
<p>规则是，如果 t’ &lt;&#x3D; t，那么交易将看到相应的值（因此我们基本上会在该编写者之后对交易进行排序）。 原因是，要么我们的事务在另一个事务提交后才真正开始，要么如果不是，则两个事务是并发的，因此我们可以以任何一种方式排序。</p>
<p>If <em>t’ &gt; t</em>, then it gets tricky. Did the writer really start and commit before the reader began its transaction, or did it commit earlier than that but t’ was assigned by a clock that’s ahead of ours? What CRDB does is define an “uncertainty interval”: if the values are close enough so that <em>t’</em> could be explained by a trailing clock, we say that we’re unsure about whether the value needs to be visible or not, and our transaction needs to change its timestamp (which, unless we can avoid it, means the transaction might have to restart. Which, unless we can further avoid it, means the client might get a retriable error). This is what allows CockroachDB to guarantee no stale reads. In the Hacker News example, if Nathan starts his transaction after me and Tobi committed ours, the worst that could happen is that he gets a timestamp that’s slightly in the past and has to consider some of our other writes uncertain, at which point he’ll restart at a higher timestamp.</p>
<p>如果 t’ &gt; t，那么事情就会变得棘手。 写入器是否真的在读取器开始其事务之前开始并提交，或者它是否早于该时间提交，但 t’ 是由比我们早的时钟分配的？ CRDB 所做的是定义一个“不确定性区间”：如果这些值足够接近以至于 t’ 可以用尾随时钟来解释，我们就说我们不确定该值是否需要可见，并且我们的交易 需要更改其时间戳（除非我们可以避免它，否则意味着事务可能必须重新启动。除非我们可以进一步避免它，否则意味着客户端可能会遇到可重试的错误）。 这就是 CockroachDB 能够保证没有过时读取的原因。 在 Hacker News 的例子中，如果 Nathan 在我和 Tobi 提交我们的交易之后开始他的交易，最糟糕的情况是他得到的时间戳稍微有点过去，并且必须考虑我们的其他一些写入不确定，此时他’ 将以更高的时间戳重新启动。</p>
<p>We work quite hard to minimize the effects of this uncertainty interval. For one, transactions keep track of what timestamps they’ve observed at each node and uncertainty is tracked between nodes pair-wise. This, coupled with the fact that a node’s clock is bumped up when someone tries to write on it with a higher timestamp, allows a transaction to not have to restart more than once because of an uncertain value seen on a particular node. Also, overall, once the maximum admissible clock skew elapses since a transaction started, a transaction no longer has any uncertainty.</p>
<p>我们非常努力地工作以尽量减少这种不确定性区间的影响。 首先，交易会跟踪它们在每个节点观察到的时间戳，并且成对地跟踪节点之间的不确定性。 再加上当有人试图用更高的时间戳在节点上写入时，节点的时钟会被调高，使得事务不必因为在特定节点上看到的不确定值而多次重新启动。 此外，总体而言，一旦自事务开始以来最大允许时钟偏差过去，事务就不再具有任何不确定性。</p>
<p>Separately, when a transaction’s timestamp does need to be bumped, we try to be smart about it. If either the transaction hasn’t read anything before encountering the uncertain value, or if we can verify that there’s been no writes on the data its already read before encountering the uncertainty, then the transaction can be bumped with no fuss. If we can’t verify that, then the transaction needs to restart so it can perform its writes again. If it does have to restart, we don’t necessarily tell the client about it. If we haven’t yet returned any results for the transaction to the client (which is common if the client can send parts of a transaction’s statements as a batch), then we can re-execute all the transaction’s statements on the server-side and the client is none the wiser.</p>
<p>另外，当交易的时间戳确实需要改变时，我们会尽量聪明地处理它。 如果事务在遇到不确定值之前没有读取任何内容，或者如果我们可以验证在遇到不确定值之前已经读取的数据没有被写入，那么事务就可以毫不费力地被碰撞。 如果我们无法验证这一点，则事务需要重新启动，以便它可以再次执行写入操作。 如果确实需要重新启动，我们不一定会告诉客户。 如果我们尚未将事务的任何结果返回给客户端（如果客户端可以批量发送事务的部分语句，则这是常见的），那么我们可以在服务器端重新执行所有事务的语句，并 客户却一无所知。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>CockroachDB provides a high level of “consistency”, second only to Spanner among distributed databases as far as I know (but then CockroachDB is a more flexible and easy to migrate to database — think <a href="https://www.cockroachlabs.com/docs/stable/install-client-drivers">ORM support</a> — so I’ll<a href="https://www.cockroachlabs.com/blog/spanner-vs-cockroachdb/"> take it over Spanner any day</a>). We offer a relatively easy to understand programming model, although the literature doesn’t give us a good name for it. It stronger than serializability, but somewhat weaker than strict serializability (and than linearizability, although using that term in the context of a transactional system is an abuse of the language). It’s probably easiest to qualify it by understanding the anomaly that it allows — “causal reverse” — and the limited set of circumstances under which it can occur. In the majority of cases where one might be wondering about semantics of reads and writes in CockroachDB, the slogan “no stale reads” should settle most discussions.</p>
<p>据我所知，CockroachDB 提供了高水平的“一致性”，在分布式数据库中仅次于 Spanner（但 CockroachDB 是一个更灵活、更容易迁移到的数据库——想想 ORM 支持——所以我会用它来取代 Spanner） 天）。 我们提供了一个相对容易理解的编程模型，尽管文献没有给我们一个好名字。 它比可序列化性更强，但比严格的可序列化性稍弱（并且比线性化更弱，尽管在事务系统的上下文中使用该术语是对该语言的滥用）。 通过理解它所允许的异常——“因果逆转”——以及它可能发生的有限情况，来限定它可能是最简单的。 在大多数情况下，人们可能想知道 CockroachDB 中读写的语义，“无陈旧读取”这一口号应该可以解决大多数讨论。</p>
<p>Although I think the definition of the Serializable isolation level would have benefitted from introducing some notion of different clients. As phrased by the SQL standard, I believe it technically allows empty results to be produced for any read-only transaction with the justification that those transactions are simply ordered before any other transaction. Implementing that would be egregious, though.[^2]: We’re thinking of ways to make CRDB resilient to more arbitrarily unsynchronized clocks.[^3]: As discussed in the “A note on clocks” section, figuring out what “afterwards” means is not always trivial when the clients involved are not on the same machine. But still, sometimes (in the cases that matter most), a transaction is known to happen after another one, usually through a causal relationship between the two. </p>
<p>尽管我认为可串行化隔离级别的定义会受益于引入不同客户端的一些概念。 正如 SQL 标准所述，我相信它在技术上允许为任何只读事务生成空结果，理由是这些事务只是在任何其他事务之前排序。 然而，实现这一点将是令人震惊的。[^2]：我们正在考虑如何使 CRDB 能够适应更多任意不同步的时钟。[^3]：正如“时钟注释”部分中所讨论的，弄清楚“ 当涉及的客户端不在同一台机器上时，“之后”的意思并不总是微不足道的。 但有时（在最重要的情况下），一笔交易会在另一笔交易之后发生，通常是通过两者之间的因果关系。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>Isolation in CockroachDB</title>
    <url>/2023/10/27/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/03-Isolation%20in%20CockroachDB/</url>
    <content><![CDATA[<p>原文：<a href="https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/">https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/</a></p>
<p><em><strong>Editor’s Note*</strong></em>: This post was originally authored when CockroachDB was pre-1.0. CockroachDB’s architecture has undergone many changes since then. One of the most significant, as it relates to this post which focuses on our previous “lockless” design, is that we now use more locking and lock-like structures to provide <code>SERIALIZABLE</code> isolation. For more current details about CockroachDB’s transaction model, read our <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer">transaction layer architecture documentation</a>.*</p>
<p>编者注：这篇文章最初是在 CockroachDB 1.0 版本之前撰写的。 从那时起，CockroachDB 的架构经历了许多变化。 最重要的一个，因为它与这篇文章相关，重点关注我们之前的“无锁”设计，我们现在使用更多的锁定和类似锁的结构来提供可串行化的隔离。 有关 CockroachDB 事务模型的更多最新详细信息，请阅读我们的事务层架构文档。</p>
<span id="more"></span>

<p>Several months ago, I discussed how CockroachDB’s <a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/">distributed transactions</a> are executed atomically. However, that discussion was incomplete; it ignored the concept of <em>concurrency,</em> where multiple transactions are active on the same data set at the same time. CockroachDB, like all database systems, tries to allow as much concurrency as possible in order to maximize access to the data set.</p>
<p>几个月前，我讨论了 CockroachDB 的分布式事务是如何原子执行的。 然而，这一讨论并不完整； 它忽略了并发的概念，即多个事务同时在同一数据集上处于活动状态。 与所有数据库系统一样，CockroachDB 尝试允许尽可能多的并发性，以便最大限度地访问数据集。</p>
<p>Unfortunately, our atomicity guarantee is not sufficient to keep the database consistent in a world of concurrent transactions. Recall that guarantee:</p>
<p>不幸的是，我们的原子性保证不足以在并发事务的世界中保持数据库的一致性。 回想一下这个保证：</p>
<blockquote>
<p> For a group of database operations, either all of the operations are applied or none of them are applied.</p>
<p>对于一组数据库操作，要么应用所有操作，要么不应用任何操作。</p>
</blockquote>
<p>What this does not address is the way that concurrent transactions may <em>interleave.</em> The individual operations (reads and writes) in a transaction do not happen simultaneously; there is time in between the individual operations. In a concurrent system, one transaction may commit during the execution window of a second transaction; even if the first transaction (T1) commits atomically, this can still allow operations later in the second transaction (T2) to see the results of T1, even though earlier operations on T2 did <em>not</em> see the results of T1. This interleaving can create a <a href="https://blog.acolyer.org/2016/02/24/a-critique-of-ansi-sql-isolation-levels/">number of undesired anomalies</a>, ultimately breaking the consistency of the database.</p>
<p>这没有解决并发事务可能交错的方式。 事务中的各个操作（读取和写入）不会同时发生； 各个操作之间有时间间隔。 在并发系统中，一个事务可能会在第二个事务的执行窗口期间提交； 即使第一个事务 (T1) 以原子方式提交，这仍然可以允许第二个事务 (T2) 中稍后的操作看到 T1 的结果，即使 T2 上的早期操作没有看到 T1 的结果。 这种交错可能会产生许多不需要的异常，最终破坏数据库的一致性。</p>
<p>To protect against these anomalies, we require an <em>Isolation</em> guarantee:</p>
<p>为了防止这些异常情况，我们需要隔离保证：</p>
<blockquote>
<p>For a group of atomic, concurrent transactions, the commit of one transaction may not interleave with the operations of another transaction.</p>
<p>对于一组原子并发事务，一个事务的提交可能不会与另一个事务的操作交错。</p>
</blockquote>
<p>Perfect isolation can be trivially achieved through <em>serial execution:</em> executing all transactions on the system one at a time, with no concurrency. This has terrible performance implications; fortunately, it is also unnecessary to achieve perfect isolation. Many concurrent databases, including CockroachDB, instead offer <strong>serializable</strong> execution, which is <em>equivalent</em> to serial execution while allowing a considerable level of concurrent transactions.</p>
<p>通过串行执行可以轻松实现完美的隔离：一次执行系统上的所有事务，无需并发。 这会对性能产生严重影响； 幸运的是，也没有必要实现完美的隔离。 许多并发数据库（包括 CockroachDB）都提供可序列化执行，这相当于串行执行，同时允许相当水平的并发事务。</p>
<p>CockroachDB’s default isolation level is called <strong>Serializable Snapshot.</strong> It is an optimistic, multi-version, timestamp-ordered concurrency control system with the following properties:</p>
<p>CockroachDB 的默认隔离级别称为可序列化快照。 它是一个乐观的、多版本的、时间戳有序的并发控制系统，具有以下特性：</p>
<ul>
<li><p><strong>Serializable:</strong> The resulting database state is equivalent to a serial execution of component transactions.</p>
<p>生成的数据库状态相当于组件事务的串行执行。</p>
</li>
<li><p><strong>Recoverable:</strong> A set of database transactions is considered recoverable if aborted or abandoned transactions will have no effect on the database state. Our atomic commit system already guarantees that individual transactions are recoverable; our Isolation system uses strict scheduling to ensure that any combination of transactions is also recoverable.</p>
<p>如果中止或放弃的事务对数据库状态没有影响，则认为一组数据库事务是可恢复的。 我们的原子提交系统已经保证单个事务是可恢复的； 我们的隔离系统使用严格的调度来确保任何事务组合也是可恢复的。</p>
</li>
<li><p><strong>Lockless:</strong> Operations execute without taking locks on resources. Correctness is enforced by aborting transactions which would violate either serializability or strict scheduling.</p>
<p>操作的执行无需锁定资源。 通过中止会违反可串行性或严格调度的事务来强制执行正确性。</p>
</li>
<li><p><strong>Distributed:</strong> There is no central oracle, coordinator or service involved in this system.</p>
<p>该系统不涉及中央预言机、协调器或服务。</p>
</li>
</ul>
<h2 id="Providing-Serializable-Execution"><a href="#Providing-Serializable-Execution" class="headerlink" title="Providing Serializable Execution"></a>Providing Serializable Execution</h2><p>CockroachDB uses a multi-version timestamp ordering to guarantee that its complete transaction commit history is serializable. The basic technique has been <a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx">textbook material</a> for three decades, but we will briefly go over how it works:</p>
<p>CockroachDB使用多版本时间戳排序来保证其完整的事务提交历史是可序列化的。 三十年来，这项基本技术一直是教科书材料，但我们将简要介绍一下它的工作原理：</p>
<h3 id="Serializability-Graphs"><a href="#Serializability-Graphs" class="headerlink" title="Serializability Graphs"></a>Serializability Graphs</h3><p>To demonstrate the correctness of timestamp ordering, we look to <em>serializability theory</em>, and specifically one of its core concepts, the <em>serializability graph.</em> This graph is used to analyze a history of database transactions in terms of <em>operation conflicts</em>.</p>
<p>为了证明时间戳排序的正确性，我们研究了可序列化理论，特别是其核心概念之一，可序列化图。 该图用于根据操作冲突来分析数据库事务的历史记录。</p>
<p>In the theory, a <em>conflict</em> occurs when two different transactions perform an operation on the same piece of data (one after the other), where at least one of the operations is a write. The second operation is said to be <em>in conflict</em> with the first operation. There are three types of conflicts:</p>
<p>理论上，当两个不同的事务对同一条数据（一个接一个）执行操作时，就会发生冲突，其中至少有一个操作是写入。 据说第二个操作与第一个操作冲突。 冲突分为三种类型：</p>
<ul>
<li><strong>Read-Write (RW)</strong> – Second operation <strong>overwrites</strong> a value that was <strong>read</strong> by the first operation.</li>
<li><strong>Write-Read (WR)</strong> – Second operation <strong>reads</strong> a value that was <strong>written</strong> by the first operation.</li>
<li><strong>Write-Write (WW)</strong> – Second operation <strong>overwrites</strong> a value that was <strong>written</strong> by first operation.</li>
</ul>
<p>For any given transaction history, these conflicts can be used to create a serializability graph, which is a directed graph linking all transactions.</p>
<p>对于任何给定的事务历史记录，这些冲突可用于创建可序列化图，这是链接所有事务的有向图。</p>
<ul>
<li><p>Transactions are nodes in the graph.</p>
<p>交易是图中的节点。</p>
</li>
<li><p>Whenever an operation conflicts with an operation from a different transaction, draw a directed edge from the conflicting operation to the conflicted operation.</p>
<p>每当一个操作与来自不同事务的操作发生冲突时，就从冲突操作到冲突操作绘制一条有向边。</p>
</li>
</ul>
<p><img src="/img/Isolation_in_CockroachDB/1.avif"> </p>
<p>*Figure 1: Example of a serializability graph for a simple transaction history.*简单交易历史记录的可序列化图示例</p>
<p>And we now arrive at a key statement of this theory: a history is guaranteed to be <em>serializable</em> if (and only if) its serializability graph is acyclic.  (<a href="https://web.archive.org/web/20160316224001/http://research.microsoft.com/en-us/people/philbe/chapter2.pdf">Proof</a>, for those interested).</p>
<p>现在我们得出了该理论的一个关键陈述：当（且仅当）其可序列化图是非循环的时，历史才保证可序列化。 （证明，对于那些有兴趣的人）。</p>
<p><img src="/img/Isolation_in_CockroachDB/2.avif"></p>
<p><em>Figure 2: Example of a transaction history with a cyclic serializability graph. This history is not serializable.</em></p>
<p>CockroachDB’s timestamp ordering <strong>guarantees an acyclic serializability graph</strong>, and this is straightforward to demonstrate:</p>
<p>CockroachDB 的时间戳排序保证了非循环可序列化图，这很容易演示：</p>
<ul>
<li><p>Every transaction is assigned a timestamp (from the node on which it starts) when it begins. All operations in that transaction take place at this same timestamp, for the duration of the transaction.</p>
<p>每个事务在开始时都会被分配一个时间戳（从它开始的节点开始）。 在该事务的持续时间内，该事务中的所有操作都在同一时间戳发生。</p>
</li>
<li><p>Individual operations can locally determine when they conflict with another operation, <em>and</em> what the transaction timestamp of the conflicted operation is.</p>
<p>各个操作可以在本地确定它们何时与另一个操作发生冲突，以及冲突操作的事务时间戳是什么。</p>
</li>
<li><p>Operations are only allowed to conflict with earlier timestamps; a transaction is not allowed to commit if doing so would create a conflict with a later timestamp.</p>
<p>操作只允许与较早的时间戳发生冲突； 如果提交事务会与较晚的时间戳产生冲突，则不允许提交该事务。</p>
</li>
</ul>
<p>By disallowing any conflicts that flow against the timestamp-ordered direction, cyclic serializability graphs are impossible. However, let’s explore in detail how CockroachDB actually goes about detecting and disallowing these conflicts.</p>
<p>通过禁止任何与时间戳排序方向相反的冲突，循环可串行化图是不可能的。 然而，让我们详细探讨一下 CockroachDB 实际上是如何检测和禁止这些冲突的。</p>
<h4 id="Write-Read-Conflicts-–-MVCC-Database"><a href="#Write-Read-Conflicts-–-MVCC-Database" class="headerlink" title="Write-Read Conflicts – MVCC Database"></a><strong>Write-Read Conflicts</strong> – MVCC Database</h4><p>This is where the “multi-version” aspect of our control mechanism comes into play. CockroachDB keys do not store a single value, but rather store multiple timestamped versions of that value. New writes do not overwrite old values, but rather create a new version with a later timestamp.</p>
<p>这就是我们控制机制的“多版本”发挥作用的地方。 CockroachDB 键不存储单个值，而是存储该值的多个带时间戳的版本。 新写入不会覆盖旧值，而是创建具有较晚时间戳的新版本。</p>
<p><img src="/img/Isolation_in_CockroachDB/3.avif"></p>
<p><em>Figure 3: Comparison of multi-versioned value store with a single-value store. Note that the multi-version store is sorted by timestamp.</em></p>
<p>Read operations on a key return the most recent version with a lower timestamp than the operation:</p>
<p>对键的读取操作返回时间戳低于操作的最新版本：</p>
<p><img src="/img/Isolation_in_CockroachDB/4.avif"></p>
<p>Thus, it is not possible in CockroachDB to form WR conflicts with later transactions; read operations will never read a value with a later timestamp.</p>
<p>因此，CockroachDB中不可能与后面的事务形成WR冲突； 读取操作永远不会读取具有较晚时间戳的值。</p>
<ul>
<li><p>Note: This is the “Snapshot” in serializable snapshot; in-progress transactions essentially see a temporal snapshot of the database, ignoring anything committed later.</p>
<p>注意：这是可序列化快照中的“快照”； 正在进行的事务本质上看到数据库的临时快照，忽略稍后提交的任何内容。</p>
</li>
</ul>
<h4 id="Read-Write-Conflicts-–-Read-Timestamp-Cache"><a href="#Read-Write-Conflicts-–-Read-Timestamp-Cache" class="headerlink" title="Read-Write Conflicts – Read Timestamp Cache"></a><strong>Read-Write Conflicts</strong> – Read Timestamp Cache</h4><p>On any read operation, the timestamp of that read operation is recorded in a node-local <em>timestamp cache</em>. This cache will return the most recent timestamp at which the key was read.</p>
<p>在任何读取操作中，该读取操作的时间戳都会记录在节点本地时间戳缓存中。 该缓存将返回读取密钥的最新时间戳。</p>
<p>All write operations consult the timestamp cache for the key they are writing; if the returned timestamp is greater than the operation timestamp, this indicates a RW conflict with a later timestamp.  To disallow this, the operation (and its transaction) must be aborted and restarted with a later timestamp.</p>
<p>所有写入操作都会查询时间戳缓存以获取它们正在写入的密钥； 如果返回的时间戳大于操作时间戳，则表明与较晚的时间戳发生 RW 冲突。 为了禁止这种情况，必须中止操作（及其事务）并使用稍后的时间戳重新启动。</p>
<p>The timestamp cache is an interval cache, meaning that its keys are actually key ranges. If a read operation is actually a predicate operating over a range of keys (such as a scan), then the entire scanned key range is written to the timestamp cache. This prevents RW conflicts where the key being written was not present during the scan operation.</p>
<p>时间戳缓存是一个区间缓存，这意味着它的键实际上是键范围。 如果读取操作实际上是对一系列键进行操作的谓词（例如扫描），则整个扫描的键范围将写入时间戳缓存。 这可以防止在扫描操作期间不存在正在写入的密钥时发生 RW 冲突。</p>
<p>The timestamp cache is a size-limited, in-memory LRU (least recently used) data structure, with the oldest timestamps being evicted when the size limit is reached. To deal with keys not in the cache, we also maintain a “low water mark”, which is equivalent to the earliest read timestamp of any key that is present in the cache. If a write operation writes to a key not present in the cache, the “low water mark” is returned instead.</p>
<p>时间戳缓存是一种大小有限的内存中 LRU（最近最少使用）数据结构，当达到大小限制时，最旧的时间戳将被逐出。 为了处理不在缓存中的键，我们还维护一个“低水位线”，这相当于缓存中存在的任何键的最早读取时间戳。 如果写入操作写入缓存中不存在的键，则会返回“低水位线”。</p>
<h4 id="Write-Write-Conflicts-–-Can-only-write-the-most-recent-version-of-a-key"><a href="#Write-Write-Conflicts-–-Can-only-write-the-most-recent-version-of-a-key" class="headerlink" title="Write-Write Conflicts – Can only write the most recent version of a key"></a><strong>Write-Write Conflicts</strong> – Can only write the most recent version of a key</h4><p>If a write operation attempts to write to a key, but that key already has a version with a later timestamp than the operation itself, allowing the operation would create a WW conflict with the later transaction.  To ensure serializability, the operation (and its transaction) must be aborted and restarted with a later timestamp.</p>
<p>如果写入操作尝试写入某个键，但该键已经具有比操作本身晚的时间戳的版本，则允许该操作将与后面的事务产生 WW 冲突。 为了确保可串行性，必须中止操作（及其事务）并使用稍后的时间戳重新启动。</p>
<p>By choosing a timestamp-based ordering, and rejecting all conflicts which disagree with that ordering, CockroachDB’s Serializable Snapshot guarantees a serializable schedule.</p>
<p>通过选择基于时间戳的排序，并拒绝所有与该排序不一致的冲突，CockroachDB 的可序列化快照保证了可序列化的计划。</p>
<h2 id="Recoverable-with-Strict-Scheduling"><a href="#Recoverable-with-Strict-Scheduling" class="headerlink" title="Recoverable with Strict Scheduling"></a>Recoverable with Strict Scheduling</h2><p>While the previous conflict rules are sufficient to guarantee a serializable history, a different concern arises when two <em>uncommitted</em> transactions have a conflict: even if that conflict is allowed by our timestamp ordering rules, additional rules are required to ensure that the transaction schedule remains <em>recoverable</em>.</p>
<p>虽然前面的冲突规则足以保证可序列化的历史记录，但当两个未提交的事务发生冲突时，就会出现不同的问题：即使我们的时间戳排序规则允许该冲突，也需要额外的规则来确保事务计划保持可恢复。</p>
<p>The issue of can be explained with an example:</p>
<p>这个问题可以用一个例子来解释：</p>
<p><em>consider two transactions [T1, T2], where timestamp(T1) &lt; timestamp(T2). T1 writes to a key ‘A’. Later, T2 reads from key ‘A’, before T1 has committed.</em></p>
<p>This conflict is allowed according to our timestamp ordering rules. However, what value should T2’s read retrieve from ‘A’?</p>
<p>根据我们的时间戳排序规则，这种冲突是允许的。 然而，T2 的读取应该从“A”检索什么值？</p>
<ul>
<li><p>Assume it ignores the uncommitted value written by T1, and retrieves the previous value instead. If T1 and T2 both commit, this will create a WR conflict with T2 (T1 will have overwritten a value read by T2). This violates our timestamp ordering guarantee, and thus serializability.</p>
<p>假设它忽略 T1 写入的未提交值，而是检索以前的值。 如果 T1 和 T2 都提交，这将与 T2 产生 WR 冲突（T1 将覆盖 T2 读取的值）。 这违反了我们的时间戳排序保证，从而违反了可序列化性。</p>
</li>
<li><p>Assume it retrieves the value written by T1. If T2 commits, but T1 later aborts, this will have violated the <em>atomicity</em> of T1: T1 still will have had an effect on the database state, even though it aborted.</p>
<p>假设它检索 T1 写入的值。 如果 T2 提交，但 T1 后来中止，这将违反 T1 的原子性：T1 仍然会对数据库状态产生影响，即使它中止了。</p>
</li>
</ul>
<p>Thus, neither possibility can allowed: in this situation, there is no way that T2 can be safely committed before T1 while maintaining a <em>recoverable</em> schedule.</p>
<p>因此，这两种可能性都不允许：在这种情况下，无法在保持可恢复调度的同时在 T1 之前安全地提交 T2。</p>
<p>CockroachDB uses <strong>strict scheduling</strong> to handle this situation: operations are only allowed to read or overwrite <em>committed</em> values; operations are never allowed to act on an uncommitted value.</p>
<p>CockroachDB使用严格的调度来处理这种情况：操作只允许读取或覆盖提交的值； 永远不允许操作对未提交的值进行操作。</p>
<h3 id="Enforcing-Strict-Scheduling-执行严格的调度"><a href="#Enforcing-Strict-Scheduling-执行严格的调度" class="headerlink" title="Enforcing Strict Scheduling 执行严格的调度"></a>Enforcing Strict Scheduling 执行严格的调度</h3><p>As established in our <a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/">atomicity post</a>, uncommitted data is staged in <em>intents</em> on each key, for the purpose of atomic commits. In an MVCC data store, the intent on a key (if present) is stored in a special value which sorts immediately before the most recent committed value:</p>
<p>正如我们在原子性帖子中所建立的，为了原子提交的目的，未提交的数据在每个键上的意图中暂存。 在 MVCC 数据存储中，键（如果存在）的意图存储在一个特殊值中，该值紧邻最近提交的值之前排序：</p>
<p><img src="/img/Isolation_in_CockroachDB/5.avif"></p>
<p>In our previous post on atomicity, we assumed that any intent encountered by a transaction was the result of an abandoned transaction; however, in a concurrent environment, the intent might instead be from a concurrent transaction which is still running.</p>
<p>在我们之前关于原子性的文章中，我们假设事务遇到的任何意图都是被放弃的事务的结果； 但是，在并发环境中，意图可能来自仍在运行的并发事务。</p>
<p>Strict scheduling actions are required in two situations: if a read operation encounters an intent with a lower timestamp, or if a write encounters <em>any</em> intent from another transaction (regardless of timestamp ordering). In these situations, there are two options available to CockroachDB:</p>
<p>在两种情况下需要严格的调度操作：如果读取操作遇到具有较低时间戳的意图，或者如果写入遇到来自另一个事务的任何意图（无论时间戳顺序如何）。 在这些情况下，CockroachDB 有两种选择：</p>
<ul>
<li><p>If the second transaction has a higher timestamp, it can wait for the first transaction to commit or abort before completing the operation.</p>
<p>如果第二个事务具有更高的时间戳，则它可以等待第一个事务提交或中止，然后再完成操作。</p>
</li>
<li><p>One of the two transactions can be aborted.</p>
<p>两个事务之一可以被中止。</p>
</li>
</ul>
<p>As an optimistic system (no waiting), CockroachDB <em>always</em> chooses to abort one of the transactions. The process of determining which transaction is as follows:</p>
<p>作为一个乐观的系统（无需等待），CockroachDB 总是选择中止其中一个事务。 确定哪笔交易的过程如下：</p>
<ul>
<li><p>The second transaction (which is encountering an intent) looks up the first transaction’s transaction record, the location of which is present in the intent.</p>
<p>第二个事务（遇到意图）查找第一个事务的事务记录，该记录的位置存在于意图中。</p>
</li>
<li><p>The transaction performs a “<strong>push</strong>” on the discovered transaction record. The push operation is as follows:</p>
<p>交易对发现的交易记录执行“推送”。 推送操作如下：</p>
<ul>
<li><p>If the first transaction is already committed (the intent was not yet cleaned up), then the second transaction can clean up the intent and proceed as if the intent were a normal value.</p>
<p>如果第一个事务已经提交（意图尚未清除），则第二个事务可以清除意图并继续执行，就好像意图是正常值一样。</p>
</li>
<li><p>Likewise, if the other transaction already aborted, the intent can be removed and the second transaction can proceed as if the intent were not present.</p>
<p>同样，如果另一个事务已经中止，则可以删除意图，并且第二个事务可以继续进行，就好像意图不存在一样。</p>
</li>
<li><p>Otherwise, the surviving transaction is deterministic according to <strong>priority</strong>.</p>
<p>否则，幸存的事务根据优先级是确定的。</p>
<ul>
<li><p>It is not optimal to always abort either the pusher or pushee; there are cases where both transactions will attempt to push the other, so “victory” must be deterministic between any transaction pair.</p>
<p>始终中止推动者或被推动者并不是最佳选择； 在某些情况下，两个交易都会尝试推动另一个交易，因此任何交易对之间的“胜利”必须是确定性的。</p>
</li>
<li><p>Each transaction record is thus assigned a <strong>priority</strong>; priority is an integer number. In a push operation, the transaction with the lowest priority is always aborted (if priority is equal, the transaction with the higher timestamp is aborted. In the extremely rare case where both are equal, the pushing transaction is aborted).</p>
<p>每条交易记录因此被分配一个优先级； 优先级是一个整数。 在推送操作中，优先级最低的事务总是被中止（如果优先级相同，则时间戳较高的事务将被中止。在极少数情况下，两者相等，推送事务将被中止）。</p>
</li>
<li><p>New transactions have a random priority. If a transaction is aborted by a push operation and is restarted, its new priority is <code>max(randomInt(), [priority of transaction that caused the restart] - 1]);</code> this has the effect of probabilistically ratcheting up a transaction’s priority if it is restarted multiple times.</p>
<p>新交易具有随机优先级。 如果事务被推送操作中止并重新启动，则其新优先级为 max(randomInt(), [导致重新启动的事务的优先级] - 1])； 如果多次重新启动事务，这会在概率上提高事务的优先级。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In this way, all conflicts between uncommitted transactions are immediately resolved by aborting one of the transactions, thus enforcing strict scheduling and guaranteeing that all transaction histories are recoverable.</p>
<p>这样，未提交事务之间的所有冲突都可以通过中止其中一个事务来立即解决，从而执行严格的调度并保证所有事务历史都是可恢复的。</p>
<h4 id="Note-on-Abandoned-Transactions"><a href="#Note-on-Abandoned-Transactions" class="headerlink" title="Note on Abandoned Transactions"></a>Note on Abandoned Transactions</h4><p>As mentioned earlier, in a concurrent environment we can no longer assume that unresolved write intents belong to abandoned transactions; we must deal with abandoned transactions in a different way. The <strong>priority</strong> system already aborts abandoned transactions probabilistically – transactions blocked by the abandoned transaction will eventually have a high enough priority to usurp it.</p>
<p>如前所述，在并发环境中，我们不能再假设未解决的写意图属于已放弃的事务； 我们必须以不同的方式处理废弃的交易。 优先级系统已经有概率地中止被放弃的交易——被被放弃的交易阻止的交易最终将拥有足够高的优先级来篡夺它。</p>
<p>However, we additionally add a <strong>heartbeat</strong> timestamp to every transaction. While in progress, an active transaction is responsible for periodically updating the heartbeat timestamp on its central transaction record; if a push operation encounters a transaction with an expired heartbeat timestamp, then it is considered abandoned and can be aborted regardless of priority.</p>
<p>但是，我们还为每笔交易添加了心跳时间戳。 在进行过程中，活动事务负责定期更新其中央事务记录上的心跳时间戳； 如果推送操作遇到心跳时间戳过期的事务，则该操作将被视为放弃，并且无论优先级如何都可以中止。</p>
<h2 id="Wrap-Up-包起来"><a href="#Wrap-Up-包起来" class="headerlink" title="Wrap Up 包起来"></a>Wrap Up 包起来</h2><p>We have now demonstrated how CockroachDB’s Isolation system is able to provide a serializable and recoverable transaction history in a completely distributed fashion. Combined with our atomic commit post, we have already described a fairly robust system for executing concurrent, distributed ACID transactions. That said, there are still many aspects to CockroachDB’s transaction system that we have not yet covered.</p>
<p>我们现在已经演示了 CockroachDB 的隔离系统如何能够以完全分布式的方式提供可序列化和可恢复的事务历史记录。 结合我们的原子提交帖子，我们已经描述了一个用于执行并发、分布式 ACID 事务的相当健壮的系统。 也就是说，CockroachDB 的事务系统还有很多方面我们还没有涉及到。</p>
<p>For example, CockroachDB offers another, more <em>relaxed</em> isolation level known as <strong>Snapshot</strong> (without the “serializable”)<em>.</em> Like relaxed isolation levels in other database systems, this mode increases concurrency performance by allowing transactions to interleave in certain cases; for some applications, this is an acceptable tradeoff.</p>
<p>例如，CockroachDB 提供了另一种更宽松的隔离级别，称为快照（没有“可序列化”）。 与其他数据库系统中宽松的隔离级别一样，此模式通过允许事务在某些情况下交错来提高并发性能； 对于某些应用程序来说，这是一个可以接受的折衷方案。</p>
<p>Another aspect is how CockroachDB provides <em>linearizable</em> access to its data. Linearizability is a property that can be difficult to provide in a distributed system. Spencer Kimball has already written <a href="https://www.cockroachlabs.com/blog/living-without-atomic-clocks/">this blog post</a> demonstrating how CockroachDB deals with this in some detail (contrasting it with the way a similar system, Google’s Spanner, does the same); however, we may eventually write an additional linearizability blog post focused more directly on our transaction system.</p>
<p>另一个方面是 CockroachDB 如何提供对其数据的线性化访问。 线性化是分布式系统中很难提供的属性。 Spencer Kimball 已经撰写了这篇博客文章，详细演示了 CockroachDB 如何处理此问题（将其与类似系统 Google 的 Spanner 的处理方式进行对比）； 然而，我们最终可能会写一篇额外的线性化博客文章，更直接地关注我们的交易系统。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>How Pipelining consensus writes speeds up distributed SQL transactions</title>
    <url>/2023/10/31/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/04-Pipelining%20consensus%20writes/</url>
    <content><![CDATA[<p>CockroachDB supports ACID transactions across arbitrary data in a distributed database. A discussion on how this works was first <a href="https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/">published on our blog</a> three years ago. Since then, a lot has changed. Perhaps most notably, CockroachDB has transitioned from a key-value store to a full SQL database that can be plugged in as a <a href="https://www.cockroachlabs.com/blog/why-postgres/">scalable, highly-available replacement for PostgreSQL</a>. It did so by introducing a SQL execution engine which <a href="https://www.cockroachlabs.com/blog/sql-in-cockroachdb-mapping-table-data-to-key-value-storage/">maps SQL tables onto its distributed key-value architecture</a>. However, over this period of time, the fundamentals of the distributed, atomic transaction protocol at the core of CockroachDB have remained untouched <a href="https://www.cockroachlabs.com/blog/transaction-pipelining/#fn:1">1</a>.</p>
<p>CockroachDB 支持分布式数据库中任意数据的 ACID 事务。 三年前，我们的博客首次发表了关于其工作原理的讨论。 从那时起，很多事情都发生了变化。 也许最值得注意的是，CockroachDB 已经从键值存储转变为完整的 SQL 数据库，可以作为 PostgreSQL 的可扩展、高可用性替代品插入。 它通过引入 SQL 执行引擎来实现这一点，该引擎将 SQL 表映射到其分布式键值架构上。 然而，在这段时间里，CockroachDB 核心的分布式原子事务协议的基本原理并未受到影响1。</p>
<span id="more"></span>

<p>For the most part, this hasn’t been an issue. The transaction protocol in CockroachDB was built to scale out to tremendously large clusters with arbitrary data access patterns. It does so efficiently while permitting serializable multi-key reads and writes. These properties have been paramount in allowing CockroachDB to evolve from a key-value store to a SQL database. However, CockroachDB has had to pay a price for this consistency in terms of transaction latency. When compared to other consensus systems offering weaker transaction semantics, CockroachDB often needed to perform more synchronous consensus rounds to navigate a transaction. However, we realized that we could improve transaction latency by introducing concurrency between these rounds of consensus.</p>
<p>在大多数情况下，这不是问题。 CockroachDB 中的事务协议旨在扩展到具有任意数据访问模式的超大型集群。 它可以高效地实现这一点，同时允许可序列化的多键读取和写入。 这些属性对于 CockroachDB 从键值存储发展为 SQL 数据库至关重要。 然而，CockroachDB 必须为这种一致性付出交易延迟方面的代价。 与提供较弱事务语义的其他共识系统相比，CockroachDB 通常需要执行更多同步共识轮次来导航事务。 然而，我们意识到，我们可以通过在这些轮次共识之间引入并发性来改善交易延迟。</p>
<p><strong>This post will focus on an extension to the CockroachDB transaction protocol called <code>Transactional Pipelining</code>, which was introduced in CockroachDB’s recent 2.1 release. The optimization promises to dramatically speed up distributed transactions, reducing their time complexity from <code>O(n)</code> to <code>O(1)</code>, where <code>n</code> is the number of DML SQL statements executed in the transaction and the analysis is expressed with respect to the latency cost of distributed consensus.</strong></p>
<p>这篇文章将重点介绍 CockroachDB 事务协议的扩展，称为事务管道，它是在 CockroachDB 最近的 2.1 版本中引入的。 该优化有望显着加速分布式事务，将其时间复杂度从 O(n) 降低到 O(1)，其中 n 是事务中执行的 DML SQL 语句的数量，分析是针对以下延迟成本来表示的： 分布式共识。</p>
<p>The post will give a recap of core CockroachDB concepts before using them to derive a performance model for approximating transaction latency in CockroachDB. It will then dive into the extension itself, demonstrating its impact on the performance model and providing experimental results showing its effects on real workloads. The post will wrap up with a preview of how we intend to extend this optimization further in upcoming releases to continue speeding up transactions.</p>
<p>这篇文章将回顾 CockroachDB 的核心概念，然后使用它们来导出性能模型来近似 CockroachDB 中的事务延迟。 然后，它将深入研究扩展本身，展示其对性能模型的影响，并提供实验结果来显示其对实际工作负载的影响。 这篇文章最后将预览我们打算如何在即将发布的版本中进一步扩展此优化，以继续加快交易速度。</p>
<h2 id="Distributed-Transactions-A-Recap-回顾"><a href="#Distributed-Transactions-A-Recap-回顾" class="headerlink" title="Distributed Transactions: A Recap 回顾"></a>Distributed Transactions: A Recap 回顾</h2><p>CockroachDB allows transactions to span an entire cluster, providing ACID guarantees across arbitrary numbers of machines, data centers, and geographical regions. This is all exposed through SQL — meaning that you can <code>BEGIN</code> a transaction, issue any number of read and write statements, and <code>COMMIT</code> the transaction, all without worrying about inconsistencies or loss of durability. In fact, CockroachDB provides the <a href="https://heoric.github.io/2023/10/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/Real%20transactions%20are%20serializable/">strongest level of isolation</a>, <code>SERIALIZABLE</code>, so that the integrity of your data is always preserved.</p>
<p>CockroachDB 允许事务跨越整个集群，为任意数量的机器、数据中心和地理区域提供 ACID 保证。 这一切都是通过 SQL 公开的 — 这意味着您可以 BEGIN 事务、发出任意数量的读取和写入语句以及 COMMIT 事务，而无需担心不一致或持久性损失。 事实上，CockroachDB 提供了最强的隔离级别（SERIALIZABLE），以便始终保持数据的完整性。</p>
<p>There are a few competing ideas which combine to make this all possible, each of which is important to understand. Below is a brief introduction to each. For those interested in exploring further, more detail can be found in our <a href="https://www.cockroachlabs.com/docs/stable/architecture/overview">architecture documentation</a>.</p>
<p>有一些相互竞争的想法结合起来使这一切成为可能，理解每一个都很重要。 下面对每一项进行简要介绍。 对于那些有兴趣进一步探索的人，可以在我们的架构文档中找到更多详细信息。</p>
<h3 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h3><p>At its most fundamental level, the goal of a durable database is to persist committed data such that it will survive permanently. This is traditionally performed by a storage engine, which writes bytes to a non-volatile storage medium. CockroachDB uses <a href="https://rocksdb.org/">RocksDB</a>, an embedded key-value database maintained by Facebook, as its storage engine. RocksDB builds upon its pedigree (<a href="https://github.com/google/leveldb">LevelDB</a> and more generally the <a href="https://www.cs.umb.edu/~poneil/lsmtree.pdf">Log-structured merge-tree (LSM tree) data structure</a>) to strike a balance between high write throughput, low space amplification, and acceptable read performance. This makes it a good choice for CockroachDB, which runs a separate instance of RocksDB on each individual node in a cluster.</p>
<p>在最基本的层面上，持久数据库的目标是持久保存已提交的数据，使其永久保存。 传统上，这是由存储引擎执行的，该引擎将字节写入非易失性存储介质。 CockroachDB 使用 RocksDB（Facebook 维护的嵌入式键值数据库）作为其存储引擎。 RocksDB 建立在其谱系（LevelDB 以及更普遍的日志结构合并树（LSM 树）数据结构）之上，以在高写入吞吐量、低空间放大和可接受的读取性能之间取得平衡。 这使得它成为 CockroachDB 的一个不错的选择，它在集群中的每个节点上运行一个单独的 RocksDB 实例。</p>
<p>Even with software improvements like improved indexing structures and hardware improvements like the emergence of SSDs, persistence is still expensive both in terms of the latency it imposes on each individual write and in terms of the bounds it places on write throughput. For the remainder of this post, we’ll refer to the first cost here as <strong>“storage latency”</strong>.</p>
<p>即使有了索引结构改进等软件改进和 SSD 出现等硬件改进，持久性仍然是昂贵的，无论是在每次写入所带来的延迟方面，还是在写入吞吐量的限制方面。 在本文的其余部分中，我们将这里的第一个成本称为“存储延迟”。</p>
<h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><p>Replicating data across nodes allows CockroachDB to provide high-availability in the face of the chaotic nature of distributed systems. By default, every piece of data in CockroachDB is replicated across three nodes in a cluster (though this is configurable)—we refer to these as “replicas”, and each node contains many replicas. Each individual node takes responsibility for persisting its own replica data. This ensures that even if nodes lose power or lose connectivity with one another, as long as a majority of the replicas are available, the data will stay available to read and write. Like other modern distributed systems, CockroachDB uses the <a href="https://raft.github.io/">Raft consensus protocol</a> to manage coordination between replicas and to achieve fault-tolerant consensus, upon which this state replication is built. We’ve <a href="https://www.cockroachlabs.com/blog/consensus-made-thrive/">published</a> about this topic before.</p>
<p>跨节点复制数据使 CockroachDB 能够在面对分布式系统的混乱性质时提供高可用性。 默认情况下，CockroachDB 中的每条数据都会在集群中的三个节点上进行复制（尽管这是可配置的）——我们将这些称为“副本”，每个节点都包含许多副本。 每个单独的节点负责保存自己的副本数据。 这确保了即使节点断电或彼此失去连接，只要大多数副本可用，数据将保持可读写状态。 与其他现代分布式系统一样，CockroachDB 使用 Raft 共识协议来管理副本之间的协调并实现容错共识，在此基础上构建状态复制。 我们之前曾发表过有关此主题的文章。</p>
<p>Of course, the benefits of replication come at the cost of coordination latency. Whenever a replica wants to make a change to a particular piece of its replicated data, it “proposes” that change to the other replicas and multiple nodes must come to an agreement about what to change and when to change it. To maintain strong consistency during this coordination, Raft (and other consensus protocols like it) require at least a majority of replicas (e.g. a quorum of 2 nodes for a replication group of 3 nodes) to agree on the details of the change.</p>
<p>当然，复制的好处是以调度延迟为代价的。 每当副本想要对其复制数据的特定部分进行更改时，它都会“建议”对其他副本进行更改，并且多个节点必须就更改内容以及何时更改达成一致。 为了在协调过程中保持强一致性，Raft（以及其他类似的共识协议）需要至少大多数副本（例如，3 个节点的复制组的 2 个节点的法定数量）就更改的细节达成一致。</p>
<p>In its steady-state, Raft allows the proposing replica to achieve this agreement with just a single network call to each other replica in its replication group. The proposing replica must then wait for a majority of replicas to respond positively to its proposal. This can be done in parallel for every member in the group, meaning that at a minimum, consensus incurs the cost of a single round-trip network call to the median slowest member of the replication group. For the remained of this post, we’ll refer to this as “replication latency”.</p>
<p>在稳定状态下，Raft 允许提议副本只需对其复制组中的每个其他副本进行一次网络调用即可达成此协议。 然后，提议的副本必须等待大多数副本对其提议做出积极响应。 这可以为组中的每个成员并行完成，这意味着共识至少会产生对复制组中最慢成员的单次往返网络调用的成本。 在本文的其余部分，我们将其称为“复制延迟”。</p>
<h3 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h3><p>Replicating data across nodes improves resilience, but it doesn’t allow data to scale indefinitely. For that, CockroachDB needs to distribute different data across the nodes in a cluster, storing only a subset of the total data on each individual node. To do this, CockroachDB breaks data into 64MB chunks, called Ranges. These Ranges operate independently and each manage its own N-way replication. The Ranges automatically split, merge, and move around a cluster to hold a suitable amount of data and to stay healthy (i.e. fully-replicated) if nodes crash or become unreachable.</p>
<p>跨节点复制数据可以提高弹性，但不允许数据无限扩展。 为此，CockroachDB 需要在集群中的节点之间分布不同的数据，在每个单独的节点上仅存储总数据的子集。 为此，CockroachDB 将数据分成 64MB 的块，称为范围。 这些范围独立运行，并且每个范围都管理自己的 N 路复制。 范围会自动拆分、合并并在集群中移动，以保存适量的数据，并在节点崩溃或无法访问时保持健康（即完全复制）。</p>
<p>A Range is made up of Replicas, which are members of the Range who hold a copy of its state and live on different nodes. Each Range has a single “leaseholder” Replica who both coordinates writes for the Range, as well as serves reads from its local RocksDB store. The leaseholder Replica is defined as the Replica at any given time who holds a time-based “range lease”. This lease can be moved between the Replicas as they see fit. For the purpose of this post, we’ll always assume that the leaseholder Replica is collocated with (in the same data center as) the node serving SQL traffic. This is not always the case, but automated processes like <a href="https://www.cockroachlabs.com/docs/stable/topology-follow-the-workload">Follow-the-Workload</a> do their best to enforce this collocation, and lease preferences make it possible to manually control leaseholder placement.</p>
<p>Range 由副本组成，副本是 Range 的成员，拥有其状态的副本并位于不同的节点上。 每个 Range 都有一个“租赁持有者”副本，它既协调 Range 的写入操作，又提供本地 RocksDB 存储的读取服务。 租赁持有者副本被定义为在任何给定时间持有基于时间的“范围租约”的副本。 该租约可以在副本之间移动，只要他们认为合适。 出于本文的目的，我们始终假设租赁者副本与提供 SQL 流量的节点并置（在同一数据中心）。 情况并非总是如此，但像“跟踪工作负载”这样的自动化流程会尽最大努力强制实施这种搭配，并且租赁偏好使得手动控制承租人安置成为可能。</p>
<p>With a distribution policy built on top of consistent replication, a CockroachDB cluster is able to scale to an arbitrary number of Ranges and to move Replicas in these Ranges around to ensure resilience and <a href="https://www.cockroachlabs.com/product/geo-partitioning/">localized access</a>. However, as is becoming the trend in this post, this also comes at a cost. Because distribution forces data to be split across multiple replication groups (i.e. multiple Ranges), we lose the ability to trivially order operations if they happen in different replication groups. This loss of linearizable ordering across Ranges is what necessitates the distributed transaction protocol that the rest of this post will focus on.</p>
<p>通过建立在一致复制之上的分发策略，CockroachDB 集群能够扩展到任意数量的范围，并在这些范围内移动副本，以确保弹性和本地化访问。 然而，正如本文中的趋势一样，这也是有代价的。 因为分布强制数据跨多个复制组（即多个范围）进行分割，所以如果操作发生在不同的复制组中，我们就失去了对操作进行简单排序的能力。 这种跨范围的线性化排序的损失使得分布式事务协议成为必要，本文的其余部分将重点讨论这一点。</p>
<h3 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h3><p>CockroachDB’s transactional protocol implements ACID transactions on top of the scalable, fault-tolerant foundation that its storage, replication, and distribution layers combine to provide. It does so while allowing transactions to span an arbitrary number of Ranges and as many participating nodes as necessary. The protocol was inspired in part by <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf">Google Percolator</a>, and it follows a similar pattern of breaking distributed transactions into three distinct phases:</p>
<p>CockroachDB 的事务协议在其存储、复制和分发层共同提供的可扩展、容错基础之上实现了 ACID 事务。 它这样做的同时允许交易跨越任意数量的范围和所需的任意数量的参与节点。 该协议部分受到 Google Percolator 的启发，它遵循类似的模式，将分布式事务分为三个不同的阶段：</p>
<p><strong>1. Preparation</strong></p>
<p>A transaction begins when a SQL <code>BEGIN</code> statement is issued. At that time, the transaction determines the timestamp at which it will operate and prepares to execute SQL statements. From this point on, the transaction will perform all reads and writes at its pre-determined timestamp. Those with prior knowledge of CockroachDB may remember that its storage layer implements <a href="https://www.cockroachlabs.com/docs/stable/architecture/storage-layer#mvcc">multi-version concurrency control</a>, meaning that transactional reads are straightforward even if other transactions modify the same data at later timestamps.</p>
<p>当发出 SQL BEGIN 语句时事务开始。 此时，事务确定将要操作的时间戳并准备执行 SQL 语句。 从此时起，事务将按照其预先确定的时间戳执行所有读取和写入。 了解过 CockroachDB 的人可能还记得，它的存储层实现了多版本并发控制，这意味着即使其他事务在稍后的时间戳修改了相同的数据，事务读取也很简单。</p>
<p>When the transaction executes statements that mutate data (<a href="https://en.wikipedia.org/wiki/Data_manipulation_language">DML statements</a>), it doesn’t write committed values immediately. Instead, it creates two things that help it manage its progress:</p>
<p>当事务执行改变数据的语句（DML 语句）时，它不会立即写入提交的值。 相反，它创建了两个东西来帮助它管理进度：</p>
<ul>
<li><p>The first write of the transaction creates a <strong>transaction record</strong> which includes the transaction’s current status. This transaction record acts as the transaction’s “switch”. It begins in the “pending” state and is eventually switched to “committed” to signify that the transaction has committed.</p>
<p>交易的第一次写入会创建一个<strong>交易记录</strong>，其中包括交易的当前状态。 这条交易记录充当了交易的“开关”。 它从“待处理”状态开始，最终切换到“已提交”状态，表示事务已提交。</p>
</li>
<li><p>The transaction creates <strong>write intents</strong> for each of the key-value data mutations it intends to make. The intents represent provisional, uncommitted state which lives on the same Ranges as their corresponding data records. As such, a transaction can end up spreading intents across Ranges and across an entire cluster as it performs writes during the preparation phase. Write intents point at their transaction’s record and indicate to readers that they must check the status of the transaction record before treating the intent’s value as the source of truth or before ignoring it entirely.</p>
<p>该事务为其打算进行的每个键值数据突变创建<strong>写入意图</strong>。 意图表示临时的、未提交的状态，其与其相应的数据记录位于相同的范围内。 因此，当事务在准备阶段执行写入时，它最终可能会跨范围和整个集群传播意图。 写入意图指向其交易记录，并向读者表明他们必须在将意图的值视为事实来源或完全忽略它之前检查交易记录的状态。</p>
</li>
</ul>
<p><strong>2. Commit</strong></p>
<p>When a SQL transaction has finished issuing read and write statements, it executes a <code>COMMIT</code> statement. What happens next is simple - the transaction visits its transaction record, checks if it has been aborted, and if not, it flips its switch from “pending” to “committed”. The transaction is now committed and the client can be informed of the success.</p>
<p>当 SQL 事务完成发出读写语句时，它会执行 COMMIT 语句。 接下来发生的事情很简单 - 交易访问其交易记录，检查是否已中止，如果没有，则将其开关从“待处理”切换到“已提交”。 交易现已提交，并且可以通知客户交易成功。</p>
<p><strong>3. Cleanup</strong></p>
<p>After the transaction has been resolved and the client has been acknowledged, an asynchronous process is launched to replace all provisional write intents with committed values. This reduces the chance that future readers will observe the intents and need to check in with the intents’ transaction record to determine its disposition. This can be important for performance because checking the status of another transaction by visiting its transaction record can be expensive. However, this cleanup process is strictly an optimization and not a matter of correctness.</p>
<p>事务解决并且客户端已被确认后，将启动异步进程以用提交的值替换所有临时写入意图。 这减少了未来读者观察意图并需要检查意图的交易记录以确定其处置的机会。 这对于性能来说很重要，因为通过访问另一个事务的事务记录来检查另一个事务的状态可能会很昂贵。 然而，这个清理过程严格来说是一种优化，而不是正确性问题。</p>
<p>That high-level overview of the transaction protocol in CockroachDB should be sufficient for the rest of this post, but those who are interested can learn more in <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer">our docs</a>.</p>
<p>CockroachDB 中事务协议的高级概述对于本文的其余部分来说应该足够了，但是感兴趣的人可以在我们的文档中了解更多信息。</p>
<h2 id="The-Cost-of-Distributed-Transactions-in-CockroachDB"><a href="#The-Cost-of-Distributed-Transactions-in-CockroachDB" class="headerlink" title="The Cost of Distributed Transactions in CockroachDB"></a>The Cost of Distributed Transactions in CockroachDB</h2><p>With an understanding of the three phases of a distributed transaction in CockroachDB and an understanding of the abstractions upon which they are built, we can begin to construct a performance model that captures the cost of distributed transactions. Specifically, our model will approximate the latency that a given transaction will incur when run through CockroachDB 2.0 and earlier.</p>
<p>了解了 CockroachDB 中分布式事务的三个阶段以及构建这些阶段的抽象之后，我们就可以开始构建一个捕获分布式事务成本的性能模型。 具体来说，我们的模型将近似给定事务在通过 CockroachDB 2.0 及更早版本运行时将产生的延迟。</p>
<h3 id="Model-Assumptions-模型假设"><a href="#Model-Assumptions-模型假设" class="headerlink" title="Model Assumptions 模型假设"></a>Model Assumptions 模型假设</h3><p>To begin, we’ll establish a few simplifying assumptions that will make our latency model easier to work with and visualize.</p>
<p>首先，我们将建立一些简化的假设，使我们的延迟模型更易于使用和可视化。</p>
<ol>
<li><p><strong>The first assumption we’ll make is that the two dominant latency costs in distributed transactions are storage latency and replication latency</strong>. That is, the cost to replicate data between replicas in a Range and the cost to persist it to disk on each replica will dominate all other latencies in the transaction such that everything else can safely be ignored in our model. To safely make this approximation, we must assume that Range leaseholders are collocated with the CockroachDB nodes serving SQL traffic. This allows us to ignore any network latency between SQL gateways and Range leaseholders when performing KV reads and writes. As we <a href="https://www.cockroachlabs.com/blog/transaction-pipelining/#distribution">discussed earlier</a>, this is a safe and realistic assumption to make. Likewise, we must also assume that the network latency between the client application issuing SQL statements and the SQL gateway node executing them is sufficiently negligible. If all client applications talk to CockroachDB nodes within their local data centers&#x2F;zones, this is also a safe assumption.</p>
<p>我们要做的第一个假设是分布式事务中两个主要的延迟成本是存储延迟和复制延迟。 也就是说，在范围内的副本之间复制数据的成本以及将数据持久保存到每个副本上的磁盘的成本将主导事务中的所有其他延迟，以便在我们的模型中可以安全地忽略其他所有内容。 为了安全地进行这种近似，我们必须假设 Range 租用者与提供 SQL 流量的 CockroachDB 节点并置。 这使我们能够在执行 KV 读写时忽略 SQL 网关和 Range 租用者之间的任何网络延迟。 正如我们之前讨论的，这是一个安全且现实的假设。 同样，我们还必须假设发出 SQL 语句的客户端应用程序和执行它们的 SQL 网关节点之间的网络延迟可以忽略不计。 如果所有客户端应用程序都与本地数据中心&#x2F;区域内的 CockroachDB 节点通信，这也是一个安全的假设。</p>
</li>
<li><p><strong>The second assumption we’ll make is that the transactional workload being run is sufficiently uncontended such that any additional latency due to queuing for lock and latch acquisition is negligible.</strong> This holds true for most workloads, but will not always be the case in workloads that create large write hotspots, like <a href="https://github.com/brianfrankcooper/YCSB">YCSB</a> in its zipfian distribution mode. It’s our belief that a crucial property of successful schema design is the avoidance of write hotspots, so we think this is a safe assumption to make.</p>
<p>我们要做的第二个假设是，正在运行的事务工作负载完全没有竞争，因此由于排队等待锁和闩锁获取而产生的任何额外延迟都可以忽略不计。 对于大多数工作负载来说都是如此，但在创建大型写入热点的工作负载中情况并非总是如此，例如 zipfian 分发模式下的 YCSB。 我们相信，成功的模式设计的一个关键特性是避免写入热点，因此我们认为这是一个安全的假设。</p>
</li>
<li><p><strong>Finally, the third assumption we’ll make is that the CockroachDB cluster is operating under a steady-state that does not include chaos events.</strong> CockroachDB was built to survive catastrophic failures across a cluster, but failure events can still induce latencies on the order of a few seconds to live traffic as Ranges recover, Range leases change hands, and data is migrated in response to the unreachable nodes. These events are a <a href="http://www.hpl.hp.com/techreports/tandem/TR-85.7.pdf">statistical</a> <a href="https://ai.google/research/pubs/pub36737">given</a> in a large-scale distributed system, but they shouldn’t represent the cluster’s typical behavior –– so, for the sake of this performance model, it’s safe to assume they are absent.</p>
<p>最后，我们要做的第三个假设是 CockroachDB 集群在不包含混乱事件的稳态下运行。 CockroachDB 的构建是为了在整个集群中承受灾难性故障，但随着范围恢复、范围租约易手以及为响应无法访问的节点而迁移数据，故障事件仍然可能导致实时流量大约几秒的延迟。 这些事件是大规模分布式系统中给出的统计数据，但它们不应该代表集群的典型行为——因此，为了这个性能模型，可以安全地假设它们不存在。</p>
</li>
</ol>
<p>The model will not be broken if any of these assumptions are incorrect, but it will need to be adapted to account for changes in latency characteristics.</p>
<p>如果这些假设中的任何一个不正确，该模型都不会被破坏，但需要对其进行调整以考虑延迟特性的变化。</p>
<h3 id="Latency-Model"><a href="#Latency-Model" class="headerlink" title="Latency Model"></a>Latency Model</h3><p>First, let’s define exactly what we mean by “latency”. Because we’re most interested in the latency observed by applications, we define transactional latency as “the delay between when a client application first issues its BEGIN statement and when it gets an acknowledgement that its COMMIT statement succeeded.” Remember that SQL is “conversational”, meaning that clients typically issue a statement and wait for its response before issuing the next one.</p>
<p>首先，让我们准确定义“延迟”的含义。 因为我们对应用程序观察到的延迟最感兴趣，所以我们将事务延迟定义为“客户端应用程序首次发出 BEGIN 语句与收到 COMMIT 语句成功确认之间的延迟”。 请记住，SQL 是“会话式”的，这意味着客户端通常会发出一条语句并等待其响应，然后再发出下一条语句。</p>
<p>We then take this definition and apply it to CockroachDB’s <a href="https://www.cockroachlabs.com/blog/transaction-pipelining/#transactions">transaction protocol</a>. The first thing we see is that because we defined latency from the client’s perspective, the asynchronous third phase of cleaning up write intents can be ignored. This reduces the protocol down to just two client-visible phases: everything before the <code>COMMIT</code> statement is issued by the client and everything after. Let’s call these two component latencies <code>L_prep</code> and <code>L_commit</code>, respectively. Together, they combine to a total transitional latency <code>L_txn = L_prep + L_commit</code>.</p>
<p>然后我们采用这个定义并将其应用到 CockroachDB 的事务协议中。 我们首先看到的是，因为我们从客户端的角度定义了延迟，所以可以忽略清理写入意图的异步第三阶段。 这将协议减少到只有两个客户端可见的阶段：COMMIT 语句之前的所有内容均由客户端发出，以及之后的所有内容。 我们将这两个组件延迟分别称为 L_prep 和 L_commit。 它们共同构成总转换延迟 L_txn &#x3D; L_prep + L_commit。</p>
<p>The goal of our model is then to characterize <code>L_prep</code> and <code>L_commit</code> in terms of the two dominant latency costs of the transaction so that we can define <code>L_txn</code> as a function of this cost. It just so happens that these two dominant latency costs are always paid as a pair, so we can define this unit latency as <code>L_c</code>, which can be read as “the latency of distributed consensus”. This latency is a function of both the <a href="https://www.cockroachlabs.com/blog/transaction-pipelining/#replication">replication</a> layer and the <a href="https://www.cockroachlabs.com/blog/transaction-pipelining/#storage">storage</a> layer. It can be expressed to a first-order approximation as the latency of a single round-trip network call to, plus a synchronous disk write on, the median slowest member of a replication group (i.e. Range). This value is highly dependent on a cluster’s network topology and on its storage hardware, but is typically on the order of single or double digit milliseconds.</p>
<p>我们模型的目标是根据交易的两个主要延迟成本来表征 L_prep 和 L_commit，以便我们可以将 L_txn 定义为该成本的函数。 恰巧这两个主要的延迟成本总是成对付出的，所以我们可以将这个单位延迟定义为L_c，可以理解为“分布式共识的延迟”。 该延迟是复制层和存储层的函数。 它可以用一阶近似表示为单个往返网络调用的延迟，加上同步磁盘写入，复制组中最慢的成员（即范围）。 该值高度依赖于集群的网络拓扑及其存储硬件，但通常约为一位数或两位数毫秒。</p>
<p>To define <code>L_prep</code> in terms of the unit latency <code>L_c</code>, we first need to enumerate everything a transaction can do before a <code>COMMIT</code> is issued. For the sake of this model, we’ll say that a transaction can issue <code>R</code> read statements (e.g. <code>SELECT * FROM t LIMIT 1</code>) and <code>W</code> write statements (e.g. <code>INSERT INTO t VALUES (1)</code>). If we define the latency of a read statement as <code>L_r</code> and the latency of a write statement as <code>L_w</code>, then the total latency of <code>L_prep = R * L_r + W * L_w</code>. So far, so good. It turns out that because leaseholders in CockroachDB can serve KV reads locally without coordination (the committed value already achieved consensus), and because we assumed that the leaseholders were all collocated with the SQL gateways, <code>L_r</code> approaches 0 and the model simplifies to <code>L_prep = W * L_w</code>. Of course, this isn’t actually true; reads aren’t free. In some sense, this shows a limitation of our model, but given the constraints we’ve placed on it and the assumptions we’ve made, it’s reasonable to assume that sufficiently small, OLTP-like reads have a negligible cost on the latency of a transaction.</p>
<p>为了根据单位延迟 L_c 定义 L_prep，我们首先需要枚举事务在发出 COMMIT 之前可以执行的所有操作。 为了这个模型，我们会说事务可以发出 R 读语句（例如 SELECT * FROM t LIMIT 1）和 W 写语句（例如 INSERT INTO t VALUES (1)）。 如果我们将读语句的延迟定义为 L_r，将写语句的延迟定义为 L_w，则 L_prep 的总延迟 &#x3D; R * L_r + W * L_w。 到目前为止，一切都很好。 事实证明，由于 CockroachDB 中的租约持有者可以在本地无需协调地提供 KV 读取服务（承诺值已达成共识），并且由于我们假设租约持有者都与 SQL 网关并置，因此 L_r 接近 0，模型简化为 L_prep &#x3D; W * L_w。 当然，事实并非如此。 阅读不是免费的。 从某种意义上说，这显示了我们模型的局限性，但考虑到我们对其施加的约束以及我们所做的假设，可以合理地假设足够小的、类似 OLTP 的读取对延迟的成本可以忽略不计。 一笔交易。</p>
<p>With <code>L_prep</code> reduced to <code>L_prep = W * L_w</code>, we now just need to characterize the cost of <code>L_w</code> in terms of <code>L_c</code>. This is where details about CockroachDB’s transaction protocol implementation come into play.</p>
<p>随着 L_prep 减少到 L_prep &#x3D; W * L_w，我们现在只需要用 L_c 来表征 L_w 的成本。 这就是有关 CockroachDB 事务协议实现的详细信息发挥作用的地方。</p>
<p>To begin, we know that the transaction protocol creates a transaction record during the first phase. We also know that the transaction protocol creates a write intent for every modified key-pair during this phase. Both the transaction record and the write intents are replicated and persisted in order to maintain consistency. This means that naively <code>L_prep</code> would incur a single <code>L_c</code> cost when creating the transaction record and an <code>L_c</code> cost for every key-value pair modified across all writing statements. However, this isn’t actually the cost of <code>L_prep</code> for two reasons:</p>
<p>首先，我们知道交易协议在第一阶段创建交易记录。 我们还知道，交易协议在此阶段为每个修改的密钥对创建一个写入意图。 事务记录和写入意图都会被复制和持久化，以保持一致性。 这意味着，天真的 L_prep 在创建事务记录时会产生单个 L_c 成本，并且会为所有写入语句中修改的每个键值对产生 L_c 成本。 然而，这实际上并不是 L_prep 的成本，原因有二：</p>
<ol>
<li><p>The transaction record is not created immediately after the transaction begins. Instead, it is collocated with and written in the same batch as the first write intent, meaning that the latency cost to create the transaction record is completely hidden and therefore can be ignored.</p>
<p>交易记录并不是在交易开始后立即创建的。 相反，它与第一个写入意图并置并在同一批次中写入，这意味着创建事务记录的延迟成本完全隐藏，因此可以忽略不计。</p>
</li>
<li><p>Every provisional write intent for a SQL statement is created in parallel, meaning that regardless of how many key-value pairs a SQL statement modifies, it only incurs a single <code>L_c</code> cost. A SQL statement may touch multiple key-value pairs if it touches a single row with a secondary index or if it touches multiple distinct rows. This explains why using <a href="https://www.cockroachlabs.com/blog/multi-row-dml/">multi-row DML statements</a> can lead to such dramatic performance improvements.</p>
<p>SQL 语句的每个临时写入意图都是并行创建的，这意味着无论 SQL 语句修改多少个键值对，它都只会产生单个 L_c 成本。 如果 SQL 语句涉及具有辅助索引的单个行或涉及多个不同的行，则它可能会涉及多个键值对。 这就解释了为什么使用多行 DML 语句可以带来如此显着的性能改进。</p>
</li>
</ol>
<p>Together this means that <code>L_w</code>, the latency cost of a single DML SQL statement, is equivalent to <code>L_c</code>. This is true even for the first writing statement which has the important role of creating the transaction record. With this substitution, we can then define <code>L_prep = W * L_c</code></p>
<p>这意味着单个 DML SQL 语句的延迟成本 L_w 等于 L_c。 即使对于第一个写入语句也是如此，它具有创建交易记录的重要作用。 通过这种替换，我们可以定义 L_prep &#x3D; W * L_c</p>
<p>Defining <code>L_commit</code> in terms of the unit latency <code>L_c</code> is a lot more straightforward. When the <code>COMMIT</code> statement is issued, the switch on the transaction’s record is flipped with a single round of distributed consensus. This means that <code>L_commit = L_c</code>.</p>
<p>根据单位延迟 L_c 定义 L_commit 要简单得多。 当发出 COMMIT 语句时，交易记录的开关将通过单轮分布式共识进行翻转。 这意味着L_commit &#x3D; L_c。</p>
<p>然后我们可以结合这两个组件来完成 2.1 之前的延迟模型：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">L_txn = (W + 1) * L_c</span><br></pre></td></tr></table></figure>

<p>We can read this as saying that a transaction pays the cost of distributed consensus once for every DML statement it executes plus once to commit. For instance, if our cluster can perform consensus in 7ms and our transaction performs 3 <code>UPDATE</code> statements, a back-of-the-envelope calculation for how long it should take gives us <code>28ms</code>.</p>
<p>我们可以将其理解为，事务为它执行的每个 DML 语句以及提交一次支付分布式共识的成本。 例如，如果我们的集群可以在 7 毫秒内达成共识，并且我们的事务执行 3 个 UPDATE 语句，则粗略计算所需时间为 28 毫秒。</p>
<h3 id="The-“1-Phase-Transaction”-Fast-Path"><a href="#The-“1-Phase-Transaction”-Fast-Path" class="headerlink" title="The “1-Phase Transaction” Fast-Path"></a>The “1-Phase Transaction” Fast-Path</h3><p>CockroachDB contains an important optimization in its transaction protocol that has existed since its inception called the “one-phase commit” fast-path. This optimization prevents a transaction that performs all writes on the same Range and commits immediately from needing a transaction record at all. This allows the transaction to complete with just a single round of consensus (<code>L_txn = 1*L_c</code>).</p>
<p>CockroachDB 在其事务协议中包含一项重要的优化，该协议自诞生以来就存在，称为“单阶段提交”快速路径。 这种优化可以防止在同一 Range 上执行所有写入并立即提交的事务根本不需要事务记录。 这使得交易只需一轮共识即可完成（L_txn &#x3D; 1*L_c）。</p>
<p>An important property of this optimization is that the transaction needs to commit immediately. This means that typically the fast-path is only accessible by implicit SQL transactions (i.e. single statements outside of a <code>BEGIN; ... COMMIT;</code> block). Because of this limitation, we’ll ignore this optimization for the remainder of this post and focus on explicit transactions.</p>
<p>这种优化的一个重要特性是事务需要立即提交。 这意味着快速路径通常只能通过隐式 SQL 事务（即 BEGIN; … COMMIT; 块之外的单个语句）访问。 由于此限制，我们将在本文的其余部分忽略此优化，并重点关注显式事务。</p>
<h2 id="Transactional-Pipelining"><a href="#Transactional-Pipelining" class="headerlink" title="Transactional Pipelining"></a>Transactional Pipelining</h2><p>The latency model we built reveals an interesting property of transactions in CockroachDB — their latency scales linearly with respect to the number of DML statements they contain. This behavior isn’t unreasonable, but its effects are clearly noticeable when measuring the performance of large transactions. Further, its effects are especially noticeable in geo-distributed clusters with very high replication latencies. This isn’t great for a database specializing in distributed operation.</p>
<p>我们构建的延迟模型揭示了 CockroachDB 中事务的一个有趣属性——它们的延迟与它们包含的 DML 语句的数量成线性关系。 这种行为并非不合理，但在衡量大型事务的性能时，其影响是显而易见的。 此外，它的影响在复制延迟非常高的地理分布式集群中尤其明显。 这对于专门从事分布式操作的数据库来说并不是很好。</p>
<h3 id="What-is-Transactional-Pipelining"><a href="#What-is-Transactional-Pipelining" class="headerlink" title="What is Transactional Pipelining?"></a>What is Transactional Pipelining?</h3><p>Transactional Pipelining is an extension to the CockroachDB transaction protocol which was introduced in v2.1 and aims to improve performance for distributed transactions. Its stated goal is to avoid the linear scaling of transaction latency with respect to DML statement count. At a high level, it achieves this by performing distributed consensus for intent writes across SQL statements concurrently. In doing so, it achieves its goal of reducing transaction latency to a constant multiple of consensus latency.</p>
<p>事务管道是 CockroachDB 事务协议的扩展，该协议在 v2.1 中引入，旨在提高分布式事务的性能。 其既定目标是避免事务延迟相对于 DML 语句计数的线性扩展。 在较高层面上，它通过同时跨 SQL 语句执行意图写入的分布式共识来实现这一点。 通过这样做，它实现了将交易延迟减少到共识延迟的恒定倍数的目标。</p>
<h3 id="Prior-Art-a-k-a-the-curse-of-SQL"><a href="#Prior-Art-a-k-a-the-curse-of-SQL" class="headerlink" title="Prior Art (a.k.a. the curse of SQL)"></a>Prior Art (a.k.a. the curse of SQL)</h3><p>Before taking a look at how transactional pipelining works, let’s quickly take a step back and explore how CockroachDB has attempted to address this problem in the past. CockroachDB first attempted to solve this issue in its v1.0 release through <a href="https://www.cockroachlabs.com/docs/v19.1/parallel-statement-execution">parallel statement execution</a>.</p>
<p>在了解事务管道如何工作之前，让我们快速退后一步，探索 CockroachDB 过去如何尝试解决这个问题。 CockroachDB 在 v1.0 版本中首先尝试通过并行语句执行来解决这个问题。</p>
<p>Parallel statement execution worked as advertised - it allowed clients to specify that they wanted statements in their SQL transaction to run in parallel. A client would do so by suffixing DML statements with the <code>RETURNING NOTHING</code> specifier. Upon the receipt of a statement with this specifier, CockroachDB would begin executing the statement in the background and would immediately return a fake return value to the client. Returning to the client immediately allowed parallel statement execution to get around the constraints of SQL’s conversational API within session transactions and enabled multiple statements to run in parallel.</p>
<p>并行语句执行的工作方式正如宣传的那样——它允许客户端指定他们希望 SQL 事务中的语句并行运行。 客户端可以通过在 DML 语句后添加 RETURNING NOTHING 说明符来实现此目的。 收到带有此说明符的语句后，CockroachDB 将开始在后台执行该语句，并立即向客户端返回一个假的返回值。 立即返回客户端允许并行语句执行，以绕过会话事务中 SQL 会话 API 的限制，并允许多个语句并行运行。</p>
<p>There were two major problems with this. First, clients had to change their SQL statements in order to take advantage of parallel statement execution. This seems minor, but it was a big issue for ORMs or other tools which abstract the SQL away from developers. Second, the fake return value was a lie. In the happy case where a parallel statement succeeded, the correct number of rows affected would be lost. In the unhappy case where a parallel statement failed, the error would be returned, <a href="https://www.cockroachlabs.com/docs/v19.1/parallel-statement-execution#error-message-mismatch">but only later in the transaction</a>. This was true whether the error was in the SQL domain, like a foreign key violation, or in the operational domain, like a failure to write to disk. Ultimately, parallel statement execution broke SQL semantics to allow statements to run in parallel.</p>
<p>这有两个主要问题。 首先，客户端必须更改其 SQL 语句才能利用并行语句执行。 这看起来很小，但对于 ORM 或其他将 SQL 从开发人员手中抽象出来的工具来说却是一个大问题。 其次，假的返回值是一个谎言。 在并行语句成功的情况下，正确的受影响行数将会丢失。 在并行语句失败的不幸情况下，将返回错误，但仅在事务的后期返回。 无论错误是在 SQL 域（如外键冲突）还是在操作域（如写入磁盘失败），情况都是如此。 最终，并行语句执行打破了 SQL 语义，允许语句并行运行。</p>
<p>We thought we could do better, which is why we started looking at the problem again from a new angle. We wanted to retain the benefits of parallel statement execution without breaking SQL semantics. This in turn would allow us to speed up all transactions, not just those that were written with parallel statement execution in mind.</p>
<p>我们认为我们可以做得更好，这就是为什么我们开始从新的角度重新审视这个问题。 我们希望保留并行语句执行的优势而不破坏 SQL 语义。 这反过来又将使我们能够加速所有事务，而不仅仅是那些考虑到并行语句执行而编写的事务。</p>
<h3 id="Buffering-Writes-Until-Commit"><a href="#Buffering-Writes-Until-Commit" class="headerlink" title="Buffering Writes Until Commit"></a>Buffering Writes Until Commit</h3><p>We understood from working with a number of other transaction systems that a valid alternative would be to buffer all write operations at the transaction coordinator node until the transaction was ready to commit. This would allow us to flush all write intents at once and pay the “preparation” cost of all writes, even across SQL statements, in parallel. This would also bring our distributed transaction protocol more closely in line with a traditional presumed abort 2-phase commit protocol.</p>
<p>通过与许多其他事务系统的合作，我们了解到，有效的替代方案是在事务协调器节点缓冲所有写入操作，直到事务准备好提交为止。 这将使我们能够一次性刷新所有写入意图，并并行支付所有写入的“准备”成本，甚至跨 SQL 语句。 这也将使我们的分布式事务协议更加符合传统的假定中止两阶段提交协议。</p>
<p>The idea was sound and we ended up creating a prototype that did just this. However, in the end we decided against the approach. In addition to the complication of buffering large amounts of data on transaction coordinator nodes and having to impose conservative transaction size limits to accommodate doing so, we realized that the approach would have a negative effect on transaction contention in CockroachDB.</p>
<p>这个想法很合理，我们最终创建了一个原型来实现这一点。 然而，最终我们决定不采用这种方法。 除了在事务协调器节点上缓冲大量数据以及必须施加保守的事务大小限制以适应这样做的复杂性之外，我们意识到该方法会对 CockroachDB 中的事务争用产生负面影响。</p>
<p>If you squint, write intents serve a similar role to row locks in a traditional SQL database. By “acquiring” these locks later into the lifecycle of a transaction and allowing reads from other transactions to create read-write conflicts in the interim period, we observed a large uptick in transaction aborts when running workloads like <a href="https://www.cockroachlabs.com/blog/cockroachdb-2dot1-performance/">TPC-C</a>. It turns out that performing all writes (i.e. acquiring all locks) at the end of a transaction works out with weaker isolation levels like snapshot isolation because such isolation levels allow a transaction’s read timestamp and its write timestamp to drift apart.</p>
<p>如果您仔细观察，就会发现写意图的作用与传统 SQL 数据库中的行锁类似。 通过稍后在事务的生命周期中“获取”这些锁，并允许从其他事务读取，从而在过渡期间产生读写冲突，我们观察到，在运行 TPC-C 等工作负载时，事务中止的情况大幅增加。 事实证明，在事务结束时执行所有写入（即获取所有锁）会使用较弱的隔离级别（例如快照隔离），因为此类隔离级别允许事务的读取时间戳和写入时间戳偏离。</p>
<p>However, at a serializable isolation level, a transaction must read and write at the same timestamp to prevent anomalies like <a href="https://www.cockroachlabs.com/blog/what-write-skew-looks-like/">write skew</a> from corrupting data. With this restriction, writing intents as early as possible serves an important role in CockroachDB of sequencing conflicting operations across transactions and avoiding the kinds of conflicts that result in transaction aborts. As such, doing so ends up being a large performance win even for workloads with just a small amount of contention.</p>
<p>然而，在可序列化隔离级别，事务必须在同一时间戳进行读取和写入，以防止写入倾斜等异常情况损坏数据。 有了这个限制，尽早写入意图在 CockroachDB 中发挥着重要作用，它可以对事务之间的冲突操作进行排序，并避免导致事务中止的冲突。 因此，即使对于只有少量争用的工作负载，这样做最终也会带来巨大的性能提升。</p>
<p>Creating significantly more transaction aborts would have been a serious issue, so we began looking for other ways that we could speed up transactions without acquiring all locks at commit time. We’ll soon see that transactional pipelining allows us to achieve these same latency properties while still eagerly acquiring locks and discovering contention points within a transaction long before they would cause a transaction to abort.</p>
<p>创建更多的事务中止将是一个严重的问题，因此我们开始寻找其他方法来加速事务，而无需在提交时获取所有锁。 我们很快就会看到，事务管道使我们能够实现这些相同的延迟属性，同时仍然急切地获取锁并在事务中早在导致事务中止之前就发现事务中的争用点。</p>
<h3 id="A-Key-Insight"><a href="#A-Key-Insight" class="headerlink" title="A Key Insight"></a>A Key Insight</h3><p>The breakthrough came when we realized that we could separate SQL errors from operational errors. We recognized that in order to satisfy the contract for SQL writes, we only need to synchronously perform SQL-domain constraint validation to determine whether a write should return an error, and if not, determine what the effect of the write should be (i.e. rows affected). Notably, we realized that we could begin writing intents immediately but don’t actually need to wait for them to finish before returning a result to the client. Instead, we just need to make sure that the write succeeds sometime before the transaction is allowed to commit.</p>
<p>当我们意识到我们可以将 SQL 错误与操作错误分开时，突破就出现了。 我们认识到，为了满足 SQL 写入的约定，我们只需要同步执行 SQL 域约束验证来确定写入是否应该返回错误，如果不是，则确定写入的效果应该是什么（即行） 做作的）。 值得注意的是，我们意识到我们可以立即开始编写意图，但实际上不需要等待它们完成后再将结果返回给客户端。 相反，我们只需要确保在允许提交事务之前的某个时间写入成功。</p>
<p>The interesting part about this is that a Range’s leaseholder has all the information necessary to perform constraint validation and determine the effect of a SQL write, and it can do this all without any coordination with other Replicas. The only time that it needs to coordinate with its peers is when replicating changes, and this doesn’t need to happen before we return to a client who issued a DML statement. Effectively, this means that we can push the entire consensus step out of the synchronous stage of statement execution. We can turn a write into a read and do all the hard work later. In doing so, we can perform the time-consuming operation of distributed consensus concurrently across all statements in a transaction!</p>
<p>有趣的是，Range 的租约持有者拥有执行约束验证和确定 SQL 写入效果所需的所有信息，并且它可以在不与其他副本进行任何协调的情况下完成这一切。 它需要与同级协调的唯一时间是复制更改时，并且在我们返回到发出 DML 语句的客户端之前不需要发生这种情况。 实际上，这意味着我们可以将整个共识步骤推出语句执行的同步阶段。 我们可以将写入转换为读取，然后再完成所有艰苦的工作。 这样做，我们可以在一个事务中的所有语句上并发执行分布式共识的耗时操作！</p>
<h3 id="Asynchronous-Consensus-异步共识"><a href="#Asynchronous-Consensus-异步共识" class="headerlink" title="Asynchronous Consensus 异步共识"></a>Asynchronous Consensus 异步共识</h3><p>In order to make this all fit together, we had to make a few changes to CockroachDB’s key-value API and client. The KV API was extended with the concept of “asynchronous consensus”. Traditionally, a KV operation like a <code>Put</code> would acquire latches on the corresponding Range’s leaseholder, determine the <code>Put</code>s effect by evaluating against the local state of the leaseholder (i.e. creating a new write intent), replicate this effect by proposing it through consensus, and wait until consensus succeeds before finally returning to the client.</p>
<p>为了使这一切结合在一起，我们必须对 CockroachDB 的键值 API 和客户端进行一些更改。 KV API 以“异步共识”的概念进行了扩展。 传统上，像“Put”这样的 KV 操作将获取相应 Range 租用者的锁存器，通过评估租用者的本地状态（即创建新的写入意图）来确定“Put”的效果，通过提议来复制此效果 通过共识，等待共识成功才最终返回给客户端。</p>
<p>Asynchronous consensus instructs KV operations to skip this last step and return immediately after proposing the change to Raft. Using this option, CockroachDB’s SQL layer can avoid waiting for consensus during each DML statement within a transaction––this means we no longer need to wait W *L_c during a transaction’s preparation phase.</p>
<p>异步共识指示 KV 操作跳过最后一步，并在向 Raft 提出更改后立即返回。 使用这个选项，CockroachDB的SQL层可以避免在事务中的每个DML语句期间等待共识——这意味着我们不再需要在事务的准备阶段等待W *L_c。</p>
<h3 id="Proving-Intent-Writes-证明意图写入"><a href="#Proving-Intent-Writes-证明意图写入" class="headerlink" title="Proving Intent Writes 证明意图写入"></a>Proving Intent Writes 证明意图写入</h3><p>The other half of the puzzle is that transactions now need to wait for all in-flight consensus writes to complete before committing a transaction. We call this job of waiting for an in-flight consensus write “proving” the intent. To prove an intent, the transaction client, which lives on the the SQL gateway node performing a SQL transaction, talks to the leaseholder of the Range which the intent lives on and checks whether it has been successfully replicated and persisted. If the in-flight consensus operation succeeded, the intent is successfully proven. If it failed, the intent is not proven and the transaction returns an error. If consensus operation is still in-flight, the client waits until it finishes.</p>
<p>难题的另一半是，事务现在需要等待所有正在进行的共识写入完成才能提交事务。 我们将等待飞行中达成共识的这项工作称为“证明”意图。 为了证明意图，位于执行 SQL 事务的 SQL 网关节点上的事务客户端与意图所在的 Range 的租用持有者进行通信，并检查它是否已成功复制和持久化。 如果正在进行的共识操作成功，则意图已成功得到证明。 如果失败，则意图无法得到证实，交易将返回错误。 如果共识操作仍在进行中，客户端将等待直到其完成。</p>
<p>To use this new mechanism, the transaction client was modified to track all unproven intents. It was then given the critical job of proving all intent writes before allowing a transaction to commit. The effect of this is that provisional writes in a transaction never wait for distributed consensus anymore. Instead, a transaction waits for all of its intents to be replicated through consensus in parallel, immediately before it commits. Once all intent writes succeed, the transaction can flip the switch on its transaction record from PENDING to COMMITTED.</p>
<p>为了使用这种新机制，交易客户端被修改为跟踪所有未经证实的意图。 然后，它的关键工作是在允许事务提交之前证明所有意图写入。 这样做的效果是，事务中的临时写入不再等待分布式共识。 相反，事务在提交之前立即等待其所有意图通过共识并行复制。 一旦所有意图写入成功，事务就可以将其事务记录上的开关从 PENDING 切换到 COMMITTED。</p>
<h3 id="Read-Your-Writes"><a href="#Read-Your-Writes" class="headerlink" title="Read-Your-Writes"></a>Read-Your-Writes</h3><p>There is an interesting edge case here. When a transaction writes a value, it should be able to read that same value later on as if it had already been committed. This property is sometimes called “read-your-writes”. CockroachDB’s transaction protocol has traditionally made this property trivial to enforce. Before asynchronous consensus, each DML statement in a transaction would synchronously result in intents that would necessarily be visible to all later statements in the transaction. Later statements would notice these intents when they went to perform operations on the same rows and would simply treat them as the source of truth since they were part of the same transaction.</p>
<p>这里有一个有趣的边缘情况。 当事务写入一个值时，它应该能够稍后读取相同的值，就像它已经被提交一样。 此属性有时称为“read-your-writes”。 传统上，CockroachDB 的事务协议使得该属性的执行变得微不足道。 在异步共识之前，事务中的每个 DML 语句都会同步产生意图，这些意图对于事务中的所有后续语句来说必然是可见的。 稍后的语句在对同一行执行操作时会注意到这些意图，并且会简单地将它们视为事实来源，因为它们是同一事务的一部分。</p>
<p>With asynchronous consensus, this guarantee isn’t quite as strong. Now that we’re responding to SQL statements before they have been replicated or persisted, it is possible for a later statement in a transaction to try to access the same data that an earlier statement modified, before the earlier statement’s consensus has resulted in an intent.</p>
<p>对于异步共识，这种保证并不那么有力。 现在，我们在 SQL 语句被复制或持久化之前对其进行响应，因此事务中的后续语句可能会在较早语句达成共识之前尝试访问较早语句修改的相同数据。 。</p>
<p>To prevent this from causing the client to miss its writes, we create a pipeline dependency between statements in a transaction that touch the same rows. Effectively, this means that the second statement will wait for the first to complete before running itself. In doing so, the second intent write first proves the success of the first intent write before starting asynchronous consensus itself. This results in what is known as a “pipeline stall”, because the pipeline within the transaction must slow down to prevent reordering and ensure that dependent statements see their predecessor’s results.</p>
<p>为了防止这导致客户端错过其写入，我们在接触相同行的事务中的语句之间创建管道依赖关系。 实际上，这意味着第二条语句将等待第一条语句完成后再运行。 这样做时，第二个意图写入首先证明第一个意图写入的成功，然后再启动异步共识本身。 这会导致所谓的“管道停顿”，因为事务中的管道必须减慢速度以防止重新排序并确保依赖语句看到其前一个语句的结果。</p>
<p>It is worth noting that the degenerate case where all statements depend on one-other and each results in a pipeline stall is exactly the case we had before - all statements are serialized with no intermediate concurrency.</p>
<p>值得注意的是，所有语句都相互依赖并且每个语句都会导致管道停顿的退化情况正是我们之前遇到的情况 - 所有语句都被序列化，没有中间并发性。</p>
<p>This mix of asynchronous consensus, proving intent writes, and the strong ordering enforced between dependent statements that touch the same rows combine to create transactional pipelining.</p>
<p>这种异步共识、证明意图写入以及接触相同行的依赖语句之间强制执行的强排序的组合结合起来创建了事务管道。</p>
<h3 id="Latency-Model-Revisited-延迟模型：重新审视"><a href="#Latency-Model-Revisited-延迟模型：重新审视" class="headerlink" title="Latency Model: Revisited 延迟模型：重新审视"></a>Latency Model: Revisited 延迟模型：重新审视</h3><p>Transactional pipelining dramatically changes our latency model. It affects both the preparation phase and the commit phase of a transaction and forces us to rederive <code>L_prep</code> and <code>L_commit</code>. To do so, we need to remember two things. First, with transactional pipelining, intent writes no longer synchronously pay the cost of distributed consensus. Second, before committing, a transaction must prove all of its intents before changing the status on its transaction record.</p>
<p>事务流水线极大地改变了我们的延迟模型。 它会影响事务的准备阶段和提交阶段，并迫使我们重新导出 L_prep 和 L_commit。 为此，我们需要记住两件事。 首先，通过事务管道，意图写入不再同步支付分布式共识的成本。 其次，在提交之前，事务必须在更改其事务记录上的状态之前证明其所有意图。</p>
<p>We hinted at the effect of this change on <code>L_prep</code> earlier - writing statements are now just as cheap as reading statements. This means that <code>L_prep</code> approaches 0 and the model simplifies to <code>L_txn = L_commit</code>.</p>
<p>我们之前暗示过这一变化对 L_prep 的影响 - 写入语句现在与读取语句一样便宜。 这意味着 L_prep 接近 0，模型简化为 L_txn &#x3D; L_commit。</p>
<p>However, <code>L_commit</code> is now more expensive because it has to do two things: prove all intents and write to its transaction record, and it must do these operations in order. The cost of the first step is of particular interest. The transaction client is able to prove all intents in parallel. The effect of this is that the latency cost of proving intent writes at the end of a transaction is simply the latency cost of the slowest intent write, or <code>L_c</code>. The latency cost of the second step, writing to the transaction’s record to flip its switch does not change.</p>
<p>然而，L_commit 现在更加昂贵，因为它必须做两件事：证明所有意图并写入其事务记录，并且它必须按顺序执行这些操作。 第一步的成本特别令人感兴趣。 交易客户端能够并行证明所有意图。 这样做的效果是，在事务结束时证明意图写入的延迟成本只是最慢意图写入的延迟成本，即 L_c。 第二步（写入交易记录以翻转其开关）的延迟成本不会改变。</p>
<p>By adding these together we arrive at our new transaction latency model:</p>
<p>通过将这些加在一起，我们得到了新的事务延迟模型：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">L_txn = L_commit = 2 * L_c</span><br></pre></td></tr></table></figure>

<p>We can read this as saying that a transaction whose writes cross multiple Ranges pays the cost of distributed consensus twice, regardless of the reads or the writes it performs. For instance, if our cluster can perform consensus in 7ms and our transaction performs 3 <code>UPDATE</code> statements, a back-of-the-envelope calculation for how long it should take gives us <code>14ms</code>. If we add a fourth <code>UPDATE</code> statement to the transaction, we don’t expect it to pay an additional consensus round trip - the estimated cost is constant regardless of what else the transaction does.</p>
<p>我们可以这样理解，一个跨多个 Range 进行写入的交易，无论执行的是读取还是写入操作，都会付出两次分布式共识的成本。 例如，如果我们的集群可以在 7 毫秒内达成共识，并且我们的事务执行 3 个 UPDATE 语句，则粗略计算所需时间为 14 毫秒。 如果我们向交易添加第四条 UPDATE 语句，我们不希望它支付额外的共识往返费用 - 无论交易执行其他操作，估计成本都是恒定的。</p>
<h2 id="Benchmark-Results"><a href="#Benchmark-Results" class="headerlink" title="Benchmark Results"></a>Benchmark Results</h2>]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>A Critique of ANSI SQL Isolation Levels</title>
    <url>/2023/10/27/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/0-A%20Critique%20of%20ANSI%20SQL%20Isolation%20Levels/</url>
    <content><![CDATA[<p>Abstract: ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Re- peatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to characterize several popular isolation levels, including the standard locking implementations of the levels. Investigating the ambiguities of the phenomena leads to clearer definitions; in addition new phenomena that better characterize isolation types are introduced. An important multiversion isolation type, Snapshot Isolation, is defined.</p>
<p>摘要：ANSI SQL-92 [MS, ANSI] 根据现象定义了隔离级别：脏读、不可重复读和幻像。 本文表明，这些现象和 ANSI SQL 定义无法表征几种流行的隔离级别，包括这些级别的标准锁定实现。 研究现象的模糊性可以得出更清晰的定义； 此外，还引入了更好地表征隔离类型的新现象。 定义了一种重要的多版本隔离类型：快照隔离。</p>
<span id="more"></span>

<h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p>Running concurrent transactions at different isolation lev- els allows application designers to trade throughput for correctness. Lower isolation levels increase transaction concurrency but risk showing transactions a fuzzy or incorrect database. Surprisingly, some transactions can execute at the highest isolation level (perfect serializability) while concurrent transactions running at a lower isolation level can access states that are not yet committed or that postdate states the transaction read earlier [GLPT]. Of course, transactions running at lower isolation levels may produce invalid data. Application designers must prevent later transactions running at higher isolation levels from accessing this invalid data and propa- gating errors.</p>
<p>在不同的隔离级别运行并发事务允许应用程序设计者以吞吐量换取正确性。 较低的隔离级别会增加事务并发性，但存在将事务显示为模糊或不正确的数据库的风险。 令人惊讶的是，某些事务可以在最高隔离级别（完美的可串行性）执行，而在较低隔离级别运行的并发事务可以访问尚未提交的状态或事务较早读取的后期状态[GLPT]。 当然，在较低隔离级别运行的事务可能会产生无效数据。 应用程序设计者必须防止以后在更高隔离级别运行的事务访问这些无效数据并传播错误。</p>
<p>The ANSI&#x2F;ISO SQL-92 specifications [MS, ANSI] define four isolation levels: (1) READ UNCOMMITTED, (2) READ COMMITTED, (3) REPEATABLE READ, (4) SERIALIZABLE. These levels are defined with the classi- cal serializability definition, plus three prohibited action subsequences, called phenomena: Dirty Read, Non-re- peatable Read, and Phantom. The concept of a phe- nomenon is not explicitly defined in the ANSI specifica- tions, but the specifications suggest that phenomena are action subsequences that may lead to anomalous (perhaps non-serializable) behavior. We refer to anomalies in what follows when suggesting additions to the set of ANSI phenomena. As shown later, there is a technical distinction between anomalies and phenomena, but this distinction is not crucial for a general understanding.</p>
<p>ANSI&#x2F;ISO SQL-92 规范 [MS、ANSI] 定义了四种隔离级别：(1) READ UNCOMMITTED、(2) READ COMMITTED、(3) REPEATABLE READ、(4) SERIALIZABLE。 这些级别是用经典的可串行性定义以及三个禁止的操作子序列（称为现象）来定义的：脏读、不可重复读和幻像。 ANSI 规范中没有明确定义现象的概念，但规范表明现象是可能导致异常（可能不可序列化）行为的动作子序列。 当建议添加 ANSI 现象集时，我们在下文中提到了异常。 如稍后所示，异常和现象之间存在技术区别，但这种区别对于一般理解并不重要。</p>
<p>The ANSI isolation levels are related to the behavior of lock schedulers. Some lock schedulers allow transactions to vary the scope and duration of their lock requests, thus departing from pure two-phase locking. This idea was in- troduced by [GLPT], which defined Degrees of Consistency in three ways: locking, data-flow graphs, and anomalies. Defining isolation levels by phenomena (anomalies) was intended to allow non-lock-based implementations of the SQL standard.</p>
<p>ANSI 隔离级别与锁调度程序的行为有关。 一些锁调度程序允许事务改变其锁请求的范围和持续时间，从而偏离纯粹的两阶段锁定。 这个想法是由 [GLPT] 引入的，它以三种方式定义了一致性程度：锁定、数据流图和异常。 按现象（异常）定义隔离级别的目的是允许 SQL 标准的非基于锁的实现。</p>
<p>This paper shows a number of weaknesses in the anomaly approach to defining isolation levels. The three ANSI phe- nomena are ambiguous. Even their broadest interpreta- tions do not exclude anomalous behavior. This leads to some counter-intuitive results. In particular, lock-based isolation levels have different characteristics than their ANSI equivalents. This is disconcerting because commercial database systems typically use locking. Additionally, the ANSI phenomena do not distinguish among several isolation levels popular in commercial systems.</p>
<p>本文展示了定义隔离级别的异常方法的许多弱点。 这三种 ANSI 现象是不明确的。 即使他们最广泛的解释也不排除异常行为。 这会导致一些反直觉的结果。 特别是，基于锁的隔离级别与其 ANSI 等效级别具有不同的特征。 这是令人不安的，因为商业数据库系统通常使用锁定。 此外，ANSI 现象不区分商业系统中流行的几种隔离级别。</p>
<p>Section 2 introduces basic isolation level terminology. It defines the ANSI SQL and locking isolation levels. Section 3 examines some drawbacks of the ANSI isolation levels and proposes a new phenomenon. Other popular isolation levels are also defined. The various definitions map between ANSI SQL isolation levels and the degrees of con- sistency defined in 1977 in [GLPT]. They also encompass Date’s definitions of Cursor Stability and Repeatable Read [DAT]. Discussing the isolation levels in a uniform frame- work reduces confusion.</p>
<p>第 2 节介绍基本隔离级别术语。 它定义了 ANSI SQL 和锁定隔离级别。 第 3 节研究了 ANSI 隔离级别的一些缺点并提出了一种新现象。 还定义了其他流行的隔离级别。 ANSI SQL 隔离级别和 1977 年 [GLPT] 中定义的一致性程度之间存在各种定义映射。 它们还包含 Date 的游标稳定性和可重复读取 [DAT] 的定义。 在统一框架中讨论隔离级别可以减少混乱。</p>
<p>Section 4 introduces a multiversion concurrency control mechanism, called Snapshot Isolation, that avoids the ANSI SQL phenomena, but is not serializable. Snapshot Isolation is interesting in its own right, since it provides a reduced-isolation level approach that lies between READ COMMITTED and REPEATABLE READ. A new formalism (available in the longer version of this paper [OOBBGM]) connects reduced isolation levels for multiversioned data to the classical single-version locking serializability theory.</p>
<p>第4节介绍了一种多版本并发控制机制，称为快照隔离，它避免了ANSI SQL现象，但不可序列化。 快照隔离本身就很有趣，因为它提供了一种介于 READ COMMITTED 和 REPEATABLE READ 之间的降低隔离级别的方法。 一种新的形式主义（在本文的较长版本 [OOBBGM] 中提供）将多版本数据的降低隔离级别与经典的单版本锁定可串行性理论连接起来。</p>
<p>Section 5 explores some new anomalies to differentiate the isolation levels introduced in Sections 3 and 4. The ex- tended ANSI SQL phenomena proposed here lack the power to characterize Snapshot isolation and Cursor Stability. Section 6 presents a Summary and Conclusions.</p>
<p>第 5 节探讨了一些新的异常现象，以区分第 3 节和第 4 节中引入的隔离级别。这里提出的扩展 ANSI SQL 现象缺乏描述快照隔离和游标稳定性的能力。 第 6 节介绍了总结和结论。</p>
<h2 id="2、Isolation-Definitions"><a href="#2、Isolation-Definitions" class="headerlink" title="2、Isolation Definitions"></a>2、Isolation Definitions</h2><h3 id="2-1、Serializability-Concepts"><a href="#2-1、Serializability-Concepts" class="headerlink" title="2.1、Serializability Concepts"></a>2.1、Serializability Concepts</h3><p>Transactional and locking concepts are well documented in the literature [BHG, PAP, PON, GR]. The next few paragraphs review the terminology used here.</p>
<p>事务和锁概念在文献 [BHG、PAP、PON、GR] 中有详细记录。 接下来的几段回顾了这里使用的术语。</p>
<p>A transaction groups a set of actions that transform the database from one consistent state to another. A history models the interleaved execution of a set of transactions as a linear ordering of their actions, such as Reads and Writes (i.e., inserts, updates, and deletes) of specific data items. Two actions in a history are said to conflict if they are performed by distinct transactions on the same data item and at least one of is a Write action. Following [EGLT], this definition takes a broad interpretation of “data item”: it could be a table row, a page, an entire table, or a message on a queue. Conflicting actions can also occur on a set of data items, covered by a predicate lock, as well as on a single data item.</p>
<p>事务将一组操作分组，这些操作将数据库从一种一致状态转换为另一种一致状态。 历史记录将一组事务的交错执行建模为其操作的线性顺序，例如特定数据项的读取和写入（即插入、更新和删除）。 如果历史记录中的两个操作是由不同事务对同一数据项执行的，并且其中至少一个是写入操作，则称这两个操作发生冲突。 遵循 [EGLT]，此定义对“数据项”进行广义解释：它可以是表行、页面、整个表或队列上的消息。 冲突的操作也可能发生在由谓词锁覆盖的一组数据项上以及单个数据项上。</p>
<p>A particular history gives rise to a dependency graph defining the temporal data flow among transactions. The actions of committed transactions in the history are repre- sented as graph nodes. If action op1 of transaction T1 conflicts with and precedes action op2 of transaction T2 in the history, then the pair &lt;op1, op2&gt; becomes an edge in the dependency graph. Two histories are equivalent if they have the same committed transactions and the same depen- dency graph. A history is serializable if it is equivalent to a serial history — that is, if it has the same dependency graph (inter-transaction temporal data flow) as some history that executes transactions one at a time in se- quence.</p>
<p>特定的历史记录会产生定义事务之间的时间数据流的依赖图。 历史记录中已提交事务的操作被表示为图节点。 如果事务 T1 的操作 op1 在历史中与事务 T2 的操作 op2 冲突且先于事务 T2 的操作 op2，则对 &lt;op1, op2&gt; 成为依赖图中的边。 如果两个历史具有相同的已提交事务和相同的依赖关系图，则它们是等效的。 如果一个历史记录相当于一个串行历史记录，那么它就是可序列化的——也就是说，如果它与某个按顺序一次执行一个事务的历史记录具有相同的依赖图（事务间时间数据流）。</p>
<h3 id="2-2、ANSI-SQL-Isolation-Levels"><a href="#2-2、ANSI-SQL-Isolation-Levels" class="headerlink" title="2.2、ANSI SQL Isolation Levels"></a>2.2、ANSI SQL Isolation Levels</h3><p>ANSI SQL Isolation designers sought a definition that would admit many different implementations, not just locking. They defined isolation with the following three phenomena:</p>
<p>ANSI SQL 隔离设计者寻求一种允许多种不同实现的定义，而不仅仅是锁。 他们用以下三种现象来定义隔离：</p>
<p>P1 (Dirty Read): Transaction T1 modifies a data item. Another transaction T2 then reads that data item before T1 performs a COMMIT or ROLLBACK. If T1 then performs a ROLLBACK, T2 has read a data item that was never committed and so never really existed.</p>
<p>P1（脏读）：事务T1修改了数据项。 然后，另一个事务 T2 在 T1 执行 COMMIT 或 ROLLBACK 之前读取该数据项。 如果 T1 随后执行 ROLLBACK，则 T2 已读取从未提交的数据项，因此从未真正存在过。</p>
<p>P2 (Non-repeatable or Fuzzy Read): Transaction T1 reads a data item. Another transaction T2 then modifies or deletes that data item and commits. If T1 then attempts to reread the data item, it receives a modified value or discovers that the data item has been deleted.</p>
<p>P2（不可重复或模糊读取）：事务T1读取数据项。 然后另一个事务 T2 修改或删除该数据项并提交。 如果 T1 随后尝试重新读取该数据项，它会收到修改后的值或发现该数据项已被删除。</p>
<p>P3 (Phantom): Transaction T1 reads a set of data items satisfying some [search condition]. Transaction T2 then creates data items that satisfy T1’s [search condition] and commits. If T1 then repeats its read with the same [search condition], it gets a set of data items different from the first read.</p>
<p>P3（幻影）：事务 T1 读取满足某些&lt;搜索条件&gt;的一组数据项。 然后，事务 T2 创建满足 T1 的&lt;搜索条件&gt;的数据项并提交。 如果 T1 然后使用相同的&lt;搜索条件&gt;重复读取，它会得到一组与第一次读取不同的数据项。</p>
<p>None of these phenomena could occur in a serial history. Therefore by the Serializability Theorem they cannot occur in a serializable history [EGLT, BHG Theorem 3.6, GR Section 7.5.8.2, PON Theorem 9.4.2].</p>
<p>这些现象都不可能在连续的历史中发生。 因此，根据可串行性定理，它们不能出现在可串行化的历史中 [EGLT、BHG 定理 3.6、GR 第 7.5.8.2 节、PON 定理 9.4.2]。</p>
<p>Histories consisting of reads, writes, commits, and aborts can be written in a shorthand notation: “w1[x]” means a write by transaction 1 on data item x (which is how a data item is “modified’), and “r2[x]” represents a read of x by transaction 2. Transaction 1 reading and writing a set of records satisfying predicate P is denoted by r1[P] and w1[P] respectively. Transaction 1’s commit and abort (ROLLBACK) are written “c1” and “a1”, respectively.</p>
<p>由读取、写入、提交和中止组成的历史记录可以用速记符号来编写：“w1[x]”表示事务 1 对数据项 x 的写入（这就是“修改”数据项的方式），并且“ r2[x]”表示事务 2 对 x 的读取。事务 1 读取和写入满足谓词 P 的一组记录分别由 r1[P] 和 w1[P] 表示。 事务 1 的提交和中止（ROLLBACK）分别写为“c1”和“a1”。</p>
<p>Phenomenon P1 might be restated as disallowing the fol- lowing scenario:</p>
<p>现象 P1 可以重述为不允许出现以下情况：</p>
<p>(2.1) w1[x] . . . r2[x] . . . (a1 and c2 in either order)</p>
<p>The English statement of P1 is ambiguous. It does not actually insist that T1 abort; it simply states that if this happens something unfortunate might occur. Some people reading P1 interpret it to mean:</p>
<p>P1的英文表述有歧义。 它实际上并不坚持 T1 中止；而是要求 T1 中止。 它只是指出，如果发生这种情况，可能会发生一些不幸的事情。 一些阅读 P1 的人将其解释为：</p>
<p>(2.2) w1[x]…r2[x]…((c1 or a1) and (c2 or a2) in any order)</p>
<p>Forbidding the (2.2) variant of P1 disallows any history where T1 modifies a data item x, then T2 reads the data item before T1 commits or aborts. It does not insist that T1 aborts or that T2 commits.</p>
<p>禁止 P1 的 (2.2) 变体不允许 T1 修改数据项 x，然后 T2 在 T1 提交或中止之前读取该数据项的任何历史记录。 它并不坚持 T1 中止或 T2 提交。</p>
<p>Definition (2.2) is a much broader interpretation of P1 than (2.1), since it prohibits all four possible commit-abort pairs by transactions T1 and T2, while (2.1) only prohibits two of the four. Interpreting (2.2) as the meaning of P1 prohibits an execution sequence if something anomalous might in the future. We call (2.2) the broad interpretation of P1, and (2.1) the strict interpretation of P1. Interpretation (2.2) specifies a phenomenon that might lead to an anomaly, while (2.1) specifies an actual anomaly. Denote them as P1 and A1 respectively. Thus:</p>
<p>定义（2.2）是比（2.1）更广泛的对 P1 的解释，因为它禁止事务 T1 和 T2 的所有四个可能的提交-中止对，而（2.1）仅禁止四个中的两个。 将 (2.2) 解释为 P1 的含义会禁止执行序列（如果将来可能出现异常情况）。 我们称（2.2）P1 为广义解释，（2.1）P1 为严格解释。 解释（2.2）指定了可能导致异常的现象，而（2.1）指定了实际的异常。 分别将它们表示为P1和A1。 因此：</p>
<p>P1: w1[x]…r2[x]…((c1 or a1) and (c2 or a2) in any order) </p>
<p>A1: w1[x]…r2[x]…(a1 and c2 in any order)</p>
<p>Similarly, the English language phenomena P2 and P3 have strict and broad interpretations, and are denoted P2 and P3 for broad, and A2 and A3 for strict:</p>
<p>同样，英语语言现象 P2 和 P3 也有严格解释和广义解释，广义的用 P2 和 P3 表示，严格的用 A2 和 A3 表示：</p>
<p>P2: r1[x]…w2[x]…((c1 or a1) and (c2 or a2) in any order)</p>
<p>A2: r1[x]…w2[x]…c2…r1[x]…c1</p>
<p>P3: r1[P]…w2[y in P]…((c1 or a1) and (c2 or a2) any order) </p>
<p>A3: r1[P]…w2[y in P]…c2…r1[P]…c1</p>
<p>Section 3 analyzes these alternative interpretations after more conceptual machinery has been developed. It argues that the broad interpretation of the phenomena is required. Note that the English statement of ANSI SQL P3 just prohibits inserts to a predicate, but P3 above intentionally prohibits any write (insert, update, delete) affecting a tuple satisfying the predicate once the predicate has been read.</p>
<p>在更多的概念机制被开发出来后，第 3 节分析了这些替代解释。 它认为需要对这些现象进行广泛的解释。 请注意，ANSI SQL P3 的英文语句只是禁止对谓词进行插入，但上面的 P3 有意禁止在读取谓词后影响满足谓词的元组的任何写入（插入、更新、删除）。</p>
<p>This paper later deals with the concept of a multi-valued history (MV-history for short — see [BHG], Chapter 5). Without going into details now, multiple versions of a data item may exist at one time in a multi-version system. Any read must be explicit about which version is being read. There have been attempts to relate ANSI Isolation defini- tions to multi-version systems as well as more common single-version systems of a standard locking scheduler. The English language statements of the phenomena P1, P2, and P3 imply single-version histories. This is how we interpret them in the next section.</p>
<p>本文稍后讨论多值历史的概念（简称 MV 历史 — 参见 [BHG]，第 5 章）。 现在不详述，在多版本系统中，一个数据项可能同时存在多个版本。 任何读取都必须明确正在读取哪个版本。 人们曾尝试将 ANSI 隔离定义与多版本系统以及标准锁定调度程序的更常见的单版本系统相关联。 现象 P1、P2 和 P3 的英语陈述意味着单一版本的历史。 这就是我们在下一节中解释它们的方式。</p>
<p>ANSI SQL defines four levels of isolation by the matrix of Table 1. Each isolation level is characterized by the phe- nomena that a transaction is forbidden to experience (broad or strict interpretations). However, the ANSI SQL specifications do not define the SERIALIZABLE isolation level solely in terms of these phenomena. Subclause 4.28, “SQL-transactions”, in [ANSI] notes that the SERIALIZABLE isolation level must provide what is “commonly known as fully serializable execution.” The prominence of the table compared to this extra proviso leads to a common misconception that disallowing the three phenomena implies serializability. Table 1 calls histories that disallow the three phenomena ANOMALY SERIALIZABLE.</p>
<p>ANSI SQL 通过表 1 的矩阵定义了四个隔离级别。每个隔离级别的特征都是禁止事务经历的现象（广义或严格解释）。 然而，ANSI SQL 规范并没有仅仅根据这些现象来定义 SERIALIZABLE 隔离级别。 [ANSI] 中的第 4.28 节“SQL 事务”指出，SERIALIZABLE 隔离级别必须提供“通常称为完全可序列化执行”的内容。 与这个额外的附加条件相比，该表的重要性导致了一种常见的误解，即不允许这三种现象意味着可串行化。 表 1 调用了不允许三种现象异常可串行化的历史记录。</p>
<p><strong>Table 1. ANSI SQL Isolation Levels Defined in terms of the Three Original Phenomena</strong></p>
<table>
<thead>
<tr>
<th>Isolation Level</th>
<th>P1 (or A1) Dirty Read</th>
<th>P2 (or A2) Fuzzy Read</th>
<th>P3 (or A3) Phantom</th>
</tr>
</thead>
<tbody><tr>
<td>ANSI READ UNCOMMITTED</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>ANSI READ COMMITTED</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>ANSI REPEATABLE READ</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>ANOMALY SERIALIZABLE</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
</tr>
</tbody></table>
<p>The isolation levels are defined by the phenomena they are forbidden to experience. Picking a broad interpretation of a phenomenon excludes a larger set of histories than the strict interpretation. This means we are arguing for more restrictive isolation levels (more histories will be disal- lowed). Section 3 shows that even taking the broad interpretations of P1, P2, and P3, forbidding these phenomena does not guarantee true serializability. It would have been simpler in [ANSI] to drop P3 and just use Subclause 4.28 to define ANSI SERIALIZABLE. Note that Table 1 is not a final result; Table 3 will superseded it.</p>
<p>隔离级别是根据他们被禁止经历的现象来定义的。 对一种现象选择广义的解释比严格的解释排除了更多的历史。 这意味着我们主张更严格的隔离级别（将不允许更多历史记录）。 第 3 节表明，即使采用 P1、P2 和 P3 的广义解释，禁止这些现象也不能保证真正的可串行性。 在 [ANSI] 中删除 P3 并仅使用子条款 4.28 来定义 ANSI SERIALIZABLE 会更简单。 请注意，表 1 不是最终结果； 表 3 将取代它。</p>
<h3 id="2-3、Locking"><a href="#2-3、Locking" class="headerlink" title="2.3、Locking"></a>2.3、Locking</h3><p>Most SQL products use lock-based isolation. Consequently, it is useful to characterize the ANSI SQL isolation levels in terms of locking, although certain problems arise.</p>
<p>大多数 SQL 产品都使用基于锁的隔离。 因此，尽管会出现某些问题，但根据锁来表征 ANSI SQL 隔离级别还是很有用的。</p>
<p>Transactions executing under a locking scheduler request Read (Share) and Write (Exclusive) locks on data items or sets of data items they read and write. Two locks by differ- ent transactions on the same item conflict if at least one of them is a Write lock.</p>
<p>在带锁调度程序下执行的事务，对读取和写入的数据或数据集进行读取（共享）和写入（独占）锁定。 如果不同事务对同一项目的两个锁中至少有一个是写锁，则它们会发生冲突。</p>
<p>A Read (resp. Write) predicate lock on a given [search condition] is effectively a lock on all data items satisfying the [search condition]. This may be an infinite set. It includes data present in the database and also any phantom data items not currently in the database but that would satisfy the predicate if they were inserted or if current data items were updated to satisfy the [search condition]. In SQL terms, a predicate lock covers all tuples that satisfy the predicate and any that an INSERT, UPDATE, or DELETE statement would cause to satisfy the predicate. Two predicate locks by different transactions conflict if one is a Write lock and if there is a (possibly phantom) data item covered by both locks. An item lock (record lock) is a predicate lock where the predicate names the specific record.</p>
<p>给定&lt;搜索条件&gt;上的读（或写）谓词锁实际上是满足&lt;搜索条件&gt;的所有数据项上的锁。 这可能是一个无限集。 它包括数据库中存在的数据以及当前不在数据库中但如果插入它们或更新当前数据项以满足&lt;搜索条件&gt;则将满足谓词的任何虚拟数据项。 在 SQL 术语中，谓词锁涵盖满足谓词的所有元组以及 INSERT、UPDATE 或 DELETE 语句将导致满足谓词的任何元组。 如果一个是写锁并且两个锁都覆盖了一个（可能是幻影）数据项，则不同事务的两个谓词锁会发生冲突。 项锁（记录锁）是谓词锁，其中谓词命名特定记录。</p>
<p>A transaction has well-formed writes (reads) if it requests a Write (Read) lock on each data item or predicate before writing (reading) that data item, or set of data items defined by a predicate. The transaction is well-formed if it has well-formed writes and reads. A transaction has two- phase writes (reads) if it does not set a new Write (Read) lock on a data item after releasing a Write (Read) lock. A transaction exhibits two-phase locking if it does not request any new locks after releasing some lock.</p>
<p>如果事务在写入（读取）每个数据项或谓词定义的数据项集之前请求对每个数据项或谓词进行写入（读取）锁定，则该事务具有格式正确的写入（读取）。 <font color="red">如果事务具有格式良好的写入和读取，则该事务是格式良好的。</font> 如果事务在释放写（读）锁后没有对数据项设置新的写（读）锁，则该事务具有两阶段写（读）。<font color="red"> 如果事务在释放某些锁后不再请求任何新锁，则该事务表现出两阶段锁定。</font></p>
<p>The locks requested by a transaction are of long duration if they are held until after the transaction commits or aborts. Otherwise, they are of short duration. Typically, short locks are released immediately after the action completes.</p>
<p>如果事务请求的锁一直保持到事务提交或中止之后，则它们的持续时间很长。 否则，它们的持续时间很短。 通常，短锁会在操作完成后立即释放。</p>
<p>If a transaction holds a lock, and another transaction requests a conflicting lock, then the new lock request is not granted until the former transaction’s conflicting lock has been released.</p>
<p>如果一个事务持有锁，而另一个事务请求冲突锁，则在前一个事务的冲突锁被释放之前，新的锁请求不会被授予。</p>
<p>The fundamental serialization theorem is that well-formed two-phase locking guarantees serializability — each his- tory arising under two-phase locking is equivalent to some serial history. Conversely, if a transaction is not well- formed or two-phased then, except in degenerate cases, non-serializable execution histories are possible [EGLT].</p>
<p>基本的序列化定理是，格式良好的两阶段锁定保证了可序列化性——两阶段锁定下产生的每个历史都相当于一些串行历史。 相反，如果事务不是格式良好的或两阶段的，那么除了退化情况之外，不可序列化的执行历史是可能的[EGLT]。</p>
<p>The [GLPT] paper defined four degrees of consistency, at- tempting to show the equivalence of locking, dependency, and anomaly-based characterizations. The anomaly defini- tions (see Definition 1) were too vague. The authors con- tinue to get criticism for that aspect of the definitions [GR]. Only the more mathematical definitions in terms of histories and dependency graphs or locking have stood the test of time.</p>
<p>[GLPT]论文定义了四个一致性程度，试图证明锁定、依赖和基于异常的特征的等效性。 异常定义（参见定义 1）过于模糊。 作者继续因定义的这方面而受到批评[GR]。 只有历史和依赖图或锁定方面更数学的定义才能经受住时间的考验。</p>
<p>Table 2 defines a number of isolation types in terms of lock scopes (items or predicates), modes (read or write), and their durations (short or long). We believe the isolation levels called Locking READ UNCOMMITTED, Locking READ COMMITTED, Locking REPEATABLE READ, and Locking SERIALIZABLE are the locking definitions in- tended by ANSI SQL Isolation levels — but as shown next they are quite different from those of Table 1. Consequently, it is necessary to differentiate isolation lev- els defined in terms of locks from the ANSI SQL phenom- ena-based isolation levels. To make this distinction, the levels in Table 2 are labeled with the “Locking” prefix, as opposed to the “ANSI” prefix of Table 1.</p>
<p><font color="red">表 2 根据锁范围（项或谓词）、模式（读或写）及其持续时间（短或长）定义了许多隔离类型。</font> 我们相信称为“Locking READ UNCOMMITTED”、“Locking READ COMMITTED”、“Locking REPEATABLE READ”和“Locking SERIALIZABLE”的隔离级别是 ANSI SQL 隔离级别预期的锁定定义，但如下所示，它们与表 1 中的隔离级别有很大不同。因此， 有必要区分以锁定义的隔离级别和基于 ANSI SQL 现象的隔离级别。 为了进行这种区分，表 2 中的级别标有“Locking”前缀，而不是表 1 中的“ANSI”前缀。</p>
<p><strong>Table 2. Degrees of Consistency and Locking Isolation Levels defined in terms of locks.</strong></p>
<table>
<thead>
<tr>
<th>Consistency<br/> Level &#x3D; Locking Isolation Level</th>
<th>Read Locks on<br/> Data Items and Predicates (the same unless noted)</th>
<th>Write Locks on<br/> Data Items and Predicates (always the same)</th>
</tr>
</thead>
<tbody><tr>
<td>Degree 0</td>
<td>none required</td>
<td>Well-formed Writes</td>
</tr>
<tr>
<td>Degree 1 &#x3D; Locking<br>READ UNCOMMITTED</td>
<td>none required</td>
<td>Well-formed Writes<br/> Long duration Write locks</td>
</tr>
<tr>
<td>Degree 2 &#x3D; Locking<br>READ COMMITTED</td>
<td>Well-formed Reads<br/> Short duration Read locks (both)</td>
<td>Well-formed Writes,<br/> Long duration Write locks</td>
</tr>
<tr>
<td>Cursor Stability<br>(see Section 4.1)</td>
<td>Well-formed Reads<br/> Read locks held on current of cursor Short duration Read Predicate locks</td>
<td>Well-formed Writes,<br/> Long duration Write locks</td>
</tr>
<tr>
<td>Locking<br>REPEATABLE READ</td>
<td>Well-formed Reads<br/> Long duration data-item Read locks Short duration Read Predicate locks</td>
<td>Well-formed Writes, Long duration Write locks</td>
</tr>
<tr>
<td>Degree 3 &#x3D; Locking<br>SERIALIZABLE</td>
<td>Well-formed Reads<br/> Long duration Read locks (both)</td>
<td>Well-formed Writes, Long duration Write locks</td>
</tr>
</tbody></table>
<p>[GLPT] defined Degree 0 consistency to allow both dirty reads and writes: it only required action atomicity.Degrees 1, 2, and 3 correspond to Locking READ UNCOMMITTED, READ COMMITTED, and SERIALIZABLE,respectively. No isolation degree matches the Locking REPEATABLE READ isolation level.</p>
<p>[GLPT]定义了Degree 0 一致性以允许脏读和脏写：它只需要操作原子性。Degree 1,2和3分别对应于锁定READ UNCOMMITTED、READ COMMITTED和SERIALIZABLE。 没有隔离级别与 Locking REPEATABLE READ 隔离级别匹配。</p>
<p>Date and IBM originally used the name “Repeatable Reads” [DAT, DB2] to mean serializable or Locking SERIALIZABLE. This seemed like a more comprehensible name than the [GLPT] term “Degree 3 isolation.” The ANSI SQL meaning of REPEATABLE READ is different from Date’s original definition, and we feel the ter- minology is unfortunate. Since anomaly P3 is specifically not ruled out by the ANSI SQL REPEATABLE READ isolation level, it is clear from the definition of P3 that reads are NOT repeatable! We repeat this misuse of the term with Locking REPEATABLE READ in Table 2, in order to parallel the ANSI definition. Similarly, Date coined the term Cursor Stability as a more comprehensible name for Degree 2 isolation augmented with protection from lost cursor updates as explained in Section 4.1 below.</p>
<p>Date 和 IBM 最初使用名称“可重复读取”[DAT、DB2] 来表示可序列化或锁定可串行化。 这似乎是一个比 [GLPT] 术语“Degree 3 隔离”更容易理解的名称。REPEATABLE READ 的 ANSI SQL 含义与 Date 的原始定义不同，我们觉得这个术语很不幸。由于异常 P3 没有被明确规定 由于 ANSI SQL REPEATABLE READ 隔离级别的限制，从 P3 的定义可以清楚地看出读取是不可重复的！我们在表 2 中使用 Locking REPEATABLE READ 重复这个术语的误用，以便与 ANSI 定义并行。同样，<font color="red">Date 创造了术语“游标稳定性”，作为 Degree 2隔离的更容易理解的名称，并增强了对丢失游标更新的保护</font>，如下面第 4.1 节所述。</p>
<p>Definition. Isolation level L1 is weaker than isolation level L2 (or L2 is stronger than L1), denoted L1 « L2, if all nonserializable histories that obey the criteria of L2 also satisfy L1 and there is at least one non-serializable history that can occur at level L1 but not at level L2. Two isolation levels L1 and L2 are equivalent, denoted L1 &#x3D;&#x3D; L2, if the sets of non-serializable histories satisfying L1 and L2 are identical. L1 is no stronger than L2, denoted L1 «&#x3D; L2 if either L1 « L2 or L1 &#x3D;&#x3D; L2. Two isolation levels are incom- parable, denoted L1 »« L2, when each isolation level allows a non-serializable history that is disallowed by the other.</p>
<p>定义。 隔离级别 L1 弱于隔离级别 L2（或 L2 强于 L1），表示为 L1 «  L2，如果遵守 L2 标准的所有不可序列化历史也满足 L1，并且至少有一个不可序列化历史可以发生在 级别 L1，但不级别 L2。 如果满足 L1 和 L2 的不可串行化历史集相同，则两个隔离级别 L1 和 L2 是等效的，表示为 L1 &#x3D;&#x3D; L2。 L1 不强于 L2，如果 L1 « L2 或 L1 &#x3D;&#x3D; L2，则表示为 L1 « &#x3D; L2。 当每个隔离级别都允许另一个隔离级别不允许的不可序列化历史记录时，两个隔离级别是不可比较的，表示为 L1 »«L2。</p>
<p>In comparing isolation levels we differentiate them only in terms of the non-serializable histories that can occur in one but not the other. Two isolation levels can also differ in terms of the serializable histories they permit, but we say Locking SERIALIZABLE &#x3D;&#x3D; Serializable even though it is well known that a locking scheduler does not admit all possible Serializable histories. It is possible for an isolation level to be impractical because of disallowing too many serializable histories, but we do not deal with this here.</p>
<p>在比较隔离级别时，我们仅根据可能发生在一个而不是另一个中的不可序列化历史来区分它们。 两个隔离级别在它们允许的可序列化历史方面也可能有所不同，但我们说锁定可序列化&#x3D;&#x3D;可序列化，即使众所周知锁定调度程序不承认所有可能的可序列化历史。 由于不允许太多可序列化的历史记录，隔离级别可能不切实际，但我们在这里不处理这个问题。</p>
<p>These definitions yield the following remark. 这些定义产生以下评论。</p>
<p><strong>Remark</strong> 1: Locking READ UNCOMMITTED </p>
<p>​					« Locking READ COMMITTED</p>
<p>​						« Locking REPEATABLE READ </p>
<p>​							« Locking SERIALIZABLE</p>
<p>In the following section, we’ll focus on comparing the ANSI and Locking definitions.</p>
<p>在下一节中，我们将重点比较 ANSI 和锁定定义。</p>
<h2 id="3、Analyzing-ANSI-SQL-Isolation-Levels"><a href="#3、Analyzing-ANSI-SQL-Isolation-Levels" class="headerlink" title="3、Analyzing ANSI SQL Isolation Levels"></a>3、Analyzing ANSI SQL Isolation Levels</h2><p>To start on a positive note, the locking isolation levels comply with the ANSI SQL requirements.</p>
<p>从积极的方面来说，锁定隔离级别符合 ANSI SQL 要求。</p>
<p><strong>Remark</strong> 2. The locking protocols of Table 2 define lock- ing isolation levels that are at least as strong as the corre- sponding phenomena-based isolation levels of Table 1. See [OOBBGM] for proof.</p>
<p>备注 2. 表 2 的锁定协议定义的锁定隔离级别至少与表 1 相应的基于现象的隔离级别一样强。有关证明，请参阅 [OOBBGM]。</p>
<p>Hence, locking isolation levels are at least as isolated as the same-named ANSI levels. Are they more isolated? The answer is yes, even at the lowest level. Locking READ UNCOMMITTED provides long duration write locking to avoid a phenomenon called “Dirty Writes,” but ANSI SQL does not exclude this anomalous behavior other than ANSI SERIALIZABLE. Dirty writes are defined as follows:</p>
<p>因此，锁定隔离级别至少与同名的 ANSI 级别一样隔离。 他们更孤立吗？ 答案是肯定的，即使是在最低级别。 锁定 READ UNCOMMITTED 提供长时间写锁定以避免称为“脏写”的现象，但 ANSI SQL 不排除除 ANSI SERIALIZABLE 之外的这种异常行为。 脏写定义如下：</p>
<p>P0 (Dirty Write): Transaction T1 modifies a data item. Another transaction T2 then further modifies that data item before T1 performs a COMMIT or ROLLBACK. If T1 or T2 then performs a ROLLBACK, it is unclear what the correct data value should be. The broad interpretation of this is:</p>
<p>P0（脏写）：事务T1修改数据项。 然后，另一个事务 T2 在 T1 执行 COMMIT 或 ROLLBACK 之前进一步修改该数据项。 如果T1或T2随后执行ROLLBACK，则不清楚正确的数据值应该是什么。 对此的广义解释是：</p>
<p>P0: w1[x]…w2[x]…((c1 or a1) and (c2 or a2) in any order)</p>
<p>One reason why Dirty Writes are bad is that they can vio- late database consistency. Assume there is a constraint be- tween x and y (e.g., x &#x3D; y), and T1 and T2 each maintain the consistency of the constraint if run alone. However, the constraint can easily be violated if the two transactions write x and y in different orders, which can only happen if there are Dirty writes. For example consider the history w1[x] w2[x] w2[y] c2 w1[y] c1. T1’s changes to y and T2’s to x both “survive”. If T1 writes 1 in both x and y while T2 writes 2, the result will be x&#x3D;2, y &#x3D;1 violating x &#x3D; y.</p>
<p>脏写不好的原因之一是它们会破坏数据库的一致性。 假设 x 和 y 之间存在约束（例如，x &#x3D; y），并且如果单独运行，T1 和 T2 都保持约束的一致性。 然而，如果两个事务以不同的顺序写入 x 和 y，则很容易违反该约束，而这种情况只有在存在脏写入时才会发生。 例如，考虑历史 w1[x] w2[x] w2[y] c2 w1[y] c1。 T1 对 y 的更改和 T2 对 x 的更改都“幸存”。 如果 T1 在 x 和 y 中都写入 1，而 T2 写入 2，则结果将是 x&#x3D;2，y &#x3D;1，违反 x &#x3D; y。</p>
<p>As discussed in [GLPT, BHG] and elsewhere, automatic transaction rollback is another pressing reason why P0 is important. Without protection from P0, the system can’t undo updates by restoring before images. Consider the his- tory: w1[x] w2[x] a1. You don’t want to undo w1[x] by restoring its before-image of x, because that would wipe outw2’supdate. Butifyoudon’trestoreitsbefore-image, and transaction T2 later aborts, you can’t undo w2[x] by restoring its before-image either! Even the weakest locking systems hold long duration write locks. Otherwise, their recovery systems would fail. So we conclude Remark 3: Remark 3: ANSI SQL isolation should be modified to re- quire P0 for all isolation levels.</p>
<p>正如 [GLPT、BHG] 和其他地方所讨论的，自动事务回滚是 P0 重要的另一个紧迫原因。 如果没有 P0 的保护，系统无法通过恢复之前的映像来撤消更新。 考虑历史：w1[x] w2[x] a1。 你不想通过恢复 x 的前映像来撤消 w1[x]，因为这会擦除 w2 的更新。 但如果你不恢复它的前像，并且事务 T2 后来中止，你也不能通过恢复它的前像来撤消 w2[x]！ 即使是最弱的锁定系统也会持有长时间的写锁。 否则，他们的恢复系统就会失败。 因此，我们总结备注 3： 备注 3：ANSI SQL 隔离应修改为所有隔离级别都要求 P0。</p>
<p>We now argue that a broad interpretation of the three ANSI phenomena is required. Recall the strict interpreta- tions are:</p>
<p>我们现在认为需要对这三种 ANSI 现象进行广泛的解释。 回想一下严格的解释是：</p>
<p>A1: w1[x]…r2[x]…(a1 and c2 in either order)    (Dirty Read)</p>
<p>A2:  r1[x]…w2[x]…c2…r1[x]…c1   (Fuzzy or Non-Repeatable Read)</p>
<p>A3:  r1[P]…w2[y in P]…c2….r1[P]…c1    (Phantom)</p>
<p>By Table 1, histories under READ COMMITTED isolation forbid anomaly A1, REPEATABLE READ isolation for- bids anomalies A1 and A2, and SERIALIZABLE isolation forbids anomalies A1, A2, and A3. Consider history H1, involving a $40 transfer between bank balance rows x and y:</p>
<p>根据表 1，READ COMMITTED 隔离下的历史禁止异常 A1，REPEATABLE READ 隔离禁止异常 A1 和 A2，SERIALIZABLE 隔离禁止异常 A1、A2 和 A3。 考虑历史 H1，涉及银行余额行 x 和 y 之间的 40 美元转账：</p>
<p>H1: r1[x&#x3D;50]w1[x&#x3D;10]r2[x&#x3D;10]r2[y&#x3D;50]c2 r1[y&#x3D;50]w1[y&#x3D;90]c1</p>
<p>H1 is non-serializable, the classical inconsistent analysis problem where transaction T1 is transferring a quantity 40 from x to y, maintaining a total balance of 100, but T2 reads an inconsistent state where the total balance is 60. The history H1 does not violate any of the anomalies A1, A2, or A3. In the case of A1, one of the two transactions would have to abort; for A2, a data item would have to be read by the same transaction for a second time; A3 re- quires a phantom value. None of these things happen in H1. Consider instead taking the broad interpretation of A1, the phenomenon P1:</p>
<p>H1 是不可序列化的，这是经典的不一致分析问题，其中事务 T1 将数量 40 从 x 转移到 y，保持总余额为 100，但 T2 读取总余额为 60 的不一致状态。历史记录 H1 不违反 任何异常 A1、A2 或 A3。 在 A1 的情况下，两个事务之一必须中止； 对于A2，数据项必须由同一事务第二次读取； A3 需要一个幻像值。 这些事情在上半年都没有发生。 考虑对 A1（即现象 P1）进行广义解释：</p>
<p>P1: w1[x]…r2[x]…((c1 or a1) and (c2 or a2) in any order)</p>
<p>H1 indeed violates P1. Thus, we should take the interpre- tation P1 for what was intended by ANSI rather than A1. The Broad interpretation is the correct one.</p>
<p>H1确实违反了P1。 因此，我们应该采用 P1 来解释 ANSI 的意图，而不是 A1。 广义的解释是正确的。</p>
<p>Similar arguments show that P2 should be taken as the ANSI intention rather than A2. A history that discrimi- nates these two interpretations is:</p>
<p>类似的论点表明P2应该被视为ANSI意图而不是A2。 区分这两种解释的历史是：</p>
<p>H2: r1[x&#x3D;50]r2[x&#x3D;50]w2[x&#x3D;10]r2[y&#x3D;50]w2[y&#x3D;90]c2r1[y&#x3D;90]c1</p>
<p>H2 is non-serializable — it is another inconsistent analy- sis, where T1 sees a total balance of 140. This time nei- ther transaction reads dirty (i.e. uncommitted) data. Thus P1 is satisfied. Once again, no data item is read twice nor is any relevant predicate evaluation changed. The problem with H2 is that by the time T1 reads y, the value for x is out of date. If T2 were to read x again, it would have been changed; but since T2 doesn’t do that, A2 doesn’t apply. Replacing A2 with P2, the broader interpretation, solves this problem.</p>
<p>H2 是不可序列化的——这是另一个不一致的分析，其中 T1 看到的总余额为 140。这次两个事务都不会读取脏（即未提交）数据。 这样P1就满足了。 再一次，没有数据项被读取两次，也没有任何相关的谓词评估被改变。 H2 的问题是，当 T1 读取 y 时，x 的值已经过时。 如果T2再次读取x，它就会被改变； 但由于 T2 不这样做，因此 A2 不适用。 用更广泛的解释 P2 代替 A2 解决了这个问题。</p>
<p>P2: r1[x]…w2[x]…((c1 or a1) and (c2 or a2) any order)</p>
<p>H2 would now be disqualified when w2[x&#x3D;20] occurs to overwrite r1[x&#x3D;50]. Finally, consider A3 and history H3:</p>
<p>当 w2[x&#x3D;20] 覆盖 r1[x&#x3D;50] 时，H2 现在将被取消资格。 最后，考虑 A3 和历史 H3：</p>
<p>A3: r1[P]…w2[y in P]…c2…r1[P]…c1 (Phantom)</p>
<p>H3: r1[P] w2[insert y to P] r2[z] w2[z] c2 r1[z] c1</p>
<p>Here T1 performs a [search condition] to find the list of active employees. Then T2 performs an insert of a new active employee and then updates z, the count of em- ployees in the company. Following this, T1 reads the count of active employees as a check and sees a discrep- ancy. This history is clearly not serializable, but is allowed by A3 since no predicate is evaluated twice. Again, the Broad interpretation solves the problem.</p>
<p>这里，T1 执行&lt;搜索条件&gt;来查找在职员工列表。 然后 T2 插入一个新的在职员工，然后更新 z，即公司中的员工数量。 随后，T1 读取在职员工的计数作为检查，并发现差异。 这个历史显然是不可序列化的，但 A3 允许，因为没有谓词被评估两次。 广义解释再次解决了这个问题。</p>
<p>P3: r1[P]…w2[y in P]…((c1 or a1) and (c2 or a2) any order)</p>
<p>If P3 is forbidden, history H3 is invalid. This is clearly what ANSI intended. The foregoing discussion demon- strates the following results.</p>
<p>如果P3被禁止，则历史H3无效。 这显然正是 ANSI 的意图。 上述讨论证明了以下结果。</p>
<p>Remark 4. Strict interpretations A1, A2, and A3 have unintended weaknesses. The correct interpretations are the Broad ones. We assume in what follows that ANSI meant to define P1, P2, and P3.</p>
<p>备注 4. 严格解释 A1、A2 和 A3 有意想不到的弱点。 正确的解释是广义的解释。 下面我们假设 ANSI 打算定义 P1、P2 和 P3。</p>
<p>Remark 5. ANSI SQL isolation phenomena are incom- plete. There are a number of anomalies that still can arise. New phenomena must be defined to complete the definition of locking. Also, P3 must be restated. In the following definitions, we drop references to (c2 or a2) that do not restrict histories.</p>
<p>备注 5. ANSI SQL 隔离现象是不完整的。 仍然可能出现许多异常情况。 必须定义新的现象来完成锁定的定义。 此外，P3 必须重述。 在以下定义中，我们删除对不限制历史的（c2 或 a2）的引用。</p>
<p>P0:  w1[x]…w2[x]…(c1 or a1)    (Dirty Write) </p>
<p>P1:  w1[x]…r2[x]…(c1 or a1)     (Dirty Read)</p>
<p>P2:  r1[x]…w2[x]…(c1 or a1)     (Fuzzy or Non-Repeatable Read)</p>
<p>P3:  r1[P]…w2[y in P]…(c1 or a1)    (Phantom)</p>
<p>One important note is that ANSI SQL P3 only prohibits inserts (and updates, according to some interpretations) to a predicate whereas the definition of P3 above prohibits any write satisfying the predicate once the predicate has been read — the write could be an insert, update, or delete.</p>
<p>一个重要的注意事项是，ANSI SQL P3 仅禁止对谓词进行插入（和更新，根据某些解释），而上面 P3 的定义一旦读取了谓词，就禁止任何满足谓词的写入 — 写入可以是插入、更新 ，或删除。</p>
<p>The definition of proposed ANSI isolation levels in terms of these phenomena is given in Table 3.</p>
<p>表 3 给出了针对这些现象提出的 ANSI 隔离级别的定义。</p>
<p>For single version histories, it turns out that the P0, P1, P2, P3 phenomena are disguised versions of locking. For example, prohibiting P0 precludes a second transaction writing an item after the first transaction has written it, equivalent to saying that long-term Write locks are held on data items (and predicates). Thus Dirty Writes are im- possible at all levels. Similarly, prohibiting P1 is equiv- alent to having well-formed reads on data items. Prohibiting P2 means long-term Read locks on data items. Finally, Prohibiting P3 means long-term Read predicate locks. Thus the isolation levels of Table 3 defined by these phenomena provide the same behavior as the Locking isolation levels of Table 2.</p>
<p>对于单一版本历史，事实证明，P0、P1、P2、P3 现象是变相版本的锁定。 例如，禁止 P0 会阻止第二个事务在第一个事务写入项目后写入该项目，相当于对数据项（和谓词）持有长期写入锁。 因此脏写在所有级别都是不可能的。 类似地，禁止 P1 相当于对数据项进行格式良好的读取。 禁止P2意味着对数据项进行长期的Read锁定。 最后，禁止 P3 意味着长期的 Read 谓词锁。 因此，由这些现象定义的表 3 的隔离级别提供与表 2 的锁定隔离级别相同的行为。</p>
<p>Remark 6. The locking isolation levels of Table 2 and the phenomenological definitions of Table 3 are equivalent. Put another way, P0, P1, P2, and P3 are disguised redefini- tion’soflockingbehavior.</p>
<p>备注 6. 表 2 的锁定隔离级别和表 3 的现象学定义是等效的。 换句话说，P0、P1、P2 和 P3 是变相重定义的锁定行为。</p>
<p>In what follows, we will refer to the isolation levels listed in Table 3 by the names in Table 3, equivalent to the Locking versions of these isolation levels of Table 2. When we refer to ANSI READ UNCOMMITTED, ANSI READ COMMITTED, ANSI REPEATABLE READ, and ANOMALY SERIALIZABLE, we are referring to the ANSI definition of Table 1 (inadequate, since it did not include P0).</p>
<p>接下来，我们将通过表 3 中的名称来引用表 3 中列出的隔离级别，相当于表 2 中这些隔离级别的 Locking 版本。当我们引用 ANSI READ UNCOMMITTED、ANSI READ COMMITTED、ANSI REPEATABLE READ 时 和 ANOMALY SERIALIZABLE，我们指的是表 1 的 ANSI 定义（不充分，因为它不包括 P0）。</p>
<p>The next section shows that a number of commercially available isolation implementations provide isolation levels that fall between READ COMMITTED and REPEATABLE READ. To achieve meaningful isolation levels that distin- guish these implementations, we will assume P0 and P1 as a basis and then add distinguishing new phenomena.</p>
<p>下一节将展示许多商用隔离实现提供介于 READ COMMITTED 和 REPEATABLE READ 之间的隔离级别。 为了实现区分这些实现的有意义的隔离级别，我们将假设 P0 和 P1 作为基础，然后添加有区别的新现象。</p>
<p><strong>Table 3. ANSI SQL Isolation Levels Defined in terms of the four phenomena</strong></p>
<table>
<thead>
<tr>
<th>Isolation Level</th>
<th>P0 Dirty Write</th>
<th>P1 Dirty Read</th>
<th>P2  Fuzzy Read</th>
<th>P3 Phantom</th>
</tr>
</thead>
<tbody><tr>
<td>READ UNCOMMITTED</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>READ COMMITTED</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>REPEATABLE READ</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>SERIALIZABLE</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
</tr>
</tbody></table>
<h2 id="4、Other-Isolation-Types"><a href="#4、Other-Isolation-Types" class="headerlink" title="4、Other Isolation Types"></a>4、Other Isolation Types</h2><h3 id="4-1、Cursor-Stability"><a href="#4-1、Cursor-Stability" class="headerlink" title="4.1、Cursor Stability"></a>4.1、Cursor Stability</h3><p>Cursor Stability is designed to prevent the lost update phenomenon.</p>
<p>光标稳定性旨在防止丢失更新现象。</p>
<p>P4 (Lost Update): The lost update anomaly occurs when transaction T1 reads a data item and then T2 updates the data item (possibly based on a previous read), then T1 (based on its earlier read value) updates the data item and commits. In terms of histories, this is:</p>
<p><strong>P4（丢失更新）</strong>：当事务T1读取一个数据项，然后T2更新该数据项（可能基于之前的读取），然后T1（基于其之前读取的值）更新该数据项并提交时，就会发生丢失更新异常 。 从历史来看，是这样的：</p>
<p>P4: r1[x]…w2[x]…w1[x]…c1  (lost update)</p>
<p>The problem, as illustrated in history H4, is that even if T2 commits, T2’s update will be lost.</p>
<p>正如历史记录 H4 所示，问题在于即使 T2 提交，T2 的更新也会丢失。</p>
<p>H4: r1[x&#x3D;100] r2[x&#x3D;100] w2[x&#x3D;120] c2 w1[x&#x3D;130] c1</p>
<p>The final value of x contains only the increment of 30 added by T1. P4 is possible at the READ COMMITTED isolation level, since H4 is allowed when forbidding P0 (a commit of the transaction performing the first write action precedes the second write) or P1 (which would require a read after a write). However, forbidding P2 also precludes P4, since w2[x] comes after r1[x] and before T1 commits or aborts. Therefore the anomaly P4 is useful in distinguishing isolation levels intermediate in strength be- tween READ COMMITTED and REPEATABLE READ.</p>
<p>x的最终值仅包含T1加上的增量30。 P4 在 READ COMMITTED 隔离级别上是可能的，因为在禁止 P0（执行第一个写入操作的事务提交在第二个写入之前）或 P1（需要在写入之后进行读取）时允许 H4。 然而，禁止 P2 也会排除 P4，因为 w2[x] 出现在 r1[x] 之后且 T1 提交或中止之前。 因此，异常 P4 在区分已提交读和可重复读之间强度中等的隔离级别时非常有用。</p>
<p>The Cursor Stability isolation level extends READ COMMITTED locking behavior for SQL cursors by adding a new read action for FETCH from a cursor and requiring that a lock be held on the current item of the cursor. The lock is held until the cursor moves or is closed, possibly by a commit. Naturally, the Fetching transaction can update the row, and in that case a write lock will be held on the row until the transaction commits, even after the cursor moves on with a subsequent Fetch. The notation is extended to include, rc, meaning read cursor, and wc, meaning write the current record of the cursor. A rc1[x] and a later wc1[x] precludes an intervening w2[x]. Phenomenon P4, renamed P4C, is prevented in this case.</p>
<p>游标稳定性隔离级别通过为游标的 FETCH 添加新的读取操作并要求在游标的当前项上保持锁定，扩展了 SQL 游标的 READ COMMITTED 锁定行为。 锁定将一直保持到游标移动或关闭（可能是通过提交）。 当然，Fetching 事务可以更新该行，在这种情况下，即使在游标继续进行后续的 Fetch 操作之后，写入锁定也会一直保留在该行上，直到事务提交为止。 该符号被扩展为包括，rc，表示读取游标，wc，表示写入游标的当前记录。 rc1[x] 和稍后的 wc1[x] 排除了介入的 w2[x]。 在这种情况下，可以防止现象 P4（更名为 P4C）。</p>
<p>P4C: rc1[x]…w2[x]…w1[x]…c1 (Lost Update)</p>
<p>Remark 7:</p>
<p>READ COMMITTED « Cursor Stability « REPEATABLE READ</p>
<p>Cursor Stability is widely implemented by SQL systems to prevent lost updates for rows read via a cursor. READ COMMITTED, in some systems, is actually the stronger Cursor Stability. The ANSI standard allows this.</p>
<p>SQL 系统广泛实现游标稳定性，以防止通过游标读取的行丢失更新。 READ COMMITTED，在某些系统中，实际上是更强的游标稳定性。 ANSI 标准允许这样做。</p>
<p>The technique of putting a cursor on an item to hold its value stable can be used for multiple items, at the cost of using multiple cursors. Thus the programmer can parlay Cursor Stability to effective Locking REPEATABLE READ isolation for any transaction accessing a small, fixed num- ber of data items. However this method is inconvenient and not at all general. Thus there are always histories fitting the P4 (and of course the more general P2) phenomenon that are not precluded by Cursor Stability.</p>
<p>将光标放在某个项目上以保持其值稳定的技术可用于多个项目，但代价是使用多个光标。 因此，对于访问少量固定数量数据项的任何事务，程序员可以利用游标稳定性来有效锁定可重复读隔离。 但这种方法不方便且不通用。 因此，总是存在符合 P4（当然还有更一般的 P2）现象的历史，这些历史不会被游标稳定性所排除。</p>
<h3 id="4-2、Snapshot-Isolation"><a href="#4-2、Snapshot-Isolation" class="headerlink" title="4.2、Snapshot Isolation"></a>4.2、Snapshot Isolation</h3><p>These discussions naturally suggest an isolation level, called Snapshot Isolation, in which each transaction reads reads data from a snapshot of the (committed) data as of the time the transaction started, called its Start-Timestamp. This time may be any time before the transaction’s first Read. A transaction running in Snapshot Isolation is never blocked attempting a read as long as the snapshot data from its Start-Timestamp can be maintained. The transaction’s writes (updates, inserts, and deletes) will also be reflected in this snapshot, to be read again if the transaction accesses (i.e., reads or updates) the data a second time. Updates by other transactions active after the transaction Start-Timestamp are invisible to the transaction.</p>
<p>这些讨论自然地提出了一种隔离级别，称为快照隔离，其中每个事务从事务启动时（称为其开始时间戳）的（已提交）数据的快照中读取数据。 该时间可以是事务第一次读取之前的任何时间。 只要可以维护其开始时间戳中的快照数据，在快照隔离中运行的事务就不会阻止尝试读取。 事务的写入（更新、插入和删除）也将反映在该快照中，如果事务第二次访问（即读取或更新）数据，则可以再次读取。 在事务开始时间戳之后活动的其他事务的更新对该事务来说是不可见的。</p>
<p>Snapshot Isolation is a type of multiversion concurrency control. It extends the Multiversion Mixed Method de- scribed in [BHG], which allowed snapshot reads by read- only transactions.</p>
<p>快照隔离是一种多版本并发控制。 它扩展了[BHG]中描述的多版本混合方法，该方法允许只读事务进行快照读取。</p>
<p>When the transaction T1 is ready to commit, it gets a Commit-Timestamp, which is larger than any existing Start-Timestamp or Commit-Timestamp. The transaction successfully commits only if no other transaction T2 with a Commit-Timestamp in T1’s execution interval [Start- Timestamp, Commit-Timestamp] wrote data that T1 also wrote. Otherwise, T1 will abort.<font color="red"> This feature, called First- committer-wins prevents lost updates (phenomenon P4). </font>When T1 commits, its changes become visible to all transactions whose Start-Timestamps are larger than T1‘s Commit-Timestamp.</p>
<p>当事务 T1 准备好提交时，它会获得一个 Commit-Timestamp，该时间戳大于任何现有的 Start-Timestamp 或 Commit-Timestamp。 仅当在 T1 的执行间隔 [Start-Timestamp, Commit-Timestamp] 内没有其他具有 Commit-Timestamp 的事务 T2 写入 T1 也写入的数据时，事务才会成功提交。 否则，T1将中止。 这个称为“Firstcommitter-wins”的功能可以防止更新丢失（现象 P4）。 当 T1 提交时，其更改对于所有开始时间戳大于 T1 的提交时间戳的事务都可见。</p>
<p>Snapshot Isolation is a multi-version (MV) method, so single-valued (SV) histories do not properly reflect the tempo- ral action sequences. At any time, each data item might have multiple versions, created by active and committed transactions. Reads by a transaction must choose the appropriate version. Consider history H1 at the beginning of Section 3, which shows the need for P1 in a single valued execution. Under Snapshot Isolation, the same sequence of actions would lead to the multi-valued history:</p>
<p>快照隔离是一种多版本（MV）方法，因此单值（SV）历史不能正确反映时间动作序列。 在任何时候，每个数据项都可能有多个版本，由活动和已提交的事务创建。 事务读取必须选择合适的版本。 考虑第 3 节开头的历史 H1，它显示了在单值执行中需要 P1。 在快照隔离下，相同的操作序列将导致多值历史记录：</p>
<p>H1.SI: r1[x0&#x3D;50] w1[x1&#x3D;10] r2[x0&#x3D;50] r2[y0&#x3D;50] c2 r1[y0&#x3D;50] w1[y1&#x3D;90] c1</p>
<p>H1.SI has the dataflows of a serializable execution. In [OOBBGM], we show that all Snapshot Isolation histories can be mapped to single-valued histories while preserving dataflow dependencies (the MV histories are said to be View Equivalent with the SV histories, an approach covered in [BHG], Chapter 5). For example the MV his- tory H1.SI would map to the serializable SV history:</p>
<p>H1.SI 具有可串行执行的数据流。 在 [OOBBGM] 中，我们表明所有快照隔离历史都可以映射到单值历史，同时保留数据流依赖性（MV 历史被称为与 SV 历史等效的视图，[BHG] 第 5 章中介绍了这种方法） 。 例如，MV 历史记录 H1.SI 将映射到可序列化的 SV 历史记录：</p>
<p>H1.SI.SV: r1[x&#x3D;50] r1[y&#x3D;50] r2[x&#x3D;50] r2[y&#x3D;50] c2 w1[x&#x3D;10] w1[y&#x3D;90] c1</p>
<p>Mapping of MV histories to SV histories is the only rigor- ous touchstone needed to place Snapshot Isolation in the Isolation Hierarchy.</p>
<p>MV 历史到 SV 历史的映射是将快照隔离放入隔离层次结构所需的唯一严格的试金石。</p>
<p>Snapshot Isolation is non-serializable because a transac- tion’s Reads come at one instant and the Writes at another. For example, consider the single-value history:</p>
<p>快照隔离是不可串行化的，因为事务的读取在某一时刻发生，而写入在另一时刻发生。 例如，考虑单值历史记录：</p>
<p>H5: r1[x&#x3D;50] r1[y&#x3D;50] r2[x&#x3D;50] r2[y&#x3D;50] w1[y&#x3D;-40] w2[x&#x3D;-40] c1 c2</p>
<p>H5 is non-serializable and has the same inter-transactional dataflows as could occur under Snapshot Isolation (there is no choice of versions read by the transactions). Here we assume that each transaction that writes a new value for x and y is expected to maintain the constraint that x + y should be positive, and while T1 and T2 both act properly in isolation, the constraint fails to hold in H5.</p>
<p>H5 是不可序列化的，并且具有与快照隔离下可能发生的相同的事务间数据流（无法选择事务读取的版本）。 在这里，我们假设每个为 x 和 y 写入新值的事务都期望维持 x + y 应该为正的约束，并且虽然 T1 和 T2 都独立正常运行，但该约束在 H5 中不成立。</p>
<p>Constraint violation is a generic and important type of concurrency anomaly. Individual databases satisfy con- straints over multiple data items (e.g., uniqueness of keys, referential integrity, replication of rows in two tables, etc.). Together they form the database invariant constraint predi- cate, C(DB). The invariant is TRUE if the database state DB is consistent with the constraints and is FALSE otherwise. Transactions must preserve the constraint predicate to maintain consistency: if the database is consistent when the transaction starts, the database will be consistent when the transaction commits. If a transaction reads a database state that violates the constraint predicate, then the transaction suffers from a constraint violation concurrency anomaly. Such constraint violations are called inconsistent analysis in [DAT].</p>
<p>约束违反是一种常见且重要的并发异常类型。 各个数据库满足多个数据项的约束（例如，键的唯一性、引用完整性、两个表中行的复制等）。 它们一起形成数据库不变约束谓词 C(DB)。 如果数据库状态 DB 与约束一致，则不变量为 TRUE，否则为 FALSE。 事务必须保留约束谓词以保持一致性：如果事务启动时数据库是一致的，那么事务提交时数据库也会一致。 如果事务读取违反约束谓词的数据库状态，则该事务将遭受约束违反并发异常。 这种约束违反在 [DAT] 中称为不一致分析。</p>
<p>A5 (Data Item Constraint Violation). Suppose C() is a database constraint between two data items x and y in the database. Here are two anomalies arising from constraint violation.</p>
<p>A5（数据项约束违规）。 假设C()是数据库中两个数据项x和y之间的数据库约束。 这是由于违反约束而产生的两个异常。</p>
<p><strong>A5A Read Skew</strong> Suppose transaction T1 reads x, and then a second transaction T2 updates x and y to new values and commits. If now T1 reads y, it may see an inconsistent state, and therefore produce an inconsistent state as output. In terms of histories, we have the anomaly:</p>
<p>A5A 读取偏差 假设事务 T1 读取 x，然后第二个事务 T2 将 x 和 y 更新为新值并提交。 如果现在 T1 读取 y，它可能会看到不一致的状态，因此会产生不一致的状态作为输出。 就历史而言，我们有一个异常现象：</p>
<p>A5A: r1[x]…w2[x]…w2[y]…c2…r1[y]…(c1 or a1)     (Read Skew)</p>
<p><strong>A5B Write Skew</strong> Suppose T1 reads x and y, which are consistent with C(), and then a T2 reads x and y, writes x, and commits. Then T1 writes y. If there were a constraint between x and y, it might be violated. In terms of histo- ries:</p>
<p>A5B Write Skew 假设T1读取x和y，与C()一致，然后T2读取x和y，写入x并提交。 然后T1写入y。 如果 x 和 y 之间存在约束，则可能会被违反。 就历史而言：</p>
<p>A5B: r1[x]…r2[y]…w1[y]…w2[x]…(c1 and c2 occur)  (Write Skew)</p>
<p>Fuzzy Reads (P2) is a degenerate form of Read Skew where x&#x3D;y. More typically, a transaction reads two dif- ferent but related items (e.g., referential integrity). Write Skew (A5B) could arise from a constraint at a bank, where account balances are allowed to go negative as long as the sum of commonly held balances remains non-negative, with an anomaly arising as in history H5.</p>
<p>模糊读取 (P2) 是读取倾斜的退化形式，其中 x&#x3D;y。 更典型的是，一个事务读取两个不同但相关的项目（例如，引用完整性）。 写入偏差 (A5B) 可能是由银行的限制引起的，只要共同持有余额的总和保持非负值，就允许账户余额变为负值，就像历史 H5 中那样出现异常。</p>
<p><font color="red">Clearly neither A5A nor A5B could arise in histories where P2 is precluded, since both A5A and A5B have T2 write a data item that has been previously read by an un- committed T1.</font> Thus, phenomena A5A and A5B are only useful for distinguishing isolation levels that are below REPEATABLE READ in strength.</p>
<p>显然，A5A 和 A5B 都不会出现在 P2 被排除的历史中，因为 A5A 和 A5B 都让 T2 写入了先前已被未提交的 T1 读取的数据项。 因此，现象 A5A 和 A5B 仅适用于区分强度低于 REPEATABLE READ 的隔离级别。</p>
<p>The ANSI SQL definition of REPEATABLE READ, in its strict interpretation, captures a degenerate form of row constraints, but misses the general concept. To be specific, Locking REPEATABLE READ of Table 2 provides protection from Row Constraint Violations but the ANSI SQL definition of Table 1, forbidding anomalies A1 and A2, does not.</p>
<p>REPEATABLE READ 的 ANSI SQL 定义在其严格解释中捕获了行约束的简并形式，但忽略了一般概念。 具体来说，表 2 的锁定可重复读取提供了针对行约束违规的保护，但表 1 的 ANSI SQL 定义（禁止异常 A1 和 A2）却没有提供保护。</p>
<p>Returning now to Snapshot Isolation, it is surprisingly strong, even stronger than READ COMMITTED.</p>
<p>现在回到快照隔离，它非常强大，甚至比 READ COMMITTED 还要强大。</p>
<p>Remark 8. READ COMMITTED « Snapshot Isolation</p>
<p>Proof. In Snapshot Isolation, first-committer-wins pre- cludes P0 (dirty writes), and the timestamp mechanism prevents P1 (dirty reads), so Snapshot Isolation is no weaker than READ COMMITTED. In addition, <font color="red">A5A is possible under READ COMMITTED</font>, but not under the Snapshot Isolation timestamp mechanism. Therefore READ COMMITTED « Snapshot Isolation.</p>
<p>证明。 在Snapshot Isolation中，first-committer-wins排除了P0（脏写），并且时间戳机制阻止了P1（脏读），因此Snapshot Isolation并不比READ COMMITTED弱。 另外，A5A在READ COMMITTED下是可以的，但在Snapshot Isolation时间戳机制下是不行的。 因此，READ COMMITTED « 快照隔离。</p>
<p>Note that it is difficult to picture how Snapshot Isolation histories can disobey phenomenon P2 in the single-valued interpretation. Anomaly A2 cannot occur, since a transac- tion under Snapshot Isolation will read the same value of a data item even after a temporally intervening update by another transaction.<font color="red"> However, Write Skew (A5B) obviously can occur in a Snapshot Isolation history (e.g., H5), and in the Single Valued history interpretation we’ve been reason- ing about, forbidding P2 also precludes A5B. </font>Therefore Snapshot Isolation admits history anomalies that REPEATABLE READ does not.</p>
<p>请注意，很难想象快照隔离历史如何违背单值解释中的现象 P2。 异常 A2 不会发生，因为即使在另一个事务临时干预更新之后，快照隔离下的事务也会读取数据项的相同值。 然而，写倾斜（A5B）显然可能发生在快照隔离历史（例如，H5）中，并且在我们一直在推理的单值历史解释中，禁止 P2 也排除了 A5B。 因此，快照隔离允许历史异常，而可重复读取则不允许。</p>
<p><font color="red">Snapshot Isolation cannot experience the A3 anomaly</font>. A transaction rereading a predicate after an update by another will always see the same old set of data items. But the REPEATABLE READ isolation level can experience A3 anomalies. Snapshot Isolation histories prohibit histories with anomaly A3, but allow A5B, while REPEATABLE READ does the opposite. Therefore:</p>
<p>快照隔离无法遇到A3异常。 在另一个事务更新后重新读取谓词的事务将始终看到相同的旧数据项集。 但 REPEATABLE READ 隔离级别可能会出现 A3 异常。 快照隔离历史禁止异常 A3 的历史，但允许 A5B，而 REPEATABLE READ 则相反。 所以：</p>
<p>Remark 9. REPEATABLE READ »« Snapshot Isolation.</p>
<p>However, Snapshot Isolation does not preclude P3. Consider a constraint that says a set of job tasks deter- mined by a predicate cannot have a sum of hours greater than 8. T1 reads this predicate, determines the sum is only 7 hours and adds a new task of 1 hour duration, while a concurrent transaction T2 does the same thing. Since the two transactions are inserting different data items (and different index entries as well, if any), this scenario is not precluded by First-Committer-Wins and can occur in Snapshot Isolation. But in any equivalent serial history, the phenomenon P3 would arise under this scenario.</p>
<p>然而，快照隔离并不排除P3。 考虑一个约束，该约束表示由谓词确定的一组作业任务的小时总和不能大于 8。T1 读取该谓词，确定总时间仅为 7 小时，并添加一个持续时间为 1 小时的新任务，而 并发事务T2做同样的事情。 由于两个事务插入不同的数据项（以及不同的索引条目，如果有），因此 First-Committer-Wins 不会排除这种情况，并且可能会在快照隔离中发生。 但在任何同等的连续历史中，P3现象都会在这种情况下出现。</p>
<p>Perhaps most remarkable of all, Snapshot Isolation has no phantoms (in the strict sense of the ANSI definitions A3). Each transaction never sees the updates of concurrent transactions. So, one can state the following surprising re- sult (recall that section Table 1 defined ANOMALY SE- RIALIZABLE as ANSI SQL definition of SERIALIZABLE) without the extra restriction in Subclause 4.28 in [ANSI]:</p>
<p>也许最引人注目的是，快照隔离没有幻像（严格意义上的 ANSI 定义 A3）。 每个事务永远不会看到并发事务的更新。 因此，我们可以得出以下令人惊讶的结果（回想一下表 1 将 ANOMALY SERIALIZABLE 定义为 ANSI SQL SERIALIZABLE 定义），而没有 [ANSI] 中第 4.28 款中的额外限制：</p>
<p>Remark 10. Snapshot Isolation histories preclude anomalies A1, A2 and A3. Therefore, in the anomaly in- terpretation of ANOMALY SERIALIZABLE of Table 1:</p>
<p>备注 10. 快照隔离历史排除异常 A1、A2 和 A3。 因此，在表1的ANOMALY SERIALIZABLE异常解释中：</p>
<p>ANOMALY SERIALIZABLE « SNAPSHOT ISOLATION.</p>
<p>Snapshot Isolation gives the freedom to run transactions with very old timestamps, thereby allowing them to do time travel — taking a historical perspective of the database — while never blocking or being blocked by writes. Of course, update transactions with very old timestamps would abort if they tried to update any data item that had been updated by more recent transactions.</p>
<p>快照隔离提供了使用非常旧的时间戳运行事务的自由，从而允许它们进行时间旅行（从数据库的历史角度来看），同时不会阻塞或被写入阻塞。 当然，如果具有非常旧的时间戳的更新事务尝试更新已由较新的事务更新的任何数据项，则它们将中止。</p>
<p>Snapshot Isolation admits a simple implementation mod- eled on the work of Reed [REE]. There are several com- mercial implementations of such multi-version databases. Borland’s InterBase 4 [THA] and the engine underlying Microsoft’s Exchange System both provide Snapshot Isolation with the First-committer-wins feature. First- committer-wins requires the system to remember all up- dates (write locks) belonging to any transaction that commits after the Start-Timestamp of each active transac- tion. It aborts the transaction if its updates conflict with remembered updates by others.</p>
<p>快照隔离允许以 Reed [REE] 的工作为模型的简单实现。 这种多版本数据库有多种商业实现。 Borland 的 InterBase 4 [THA] 和 Microsoft Exchange 系统底层的引擎都提供具有“先提交者获胜”功能的快照隔离。 首先提交者获胜要求系统记住属于在每个活动事务的开始时间戳之后提交的任何事务的所有更新（写锁）。 如果它的更新与其他人记住的更新冲突，它会中止事务。</p>
<p>Snapshot Isolation’s “optimistic” approach to concurrency control has a clear concurrency advantage for read-only transactions, but its benefits for update transactions is still debated. It probably isn’t good for long-running update transactions competing with high-contention short transac- tions, since the long-running transactions are unlikely to be the first writer of everything they write, and so will probably be aborted. (Note that this scenario would cause a real problem in locking implementations as well, and if the solution is to not allow long-running update transactions that would hold up short transaction locks, Snapshot Isolation would also be acceptable.) Certainly in cases where short update transactions conflict minimally and long-running transactions are likely to be read only, Snapshot Isolation should give good results. In regimes where there is high contention among transactions of comparable length, Snapshot Isolation offers a classical optimistic approach, and there are differences of opinion as to the value of this.</p>
<p>快照隔离的“乐观”并发控制方法对于只读事务具有明显的并发优势，但其对于更新事务的好处仍然存在争议。 对于长时间运行的更新事务与高争用的短事务竞争来说，这可能并不好，因为长时间运行的事务不太可能是它们所写入的所有内容的第一个写入者，因此可能会被中止。 （请注意，这种情况也会导致锁定实现中出现真正的问题，如果解决方案是不允许长时间运行的更新事务会持有短事务锁，那么快照隔离也是可以接受的。） 更新事务冲突最少，并且长时间运行的事务可能是只读的，快照隔离应该会产生良好的结果。 在相当长度的交易之间存在高度争用的制度中，快照隔离提供了一种经典的乐观方法，并且对其价值存在不同意见。</p>
<h3 id="4-3、Other-Multi-Version-Systems"><a href="#4-3、Other-Multi-Version-Systems" class="headerlink" title="4.3、Other Multi-Version Systems"></a>4.3、Other Multi-Version Systems</h3><p>There are other models of multi-versio99ning. Some com- mercial products maintain versions of objects but restrict Snapshot Isolation to read-only transactions (e.g., SQL-92, Rdb,and SET TRANSACTION READ ONLYinsomeother databases [MS, HOB, ORA]; Postgres and Illustra [STO, ILL] maintain such versions long-term and provide time- travel queries). Others allow update transactions but do not provide first-committer-wins protection (e.g., Oracle Read Consistency isolation [ORA]).</p>
<p>还有其他型号的多版本。 一些商业产品维护对象的版本，但将快照隔离限制为只读事务（例如，SQL-92、Rdb 和其他数据库中的 SET TRANSACTION READ ONLY [MS、HOB、ORA]；Postgres 和 Illustra [STO、ILL] 维护 此类版本是长期的并提供时间旅行查询）。 其他允许更新事务，但不提供先提交者获胜保护（例如，Oracle 读一致性隔离 [ORA]）。</p>
<p>Oracle Read Consistency isolation gives each SQL state- ment the most recent committed database value at the time the statement began. It is as if the start-timestamp of the transaction is advanced at each SQL statement. The members of a cursor set are as of the time of the Open Cursor. The underlying mechanism recomputes the ap- propriate version of the row as of the statement timestamp. Row inserts, updates, and deletes are covered by Write locks to give a first-writer-wins rather than a first- committer-wins policy. Read Consistency is stronger than READ COMMITTED (it disallows cursor lost updates (P4C)) but allows non-repeatable reads (P3), general lost updates (P4), and read skew (A5A). Snapshot Isolation does not permit P4 or A5A.</p>
<p>Oracle 读一致性隔离为每个 SQL 语句提供该语句开始时最新提交的数据库值。 就好像事务的开始时间戳在每个 SQL 语句中都会提前。 游标集的成员是从打开游标时开始的。 底层机制重新计算截至语句时间戳的行的适当版本。 行插入、更新和删除由写锁覆盖，以提供先写者胜而不是先提交者胜的策略。 读一致性比 READ COMMITTED 更强（它不允许游标丢失更新 (P4C)），但允许不可重复读取 (P3)、一般丢失更新 (P4) 和读取倾斜 (A5A)。 快照隔离不允许 P4 或 A5A。</p>
<p>If one looks carefully at the SQL standard, it defines each statement as atomic. It has a serializable sub-transaction (or timestamp) at the start of each statement. One can imagine a hierarchy of isolation levels defined by assign- ing timestamps to statements in interesting ways (e.g., in Oracle, a cursor fetch has the timestamp of the cursor open).</p>
<p>如果仔细查看 SQL 标准，它会将每个语句定义为原子语句。 它在每个语句的开头都有一个可序列化的子事务（或时间戳）。 人们可以想象通过以有趣的方式为语句分配时间戳来定义的隔离级别层次结构（例如，在 Oracle 中，游标获取具有打开游标的时间戳）。</p>
<h2 id="5、Summary-and-Conclusions"><a href="#5、Summary-and-Conclusions" class="headerlink" title="5、Summary and Conclusions"></a>5、Summary and Conclusions</h2><p>In summary, there are serious problems with the original ANSI SQL definition of isolation levels (as explained in Section 3). The English language definitions are ambiguous and incomplete. Dirty Writes (P0) are not precluded. Remark 5 is our recommendation for cleaning up the ANSI Isolation levels to equate to the locking isolation levels of [GLPT].</p>
<p>总之，最初的 ANSI SQL 隔离级别定义存在严重问题（如第 3 节所述）。 英语语言定义不明确且不完整。 不排除脏写（P0）。 备注 5 是我们建议清理 ANSI 隔离级别以使其等同于 [GLPT] 的锁定隔离级别。</p>
<p>ANSI SQL intended to define REPEATABLE READ isolation to exclude all anomalies except Phantom. The anomaly definition of Table 1 does not achieve this goal, but the locking definition of Table 2 does. ANSI’s choice of the term Repeatable Read is doubly unfortunate: (1) repeatable reads do not give repeatable results, and (2) the industry had already used the term to mean exactly that: repeatable reads mean serializable in several products. We recommend that another term be found for this.</p>
<p>ANSI SQL 旨在定义 REPEATABLE READ 隔离以排除除 Phantom 之外的所有异常。 表 1 的异常定义没有实现此目标，但表 2 的锁定定义可以。 ANSI 选择“可重复读取”一词是双重不幸的：(1) 可重复读取不会给出可重复的结果，(2) 业界已经使用该术语来准确表示：可重复读取意味着在多种产品中可序列化。 我们建议为此找到另一个术语。</p>
<p>A number of commercially-popular isolation levels, falling between the REPEATABLE READ and SERIALIZABLE levels of Table 3 in strength, have been characterized with some new phenomena and anomalies in Section 4. All the isolation levels named here have been characterized as shown in Figure 2 and Table 4. Isolation levels at higher levels in Figure 2 are higher in strength (see the Definition at the beginning of Section 4.1) and the connecting lines are labeled with the phenomena and anomalies that differentiate them.</p>
<p>许多商业上流行的隔离级别的强度介于表 3 的 REPEATABLE READ 和 SERIALIZABLE 级别之间，在第 4 节中已通过一些新现象和异常进行了表征。此处命名的所有隔离级别均已进行表征，如图 2 所示 和表 4。图 2 中较高级别的隔离级别强度较高（请参阅第 4.1 节开头的定义），并且连接线标有区分它们的现象和异常。</p>
<p>On a positive note, reduced isolation levels for multi-ver- sion systems have never been characterized before — de- spite being implemented in several products. Many appli- cations avoid lock contention by using Cursor Stability or Oracle’s Read Consistency isolation. Such applications will find Snapshot Isolation better behaved than either: it avoids the lost update anomaly, some phantom anomalies (e.g., the one defined by ANSI SQL), it never blocks read- only transactions, and readers do not block updates.</p>
<p>从积极的方面来看，尽管在多个产品中实现了多版本系统的降低隔离级别，但以前从未有过这种特征。 许多应用程序通过使用游标稳定性或 Oracle 的读一致性隔离来避免锁争用。 此类应用程序会发现快照隔离比任何一种都表现得更好：它避免了丢失更新异常、一些幻像异常（例如 ANSI SQL 定义的异常）、它从不阻止只读事务，并且读取器不会阻止更新。</p>
<p><strong>Table 4. Isolation Types Characterized by Possible Anomalies Allowed.</strong></p>
<table>
<thead>
<tr>
<th>Isolation level</th>
<th>P0 Dirty Write</th>
<th>P1 Dirty Read</th>
<th>P4C Cursor Lost Update</th>
<th>P4  Lost Update</th>
<th>P2 Fuzzy Read</th>
<th>P3 Phantom</th>
<th>A5A</th>
<th>A5b</th>
</tr>
</thead>
<tbody><tr>
<td>READ UNCOMMITTED &#x3D;&#x3D; Degree 1</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>READ COMMITTED &#x3D;&#x3D; Degree 2</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>Cursor Stability</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Sometimes Possible</td>
<td>Sometimes Possible</td>
<td>Possible</td>
<td>Possible</td>
<td>Sometimes Possible</td>
</tr>
<tr>
<td>REPEATABLE READ</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
</tr>
<tr>
<td>Snapshot</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Sometimes Possible</td>
<td>Not Possible</td>
<td>Possible</td>
</tr>
<tr>
<td>ANSI SQL SERIALIZABLE  &#x3D;&#x3D; Degree 3  &#x3D;&#x3D; Repeatable Read   <br>Date, IBM, Tandem, …</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
<td>Not Possible</td>
</tr>
</tbody></table>
<p><strong>Figure 2:</strong></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Serializable == Degree 3 == &#123;Date, DB2&#125; Repeatable Read</span><br><span class="line">                   |              \</span><br><span class="line">                   | P3             \  A5B</span><br><span class="line">                   |            A5B   \</span><br><span class="line">              Repeatable Read  _ _ _ _  Snapshot Isolation</span><br><span class="line">       P2   /      |          \ _ _ _ _ /        /</span><br><span class="line">          /        | P2           A3           /</span><br><span class="line">  Oracle           |                         /</span><br><span class="line">  Consistent   Cursor Stability            /  A3, A5A, P4</span><br><span class="line">  Read             |                     /</span><br><span class="line">       \  P4C      | P4C               /</span><br><span class="line">         \         |                 /</span><br><span class="line">           Read Committed == Degreee 2</span><br><span class="line">                   |</span><br><span class="line">                   | p1</span><br><span class="line">                   |</span><br><span class="line">           Read Uncommitted == Degree 1</span><br><span class="line">                   |</span><br><span class="line">                   |  P0</span><br><span class="line">                   |</span><br><span class="line">                 Degree 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Figure 2: A diagram of the isolation levels and their rela- tionships. It assume that the ANSI SQL isolation levels have been strengthened to match the recommendation of Remark 5 and Table 3. The edges are annotated with the phenomena that differentiate the isolation levels. Not shown is a potential multi-version hierarchy extending Snapshot Isolation to lower degrees of isolation by picking read timestamps on a per-statement basis. Nor does it show the original ANSI SQL isolation levels based on the strict interpretation of the phenomenon P1, P2, and P3.</p>
<p>图 2：隔离级别及其关系图。 它假设 ANSI SQL 隔离级别已得到增强，以符合备注 5 和表 3 的建议。边缘用区分隔离级别的现象进行注释。 未显示的是潜在的多版本层次结构，通过在每个语句的基础上选择读取时间戳，将快照隔离扩展到较低的隔离程度。 它也没有显示基于对 P1、P2 和 P3 现象的严格解释的原始 ANSI SQL 隔离级别。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>Volcano</title>
    <url>/2023/08/15/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/0-volcano/</url>
    <content><![CDATA[<h2 id="Volcano"><a href="#Volcano" class="headerlink" title="Volcano"></a>Volcano</h2><p>An Extensible and Parallel Query Evaluation System </p>
<p>Volcano-一个可扩展的并行查询评估系统</p>
<p>Abstract-To investigate the interactions  parallelism in database query processing, we have developed a new dataflow query execution system called Volcano. The Volcano effort provides a rich environment for research and education in database systems design, heuristics for query optimization, parallel query execution, and resource allocation.</p>
<p>摘要：为了研究数据库查询处理中可扩展性和并行性的相互作用，我们开发了一种新的数据流查询执行系统，称为 Volcano。 Volcano 的工作为数据库系统设计、查询优化启发式、并行查询执行和资源分配方面的研究和教育提供了丰富的环境。</p>
<span id="more"></span>

<p>Volcano uses a standard interface between algebra operators, allowing easy addition of new operators and operator implementations. Operations on individual items, e.g., predicates, are imported into the query processing operators using support functions. The semantics of support functions is not prescribed; any data type including complex objects and any operation can be realized. Thus, Volcano is extensible with new operators, algorithms, data types, and type-specific methods.</p>
<p>Volcano 在代数运算符之间使用标准接口，允许轻松添加新运算符和运算符实现。 使用支持函数将对单个项目（例如谓词）的操作导入到查询处理运算符中。 支持函数的语义没有规定； 可以实现包括复杂对象在内的任何数据类型和任何操作。 因此，Volcano 可以通过新的运算符、算法、数据类型和特定于类型的方法进行扩展。</p>
<p>Volcano includes two novel meta-operators. The choose-plan meta-operator supports dynamic query evaluation plans that al- low delaying selected optimization decisions until run-time, e.g., for embedded queries with free variables. The exchange meta-operator supports intra-operator parallelism on parti- tioned datasets and both vertical and horizontal inter-operator parallelism, translating between demand-driven dataflow within processes and data-driven dataflow between processes.</p>
<p>Volcano 包括两个新颖的元运算符。 选择计划元运算符支持动态查询评估计划，允许将选定的优化决策延迟到运行时，例如，对于具有自由变量的嵌入式查询。 交换元操作符支持分区数据集上的操作符内并行性以及垂直和水平操作符间并行性，在进程内的需求驱动数据流和进程之间的数据驱动数据流之间进行转换。</p>
<p>All operators, with the exception of the exchange operator, have been designed and implemented in a single-process envi- ronment, and parallelized using the exchange operator. Even operators not yet designed can be parallelized using this new operator if they use and provide the interator interface. Thus, the issues of data manipulation and parallelism have become orthogonal, making Volcano the first implemented query exe- cution engine that effectively combines extensibility and parallelism.</p>
<p>除交换运算符外，所有运算符都是在单进程环境中设计和实现的，并使用交换运算符进行并行化。 即使尚未设计的运算符如果使用并提供交互器接口，也可以使用这个新运算符进行并行化。 因此，数据操作和并行性问题变得正交，使 Volcano 成为第一个实现的、有效结合可扩展性和并行性的查询执行引擎。</p>
<p>index Terms-Dynamic query evaluation plans, extensible database systems, iterators, operator model of parallelization, query execution.</p>
<p>索引术语-动态查询评估计划、可扩展数据库系统、迭代器、并行化操作模型、查询执行。</p>
<h3 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h3><p>IN ORDER to investigate the interactions of extensibil- ity, efficiency, and parallelism in database query pro- cessing and to provide a testbed for databse systems research and education, we have designed and implemented a new query evaluation system called Volcano. It is intended to provide an experimental vehicle for research into query execution techniques and query optimization optimization heuristics rather than a database system ready to support applications. it is not a complete database systern as it lacks features such as a user-friendly query language, a type system for instances (record definitions), a query optimizer, and catalogs. Because of this focus, Volcano is able to serve as an experimental vehicle for a multitude of purposes, all .of them openended, which results in a combination of requirements that have not been integrated in a single system before. </p>
<p>为了研究数据库查询处理中可扩展性、效率和并行性的相互作用，并为数据库系统研究和教育提供测试平台，我们设计并实现了一个名为 Volcano 的新查询评估系统。 它旨在提供一个用于研究查询执行技术和查询优化启发式的实验工具，而不是一个准备支持应用程序的数据库系统。它不是一个完整的数据库系统，因为它缺乏用户友好的查询语言、实例类型系统（记录定义）、查询优化器和目录等功能。 由于这一重点，Volcano 能够作为多种目的的实验工具，所有这些目的都是开放式的，从而产生了以前从未集成到单个系统中的需求组合。</p>
<p>First, it is modular and extensible to enable future research, e.g., on algorithms, data models, resource allocation, parallel execution, load balancing, and query optimization heuristics. Thus, Vol- cano provides an infrastructure for experimental research rather than a final research prototype in itself. </p>
<p>首先，它是模块化和可扩展的，可以支持未来的研究，例如算法、数据模型、资源分配、并行执行、负载平衡和查询优化启发法。 因此，Volcano 为实验研究提供了基础设施，而不是其本身的最终研究原型。</p>
<p>Second, it is simple in its design to allow student use and research. Modularity and simplicity are very important for this pur- pose because they allow students to begin working on projects without an understanding of the entire design and all its details, and they permit several concurrent student projects. </p>
<p>其次，它的设计简单，方便学生使用和研究。 模块化和简单性对于此目的非常重要，因为它们允许学生在不了解整个设计及其所有细节的情况下开始项目工作，并且允许多个并发的学生项目。</p>
<p>Third, Volcano’s design does not presume any particular data model; the only assumption is that query processing is based on transforming sets of items using parameterized operators. To achieve data model indepen- dence, the design very consistently separates set process- ing control (which is provided and inherent in the Vol- cano operators) from interpretation and manipulation of data items (which is imported into the operators, as de- scribed later). </p>
<p>第三，Volcano的设计没有假设任何特定的数据模型； 唯一的假设是查询处理基于使用参数化运算符转换项目集。 为了实现数据模型的独立性，设计非常一致地将集合处理控制（由 Volcano 运算符提供和固有的）与数据项的解释和操作（如所述导入到运算符中）分开。 之后）。</p>
<p>Fourth, to free algorithm design, imple- mentation, debugging, tuning, and initial experimentation from the intricacies of parallelism but to allow experi- mentation with parallel query processing. Volcano can be used as a single-process or as a parallel system. Single- process query evaluation plans can already be parallelized easily on shared-memory machines and soon also on dis- tributed-memory machines. </p>
<p>第四，将算法设计、实现、调试、调整和初始实验从复杂的并行性中解放出来，但允许进行并行查询处理的实验。 Volcano 可以用作单进程或并行系统。 单进程查询评估计划已经可以在共享内存机器上轻松并行化，并且很快也可以在分布式内存机器上并行化。</p>
<p>Fifth, Volcano is realistic in its query execution paradigm to ensure that students learn how query processing is really done in commercial data- base products. For example, using temporary files to transfer data from one operation to the next as suggested in most textbooks has a substantial performance penalty, and is therefore used in neither real database systems nor in Volcano. </p>
<p>第五，Volcano 的查询执行范例是现实的，以确保学生了解商业数据库产品中的查询处理是如何真正完成的。 例如，按照大多数教科书的建议，使用临时文件将数据从一个操作传输到下一个操作会带来很大的性能损失，因此既不在真正的数据库系统中使用，也不在 Volcano 中使用。</p>
<p>Finally, Volcano’s means for parallel query processing could not be based on existing models since all models explored to date have been designed with a particular data model and operator set in mind. Instead, our design goal was to make parallelism and data manip- ulation orthogonal, which means that the mechanisms for parallel query processing are independent of the operator set and semantics, and that all operators, including new ones, could be designed and implemented independently of future parallel execution.</p>
<p>最后，Volcano 的并行查询处理方法不能基于现有模型，因为迄今为止探索的所有模型都是在设计时考虑了特定的数据模型和运算符集。 相反，我们的设计目标是使并行性和数据操作正交，这意味着并行查询处理的机制独立于操作符集和语义，并且所有操作符，包括新操作符，都可以独立于操作符集和语义来设计和实现。 未来的并行执行。</p>
<p>Following a design principle well established in operating systems research but not exploited in most database system designs, Volcano provides mechanisms to support policies. Policies can be set by a human experimenter or by a query optimizer. The separation of mechanisms and policies has contributed to the extensibility and modular- ity of modern operating systems, and may make the same contribution to extensible database systems. We will re- turn to this separation repeatedly in this paper.</p>
<p>Volcano 遵循操作系统研究中确立但大多数数据库系统设计中未采用的设计原则，提供了支持策略的机制。 策略可以由人类实验者或查询优化器设置。 机制和策略的分离有助于现代操作系统的可扩展性和模块化，并且可能对可扩展数据库系统做出同样的贡献。 我们将在本文中反复讨论这种分离。</p>
<p>Since its very purpose is to allow future extensions and research, Volcano is continuously being modified and ex- tended. Among the most important recent extensions were the design and implementation of two meta-operators. Both of them are not only new operators but also embody and encapsulate new concepts for query processing. They are meta-operators since they do not contribute to data manipulation, selection, derivation, etc., but instead pro- vide additional control over query processing that cannot be provided by conventional operators like file scan, sort, and merge join. The choose-plan operator implements dynamic query evaluation plans, a concept developed for queries that must be optimized with incomplete informa- tion [ 171. For example, it is not possible to reliably op- timize an embedded query if one of the constants in the query predicate is actually a program variable and there- fore unknown during compilation and optimization. Dy- namic plans allow preparation for multiple equivalent plans, each one optimal for a certain range of actual pa- rameter values. The choose-plan operator selects among these plans at runtime while all other operators in Vol- cano’s operator set (present or future) are entirely oblivious to the presence and function of the choose-plan operator.</p>
<p>由于其目的是为了未来的扩展和研究，Volcano 正在不断地被修改和扩展。 最近最重要的扩展是两个元运算符的设计和实现。 它们不仅是新的运算符，而且体现和封装了查询处理的新概念。 它们是元运算符，因为它们不参与数据操作、选择、派生等，而是提供对查询处理的额外控制，这是文件扫描、排序和合并连接等传统运算符无法提供的。 选择计划运算符实现动态查询评估计划，这是一个为必须使用不完整信息进行优化的查询而开发的概念[171。例如，如果常量之一，则不可能可靠地优化嵌入式查询 查询谓词中的实际上是一个程序变量，因此在编译和优化期间是未知的。 动态计划允许准备多个等效计划，每个计划对于特定范围的实际参数值都是最佳的。 选择计划运算符在运行时在这些计划中进行选择，而 Volcano 运算符集中的所有其他运算符（当前或未来）完全不知道选择计划运算符的存在和功能。</p>
<p>The second meta-operator, the exchange operator, im- plements and controls parallel query evaluation in Vol- cano. While operators can exchange data without the ex- change operator, in fact within processes as easily as a single procedure call, this new operator exchanges data across process and processor boundaries. All other oper- ators are implemented and execute without regard to par- allelism; all parallelism issues like partitioning and flow control are encapsulated in and provided by the exchange operator. Thus, data manipulation and parallelism are in- deed orthogonal in Volcano [20]. Beyond the cleanliness from a software engineering point of view, it is also very encouraging to see that this method of parallelizing a query processing engine does indeed allow linear or near- linear speedup .</p>
<p>第二个元运算符，即交换运算符，在 Volcano 中实现和控制并行查询评估。 虽然操作符可以在没有交换操作符的情况下交换数据，实际上在进程内就像单个过程调用一样容易，但这个新操作符可以跨进程和处理器边界交换数据。 所有其他运算符的实现和执行均不考虑并行性； 所有并行问题（例如分区和流量控制）都封装在交换运算符中并由交换运算符提供。 因此，数据操作和并行性在 Volcano 中确实是正交的[20]。 除了从软件工程角度来看的简洁性之外，看到这种并行化查询处理引擎的方法确实允许线性或近线性加速也非常令人鼓舞。</p>
<p>This paper is a general overview describing the overaii goals and design principles. Other articles on Volcano were written on special aspects of the system, e.g., [16]- [21], [25], [26]. These articles also include experimental performance evaluations of Volcano’s techniques and algorithms, in particular [ 181, [2 11. </p>
<p>本文是描述总体目标和设计原则的总体概述。 关于 Volcano 的其他文章是关于系统的特殊方面的，例如 [16]-[21]、[25]、[26]。 这些文章还包括 Volcano 技术和算法的实验性能评估，特别是 [181, [2 11.</p>
<p>The present paper is organized as follows. In the following section, we briefly review previous work that inluenced Volcano’s design. A detailed description of Vol- cano follows in Section III. Section IV contains a discussion of extensibility in the system. Dynamic query evaluation plans and their implementation are described in Section V. Parallel processing encapsulated in the ex- change module is described in Section VI. Section VII contains a summary and our conclusions from this effort.</p>
<p>本文的结构如下。 在下一节中，我们简要回顾一下影响 Volcano 设计的先前工作。 第三节对 Volcano 进行了详细描述。 第四节讨论了系统的可扩展性。 动态查询评估计划及其实现在第五节中描述。封装在交换模块中的并行处理在第六节中描述。 第七节包含总结和我们从这项工作中得出的结论。</p>
<h3 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h3><p>Since so many different systems have been developed to process large datesets efficiently, we only survey the systems that have significantly influenced the design of Volcano. Our work has been influenced most strongly by WiSS, GAMMA, and EXODUS. The Wisconsin Storage System (WiSS) [lo] is a record-oriented file system pro- viding heap files, B-tree and hash indexes, buffering, and scans with predicates. GAMMA [1l] is a software data- base machine running on a number of general-purpose CPU’s as a backend to a UNIX host machine. It was de- veloped on 17 VAX 11&#x2F;750’s connected with each other and the VAX 11&#x2F;750 host via a 80 Mb &#x2F;s token ring. Eight GAMMA processors had a local disk device, accessed us- ing WiSS. The disks were accessible only locally, and update and selection operators used only these eight pro- cessors. The other, diskless processors were used for join processing. Recently, the GAMMA software has been ported to an Intel iPSC&#x2F;2 hypercube with 32 nodes, each with a local disk drive. GAMMA uses hash-based algo- rithms extensively, implemented in such a way that each operator is executed on several (usually all) processors and the input stream for each operator is partitioned into disjoint sets according to a hash function.</p>
<p>由于已经开发了许多不同的系统来有效地处理大型数据集，因此我们只调查对 Volcano 设计有重大影响的系统。 我们的工作受 WiSS、GAMMA 和 EXODUS 的影响最为强烈。 威斯康星存储系统 (WiSS) [lo] 是一个面向记录的文件系统，提供堆文件、B 树和哈希索引、缓冲和谓词扫描。 GAMMA [1l] 是一个软件数据库机，运行在许多通用 CPU 上，作为 UNIX 主机的后端。 它是在 17 个 VAX 11&#x2F;750 上开发的，这些 VAX 11&#x2F;750 彼此之间以及 VAX 11&#x2F;750 主机通过 80 Mb &#x2F;s 令牌环连接。 八个 GAMMA 处理器有一个本地磁盘设备，可以使用 WiSS 进行访问。 磁盘只能在本地访问，更新和选择操作员仅使用这八个处理器。 其他无盘处理器用于连接处理。 最近，GAMMA 软件已被移植到具有 32 个节点的 Intel iPSC&#x2F;2 hypercube，每个节点都有一个本地磁盘驱动器。 GAMMA 广泛使用基于散列的算法，其实现方式是每个运算符在多个（通常是所有）处理器上执行，并且每个运算符的输入流根据散列函数划分为不相交的集合。</p>
<p>The limited data model and extensibility of GAMMA led to the search for a more flexible but equally powerful query processing model. The operator design used in the GAMMA database machine software gives each operator control within its own process, leaving it to the network- ing and operating system software to synchronize multi- ple operators in producer-consumer relationships using flow-control mechanisms. This design, while working ex- tremely well in GAMMA, does not lend itself to single- process query evaluation since multiple loci of control, i.e., multiple operators, cannot be realized inside a single process without special pseudo-multiprocess mechanisms such as threads. Therefore, GAMMA’s operator and data transfer concepts are not suitable for an efficient query processing engine intended for both sequential and par- allel query execution.</p>
<p>GAMMA 有限的数据模型和可扩展性导致人们寻找更灵活但同样强大的查询处理模型。 GAMMA数据库机器软件中使用的操作员设计使每个操作员能够在自己的流程中进行控制，将其留给网络和操作系统软件使用流量控制机制来同步生产者-消费者关系中的多个操作员。 这种设计虽然在 GAMMA 中工作得非常好，但不适合单进程查询评估，因为如果没有特殊的伪多进程机制（例如线程），就无法在单个进程内实现多个控制点（即多个运算符）。 因此，GAMMA 的运算符和数据传输概念不适合用于顺序和并行查询执行的高效查询处理引擎。</p>
<p>EXODUS [7] is an extensible database system with some  components followiong the “tool-kit” approach, e.g., the optimizer generator [ 131, [ 141 and the E database implementation language [27], [28], and other compo- nents built as powerful but fixed components, e.g., the storage manager [5]. Originally, EXODUS was conceived to be data-model-indepedent, i.e., it was supposed to support a wide variety of data models, but later a novel,powerful,structurally object-oriented data model called Extra was developed. The concept of data model independence as first explored in EXODUS has been retained in the Volcano project and the design and implementation of its software.During the design of the EXODUS storage manager, many storage and access issues explored in WiSS and GAMMA were revisited. Lessons learned and trade-offs explored in these discussions certainly helped in forming the ideas behind Volcano. The  design and development of E influenced the strong emphasis on iterators for query processing.</p>
<p>EXODUS [7]是一个可扩展的数据库系统，具有一些遵循“工具包”方法的组件，例如优化器生成器[131，[141]和E数据库实现语言[27]，[28]和其他组件 构建为功能强大但固定的组件，例如存储管理器 [5]。 最初，EXODUS 被认为是独立于数据模型的，即它应该支持多种数据模型，但后来开发了一种新颖的、强大的、结构面向对象的数据模型，称为 Extra。 EXODUS 中首次探索的数据模型独立性概念已保留在 Volcano 项目及其软件的设计和实现中。在 EXODUS 存储管理器的设计过程中，重新审视了 WiSS 和 GAMMA 中探索的许多存储和访问问题。 在这些讨论中吸取的经验教训和探索的权衡无疑有助于形成 Volcano 背后的想法。 E 的设计和开发影响了对用于查询处理的迭代器的强烈重视。</p>
<p>A number of further conventional (relational) and extensible systems have influenced our design. Ingres [32] and System R [9] have probably influenced most database systems, in particular their extensible follow-on projects  Starburst [23] and Postgres [35]. It is interesting to note  that independently of our work the Starburst group has also identified the demand-driven interator paradigm as a suitable basis for an extensible single-process query evaluation architecture after using it successfully in the System R relational system, but as yet has not been able to combine extensibility with parallelism. GENESIS [l] early on stressed the importance of uniform operator in- terfaces for extensibility and software reusability.</p>
<p>许多进一步的传统（关系）和可扩展系统影响了我们的设计。 Ingres [32] 和 System R [9] 可能影响了大多数数据库系统，特别是它们的可扩展后续项目 Starburst [23] 和 Postgres [35]。 有趣的是，独立于我们的工作，Starburst 小组在 System R 关系系统中成功使用需求驱动的交互器范例后，也将其确定为可扩展的单进程查询评估架构的合适基础，但迄今为止 无法将可扩展性与并行性结合起来。 GENESIS [l]很早就强调了统一操作员界面对于可扩展性和软件可重用性的重要性。</p>
<p>XPRS has been the first project aiming to combine extensibility with parallelism [34]. Its basic premise is to implement Postgres on top of RAID disk arrays and the Sprite operating system. XPRS and GAMMA basically differ in four ways. First, GAMMA supports. a purely relational data model while XPRS supports an extensible relational model, Postgres. Second, GAMMA’s main form of parallelism is intra-operator parallelism based on partitioned data sets. XPRS, on the other hand, will rely on bushy parallelism, i.e. , concurrent execution of different subtrees in a complex query evaluation plan. Fourth, GAMMA is built on the premise that distributed memory is required to achieve scalable linear speed-up while XPRS is being im- plemented on a shared-memory machine.</p>
<p>XPRS 是第一个旨在将可扩展性与并行性相结合的项目 [34]。 其基本前提是在 RAID 磁盘阵列和 Sprite 操作系统之上实现 Postgres。 XPRS 和 GAMMA 基本上有四个方面的不同。 首先，GAMMA支持。 纯粹的关系数据模型，而 XPRS 支持可扩展的关系模型 Postgres。 其次，GAMMA 的主要并行形式是基于分区数据集的算子内并行。 另一方面，XPRS 将依赖于密集并行性，即复杂查询评估计划中不同子树的并发执行。第四，GAMMA 的构建前提是需要分布式内存来实现可扩展的线性加速，而 XPRS 是在共享内存机器上实现的。</p>
<p>Both XPRS and Volcano combine parallelism and ex- tensibility, but XPRS is a far more comprehensive project than Volcano. In particular, XPRS includes a data model and a query optimizer. On the other hand, Volcano is more extensible precisely because it does not presume a data model. Therefore, Volcano could be used as the query processing engine in a parallel extensible-relational sys- tem such as XPRS. Moreover, it will eventually include a data-model-independent optimizer generator to form a complete query processing research environment.</p>
<p>XPRS 和 Volcano 都结合了并行性和可扩展性，但 XPRS 是一个比 Volcano 更全面的项目。 具体来说，XPRS 包括数据模型和查询优化器。 另一方面，Volcano 更具有可扩展性，正是因为它不假定数据模型。 因此，Volcano 可以用作并行可扩展关系系统（例如 XPRS）中的查询处理引擎。 而且，它最终将包括一个独立于数据模型的优化器生成器，以形成一个完整的查询处理研究环境。</p>
<h3 id="III-VOLCANO-SYSTEM-DESIGN"><a href="#III-VOLCANO-SYSTEM-DESIGN" class="headerlink" title="III. VOLCANO SYSTEM DESIGN"></a>III. VOLCANO SYSTEM DESIGN</h3><p>In this section, we provide an overview of the design of Volcano. At the current time, Volcano is a library of about two dozen modules with a total of about 15 000 lines of C code. These modules include a file system, buffer management, sorting, B+-trees, and two algorithms each (sort- and hash-based) for natural join, semi-join, all three outer joins, anti-joint, aggregation, duplicate elimination, union, intersection, difference, anti-difference,and relational division. Moreover, two modules implement dynamic query evaluation plans and allow parallel processing of all algorithms listed above.</p>
<p>在本节中，我们概述了 Volcano 的设计。 目前，Volcano 是一个包含大约两打模块的库，总共大约有 15,000 行 C 代码。 这些模块包括文件系统、缓冲区管理、排序、B+树以及两种算法（基于排序和散列），用于自然连接、半连接、所有三个外连接、反连接、聚合、重复消除、 并集、交集、差集、反差集、关系除法。 此外，两个模块实现动态查询评估计划并允许并行处理上面列出的所有算法。</p>
<p>All operations on individual records are deliberately  left open for later definition. Instead of inventing a language in which to specify selection predicates, hash functions,etc., functions are passed to the query processing operators to be called when necessary with the appropriate arguments.These support functions are descibed later in more detail One common and repeating theme in the design of Volcano is that it provides mechanisms for query evaluation to allow selection of and experimentation with policies. The separation of mechanisms and policies is  a very well-known and well-established principle in the design and implementation of operating systems, but it has not been used as extensively and consistently in the design and implementation of database systems. It has contributed significantly to the extensibility and modularity of modem operating systems, and may make the same contribution to extensible database systems.</p>
<p>对单个记录的所有操作都故意保留以供以后定义。 不是发明一种语言来指定选择谓词、散列函数等，而是将函数传递给查询处理运算符，以便在必要时使用适当的参数进行调用。稍后将更详细地描述这些支持函数。 一个常见且重复的主题 Volcano 设计的一个亮点是它提供了查询评估机制，以允许选择和试验策略。 机制和策略的分离是操作系统设计和实现中众所周知且公认的原则，但它在数据库系统的设计和实现中尚未得到广泛和一致的使用。 它对现代操作系统的可扩展性和模块化做出了重大贡献，并且可能对可扩展数据库系统做出同样的贡献。</p>
<p>Currently, Volcano consists of two layers, the file system layer and the query processing layer. The former provides record, file, and index operations including scans with optional predicates, and buffering; the latter is a set of query processing modules that can be nested to build complex query evaluation trees. Fig. 1 identifies Volcano’s main modules. This separation can be found in most query evaluation systems, e.g., RSS and RDS in System R [9] and Core and Corona in Starburst [23]. System catalogs or a data dictionary are not included in Volano since the system was designed to be extensible and independent from any particular data model. We start our descirption at the bottom, the file system, and then discuss the query processing modules.</p>
<p>目前，Volcano由两层组成，文件系统层和查询处理层。 前者提供记录、文件和索引操作，包括带有可选谓词的扫描和缓冲； 后者是一组查询处理模块，可以嵌套构建复杂的查询评估树。 图 1 标识了 Volcano 的主要模块。 这种分离可以在大多数查询评估系统中找到，例如 System R [9] 中的 RSS 和 RDS 以及 Starburst [23] 中的 Core 和 Corona。 Volano 中不包含系统目录或数据字典，因为该系统被设计为可扩展且独立于任何特定数据模型。 我们从最底层的文件系统开始描述，然后讨论查询处理模块。</p>
<p><img src="/img/volcano-pic/image-20230720151302454.png"></p>
<h4 id="A-The-File-System"><a href="#A-The-File-System" class="headerlink" title="A. The File System"></a>A. The File System</h4><p>Within our discussion of the Volcano file system, we also proceed bottom-up, from buffer management to data files and indices. The existing facilities are meant to pro- vide a backbone of a query processing system, and are designed such that modifications and additions can easily be accomplished as the need arises. The buffer manager is the most interesting part of the file system. Because buffer management is performance- critical in any database system, the Volcano buffer man- ager was designed to include mechanisms that can be used most effectively and efficiently in a large variety of con- texts and with a wide array of policies. In consequence, its features include multiple buffer pools, variable-length units of buffering that are called clustres in Volcano, and replacement hints from the next higher software level.</p>
<p>在我们对 Volcano 文件系统的讨论中，我们也是自下而上进行的，从缓冲区管理到数据文件和索引。 现有设施旨在提供查询处理系统的骨干，其设计使得可以在需要时轻松完成修改和添加。缓冲区管理器是文件系统中最有趣的部分。 由于缓冲区管理在任何数据库系统中都对性能至关重要，因此 Volcano 缓冲区管理器被设计为包含可在各种上下文和各种策略中最有效和高效地使用的机制。 因此，它的功能包括多个缓冲池、可变长度缓冲单元（在 Volcano 中称为簇）以及来自下一个更高软件级别的替换提示。</p>
<p>The buffer manager’s hint facility is an excellent example of Volcano’s design principle to implement mechanisms to suppent multiple policies. The buffer manager only provides the mechanisms, i.e.,pinning, page replacement, and reading and writing disk pages, while the higher level software determines the policies depending on data semantics, importance, and access patterns. It is surprising that database buffer managers derive replace- ment decisions from observed reference behavior in spite of the fact that this behavior is generated by higher level database software and thus known and foreseeable in ad- vance within the same system, albeit different subcom- ponents.</p>
<p>缓冲区管理器的提示功能是 Volcano 设计原则的一个很好的例子，它实现了支持多种策略的机制。 缓冲区管理器仅提供机制，即固定、页面替换以及读写磁盘页面，而更高级别的软件根据数据语义、重要性和访问模式确定策略。 令人惊讶的是，数据库缓冲区管理器从观察到的参考行为中得出替换决策，尽管这种行为是由更高级别的数据库软件生成的，因此在同一系统内（尽管子组件不同）是已知的和可预见的。 </p>
<p>Files are composed of records, clusters, and extents. Since file operations are invoked very frequently in any database system, all design decisions in the file module have been made to provide basic functionality with the highest attainable performance. A cluster, consisting of one or more pages, is the unit of I&#x2F;O and of buffering, as discussed above. The cluster size is set for each file in- dividually. Thus, different files on the same device can have different cluster sizes. Disk space for files is allo- cated in physically contiguous extents, because extents allow very fast scanning without seeks and large-chunk read-ahead and write-behind.</p>
<p>文件由记录、簇和范围组成。 由于文件操作在任何数据库系统中都会非常频繁地调用，因此文件模块中的所有设计决策都是为了提供具有最高可达到性能的基本功能。 如上所述，簇由一页或多页组成，是 I&#x2F;O 和缓冲的单位。 簇大小是为每个文件单独设置的。 因此，同一设备上的不同文件可以具有不同的簇大小。 文件的磁盘空间被分配在物理上连续的扩展区中，因为扩展区允许非常快速的扫描而无需查找以及大块的预读和后写。</p>
<p>Records are identified by a record identifier (RID), and can be accessed directly using the RID. For fast access to a large set of records, Volcano supports not only individ- ual file and record operations but also scans that support read-next and append operations. There are two interfaces to file scans; one is part of the file system and is described momentarily; the other is part of the query processing level and is described later. The first one has the standard procedures for file scans, namely open, next, close, and rewind. The next procedure returns the main memory ad- dress of the next record. This address is guaranteed (pinned) until the next operation is invoked on the scan.Thus, getting the next record within the same cluster does not require calling the buffer manager and can be per- formed very efficiently.</p>
<p>记录由记录标识符(RID) 标识，并且可以使用RID 直接访问。 为了快速访问大量记录，Volcano 不仅支持单个文件和记录操作，还支持支持读取下一个和追加操作的扫描。 文件扫描有两个接口； 一个是文件系统的一部分，稍后进行描述； 另一个是查询处理级别的一部分，稍后介绍。 第一个具有文件扫描的标准过程，即 open、next、close和 rewind。 next 过程返回下一条记录的主存地址。 在扫描中调用下一个操作之前，该地址得到保证（固定）。因此，获取同一簇内的下一条记录不需要调用缓冲区管理器，并且可以非常高效地执行。</p>
<p>For fast creation of files, scans support an append op- eration. It allocates a new record slot, and returns the new slot’s main memory address. It is the caller’s responsibil- ity to fill the provided record space with useful information, i.e., the append rourine is entirely oblivious to the data and their representation.</p>
<p>为了快速创建文件，扫描支持追加操作。 它分配一个新的记录槽，并返回新槽的主内存地址。 调用者有责任用有用的信息填充提供的记录空间，即附加例程完全不关心数据及其表示。</p>
<p>Scans also support optional predicates. The predicate function is called by the next procedure with the argument and a record address. Selective scans are the first example of support functions mentioned briefly in the introduction. Instead of determining a qualification itself, the scan mechanism relies on a predicate function imported from a higher level.</p>
<p>扫描还支持可选谓词。 谓词函数由下一个过程使用参数和记录地址调用。 选择性扫描是简介中简要提到的支持功能的第一个示例。 扫描机制不是确定限定本身，而是依赖于从更高级别导入的谓词函数。</p>
<p>Support functions are passed to an operation as a function entry point and a typeless pointer that serves as a predicate argument. Arguments to support functions can be used in two ways, namely in compiled and interpreted query execution. In compiled scans, i.e., when the pred- icate evaluation function is available in macvhine code, the argument can be used to pass a constant or a pointer to several constants to the predicate function. For exam- ple, if the predicate consists of comparing a record field with a string, the comparison function is passed as pred- icate function while the search string is passed as predi- cate argument. In interpreted scans, i.e., when a general interpreter is used to evaluate all predicates in query, they can be used to pass appropriate code to the interpreter. The interpreter’s entry point is given as predicate func- tion. Thus, both interpreted and compiled scans are sup- ported with a single simple and efficient mechanism. Vol- cano’s use of support functions and their arguments is another example for a mechanism that leaves a policy de- cision, in this case whether to use compiled or interpreted scans, open to be decided by higher level software.</p>
<p>支持函数作为函数入口点和充当谓词参数的无类型指针传递给操作。 支持函数的参数可以通过两种方式使用，即在编译和解释查询执行中。 在编译扫描中，即当谓词求值函数在 macvhine 代码中可用时，参数可用于将常量或指向多个常量的指针传递给谓词函数。 例如，如果谓词包括将记录字段与字符串进行比较，则比较函数作为谓词函数传递，而搜索字符串作为谓词参数传递。 在解释扫描中，即当使用通用解释器来评估查询中的所有谓词时，它们可用于将适当的代码传递给解释器。 解释器的入口点作为谓词函数给出。 因此，解释扫描和编译扫描都由一个简单而有效的机制支持。 Volcano 对支持函数及其参数的使用是一种机制的另一个例子，该机制将策略决策（在本例中是使用编译扫描还是解释扫描）由更高级别的软件决定。</p>
<p>indices are implemented currently only in the form of B + -trees with an interface similar to files. A leaf entry consists of a key and information. The information part typically is a RID, but it could include more or different information. The key and the information can be of any type; a comparison function must be provided to compare keys. The comparison function uses an argument equiv- alent to the one described for scan predicates. Permitting any information in the leaves gives more choices in phys- ical database design. It is another example of Volcano providing a mechanism to allow a multitude of designs and usage policies. B + -trees support scans similar to files, including predicates and append operations for fast loading In addition, B+ -tree scans allow seeking to a partic- ular key, and setting lower and upper bounds.</p>
<p>索引目前仅以 B+ 树的形式实现，其接口类似于文件。 叶条目由密钥和信息组成。 信息部分通常是 RID，但它可以包括更多或不同的信息。 密钥和信息可以是任何类型； 必须提供比较函数来比较键。 比较函数使用的参数与扫描谓词所描述的参数等效。 允许叶子中存在任何信息可以为物理数据库设计提供更多选择。 这是 Volcano 提供允许多种设计和使用策略的机制的另一个例子。 B+ 树支持与文件类似的扫描，包括用于快速加载的谓词和附加操作。此外，B+ 树扫描允许查找特定键，并设置下限和上限。</p>
<p>For intermediate results in query processing (later called streams), Volcano uses special devices called virtual de- vices. The difference between virtual and disk devices is that data pages of virtual devices only exist in the buffer. As soon as such data pages are unpinned, they disappear and their contents are lost. Thus, Volcano uses the same mechanisms and function calls for permanent and inter- mediate data sets, easing implementation of new opera- tors significantly.</p>
<p>对于查询处理中的中间结果（后来称为流），Volcano 使用称为虚拟设备的特殊设备。 虚拟设备和磁盘设备的区别在于虚拟设备的数据页只存在于缓冲区中。 一旦此类数据页被取消固定，它们就会消失并且其内容也会丢失。 因此，Volcano 对永久和中间数据集使用相同的机制和函数调用，从而显着简化了新运算符的实现。</p>
<p>In summary, much of Volcano’s file system is conven- tional in its goals but implemented in a flexible, efficient, and compact way. The file system supports basic abstractions and operations, namely devices, files, records,B+-trees, and scans. It provides mechanisms to access these objects, leaving many policy decisions to higher level software. High performance was a very important goal in the design and implementation of these mecha- nisms since performance studies and parallelization only make sense if the underlying mechanisms are efficient. Furthermore, research into implementation and perfor- mance trade-offs for extensible database systems and new data models is only relevant if an efficient evaluation platform is used.</p>
<p>总之，Volcano 的文件系统的大部分目标都是传统的，但以灵活、高效和紧凑的方式实现。 文件系统支持基本的抽象和操作，即设备、文件、记录、B+树和扫描。 它提供了访问这些对象的机制，将许多策略决策留给更高级别的软件。 高性能是这些机制的设计和实现中的一个非常重要的目标，因为只有当底层机制高效时，性能研究和并行化才有意义。 此外，只有使用有效的评估平台，对可扩展数据库系统和新数据模型的实施和性能权衡的研究才有意义。</p>
<h4 id="B-Query-Processing"><a href="#B-Query-Processing" class="headerlink" title="B. Query Processing"></a>B. Query Processing</h4><p>The file system routines described above are utilized by the query processing routines to evaluate complex quer- ies. Queries are expressed as query plans or algebra expressions; the operators of this algebra are query pro- cessing algorithms and we call the algebra an executable algebra as opposed to logical algebras, ‘e .g . , relational algebra. We will describe the operations using relational terminology hoping that this will assist the reader.We must point out, however, that the operations can be viewed and are implemented as operations on sets of objects, and that Volcano does not depend on assumptions about the internal structure of such objects. In fact, we intend to use Volcano for query processing in an object- oriented database system [ 151. The key to this use of Vol- cano is that set processing and interpretation of data items are separated.</p>
<p>查询处理例程利用上述文件系统例程来评估复杂的查询。 查询被表示为查询计划或代数表达式； 该代数的运算符是查询处理算法，我们将该代数称为可执行代数，而不是逻辑代数，“例如” ，关系代数。 我们将使用关系术语来描述这些操作，希望这会对读者有所帮助。但是，我们必须指出，这些操作可以被视为并实现为对对象集的操作，并且 Volcano 不依赖于有关内部对象的假设。 此类对象的结构。 事实上，我们打算在面向对象的数据库系统中使用 Volcano 进行查询处理[151。使用 Volcano 的关键在于数据项的集合处理和解释是分离的。</p>
<p>In Volcano, all algebra operators are implemented as iterators, i.e., they support a simple open-next-close pro- tocol. Basically, iterators provide the iteration component of a loop, i.e., initialization, increment, loop termination condition, and final housekeeping. These functions allow “iteration’ ’ over the results of any operation similar to the iteration over the result of a conventional file scan.Associated with each iterator is a state record type. A state record contains arguments, e.g., the size of a hash table to be allocated in the open procedure, and state, e.g., the location of a hash table. All state information of an iterator is kept in its state record and there are no ‘ ‘static’ ’ variables; thus, an algorithm may be used multiple times in a query by including more than one state record in the query.</p>
<p>在 Volcano 中，所有代数运算符都被实现为迭代器，即它们支持简单的 open-next-close 协议。 基本上，迭代器提供循环的迭代组件，即初始化、增量、循环终止条件和最终内务处理。 这些函数允许对任何操作的结果进行“迭代”，类似于对传统文件扫描的结果进行迭代。与每个迭代器相关联的是一个状态记录类型。 状态记录包含参数（例如要在打开过程中分配的哈希表的大小）和状态（例如哈希表的位置）。 迭代器的所有状态信息都保存在其状态记录中，并且不存在“静态”变量； 因此，通过在查询中包括多于一个状态记录，可以在查询中多次使用算法。</p>
<p>All manipulation and interpretation of data objects,e.g., comparisons and hashing, is passed to the iterators by means of pointers to the entry points of appropriate support functions. Each of these support functions uses an argument allowing interpreted or compiled query eval- uation, as described earlier for file scan predicates. With- out the support functions, Volcano’s iterators are empty algorithm shells that cannot perform any useful work. In effect, the split into algorithm shells and support functions separates control and iteration over sets from interpreta- tion of records or objects. This separation is one of the cornerstones’ of Volcano’s data model independent and extensibility, which will be discussed in Section IV.</p>
<p>数据对象的所有操作和解释，例如比较和散列，都通过指向适当支持函数入口点的指针传递给迭代器。 这些支持函数中的每一个都使用一个允许解释或编译查询评估的参数，如前面针对文件扫描谓词所述。 如果没有支持函数，Volcano 的迭代器就是空的算法外壳，无法执行任何有用的工作。 实际上，分成算法外壳和支持函数将集合的控制和迭代与记录或对象的解释分开。 这种分离是 Volcano 数据模型独立性和可扩展性的基石之一，这将在第四节中讨论。</p>
<p>Iterators can be nested and then operate similarly to coroutines. State records are linked together by means of input pointers. The input pointers are also kept in the state records. Fig. 2 shows two operators in a query evaluation plan. Purpose and capabilities of the Jilter operator will be discussed shortly; one of its possible functions is to print items of a stream using a function passed to the filter operator as one of its arguments. The structure at the top gives access to the functions as well as to the state record. Using a pointer to this structure, the filter functions can be called and their local state can be passed to them as a procedure argument. The functions themselves, e.g., open–filter, can use the input pointer contained in the state record to invoke the input operator’s functions. Thus, the filter functions can invoke the file scan functions as needed, and can pace the file scan according to the needs of the filter. In other words, Fig. 2 shows a complete query evaluation plan that prints selected records from a file.</p>
<p>迭代器可以嵌套，然后与协程类似地进行操作。 状态记录通过输入指针链接在一起。 输入指针也保存在状态记录中。 图 2 显示了查询评估计划中的两个运算符。 Jilter 操作员的目的和功能将很快讨论； 它的可能功能之一是使用传递给过滤器运算符作为其参数之一的函数来打印流的项目。 顶部的结构可以访问函数以及状态记录。 使用指向该结构的指针，可以调用过滤器函数，并且可以将它们的本地状态作为过程参数传递给它们。 函数本身（例如 open–filter）可以使用状态记录中包含的输入指针来调用输入运算符的函数。 因此，过滤器函数可以根据需要调用文件扫描函数，并且可以根据过滤器的需要调整文件扫描的速度。 换句话说，图 2 显示了一个完整的查询评估计划，该计划打印从文件中选择的记录。</p>
<p><img src="/img/volcano-pic/image-20230720152944923.png"></p>
<p>Using Volcano’s standard form of iterators, an operator does not need to know what kind of operator produces its input, or whether its input comes from a complex query tree or from a simple file scan. We call this concept anon- ymous inputs or streams. Streams are a simple but pow- erful abstraction that allows combining any number and any kind of operators to evaluate a complex query, a sec- ond cornerstone to Volcano’s extensibility. Together with the iterator control paradigm, streams represent the most efficient execution model in terms of time (overhead for synchronizing operators) and space (number of records that must reside in memory concurrently) for single-pro- cess query evaluation.</p>
<p>使用 Volcano 的标准形式的迭代器，操作符不需要知道哪种操作符产生其输入，或者其输入是来自复杂的查询树还是来自简单的文件扫描。 我们将这个概念称为匿名输入或流。 流是一种简单但功能强大的抽象，它允许组合任意数量和任意类型的运算符来评估复杂的查询，这是 Volcano 可扩展性的第二个基石。 与迭代器控制范例一起，流代表了单进程查询评估的时间（同步运算符的开销）和空间（必须同时驻留在内存中的记录数量）方面最有效的执行模型。</p>
<p>Calling open for the top-most operator results in instan- tiations for the associated state record’s state, e.g., allo- cation of a hash table, and in open calls for all inputs. In this way, all iterators in a query are initiated recursively. In order to process the query, next for the top-most op- erator is called repeatedly until it fails with an end-of- stream indicator. The top-most operator calls the next procedure of its input if it needs more input data to pro- duce an output record. Finally, the close call recursively “shuts down” all iterators in the query. This model of query execution matches very closely the ones being in- eluded in the E database implementation language in EX- ODUS and the query executor of the Starburst relational database system.</p>
<p>对最顶层运算符调用 open 会导致关联状态记录的状态实例化，例如哈希表的分配，以及对所有输入的 open 调用。 这样，查询中的所有迭代器都会递归启动。 为了处理查询，重复调用最顶层运算符的 next ，直到它失败并出现流结束指示符。 如果需要更多输入数据来生成输出记录，最顶层的运算符将调用其输入的下一个过程。 最后，关闭调用递归地“关闭”查询中的所有迭代器。 这种查询执行模型与 EXODUS 中的 E 数据库实现语言和 Starburst 关系数据库系统的查询执行器中包含的模型非常匹配。</p>
<p>A number of query and environment parameters may nfluence policy decisions during opening a query evaluation plan, e.g., query predicate constants and system load information. Such parameters are passed between all open procedures in Volcano with a parameter called bindings. This is a typeless pointer that can be used to pass infor- mation for policy decisions. Such policy decisions are im- plemented using support functions again. For example, the module implementing hash join allows dynamic de- termination of the size of a hash table-another example of the separation of mechanism and policy. This bindings parameter is particularly useful in dynamic query evalu- ation plans, which will be discussed later in Section V.</p>
<p>许多查询和环境参数可能会影响打开查询评估计划期间的策略决策，例如查询谓词常量和系统负载信息。 此类参数通过称为绑定的参数在 Volcano 中的所有打开过程之间传递。 这是一个无类型指针，可用于传递策略决策信息。 此类政策决策再次使用支持功能来实施。 例如，实现散列连接的模块允许动态确定散列表的大小——机制和策略分离的另一个例子。 这个绑定参数在动态查询评估计划中特别有用，稍后将在第五节中讨论。</p>
<p>The tree-structured query evaluation plan is used to ex- ecute queries by demand-driven dataflow. The return value of a next operation is, besides a status indicator, a structure called Next-Record, which consists of an RID and a record address in the buffer pool. This record is pinned in the buffer. The protocol about fixing and unfixng records is as follows. Each record pinned in the buffer is owned by exactly one operator at any point in time.After receiving a record, the operator can hold on to it for a while, e.g., in a hash table, unfix it, e.g., when a pred- icate fails, or pass it on to the next operator. Complex operations that create new records, e.g., join, have to fix their output records in the buffer before passing them on, and have to unfix their input records. Since this could re- sult in a large number of buffer calls (one per record in each operator in the query), the interface to the buffer manager was recently redesigned such that it will require a total of two buffer calls per cluster on the procedure side (e. g., a file scan) independently of how many records a cluster contains, and only one buffer call per cluster on the consumer side.</p>
<p>树结构的查询评估计划用于通过需求驱动的数据流执行查询。 下一个操作的返回值除了状态指示符之外，还有一个名为Next-Record的结构，它由RID和缓冲池中的记录地址组成。 该记录被固定在缓冲区中。 关于修复和取消修复记录的协议如下。 固定在缓冲区中的每条记录在任何时间点都由一个操作员拥有。收到一条记录后，操作员可以将其保留一段时间（例如，在哈希表中），然后取消固定它，例如，当预测时 icate 失败，或者将其传递给下一个操作员。 创建新记录的复杂操作（例如连接）必须在传递它们之前修复缓冲区中的输出记录，并且必须取消修复它们的输入记录。 由于这可能会导致大量的缓冲区调用（查询中每个运算符中的每个记录一个），因此最近重新设计了缓冲区管理器的接口，使得过程中的每个集群总共需要两次缓冲区调用 端（例如文件扫描）独立于集群包含多少条记录，并且在消费者端每个集群只有一个缓冲区调用。</p>
<p>A Next-Record structure can point to one record only. All currently implemented query processing algorithms pass complete records between operators, e.g., join creates new, complete records by copying fields from two input records. It can be argued that creating complete new records and passing them between operators is prohibitively expensive. An alternative is to leave original records in the buffer as they were retrieved from the stored data, and compose Next-Record pairs, triples, etc., as intermediate results. Although this alternative results in less memory-to-memory copying, it is not implemented explicitly because Volcano already provides the necessary mechanisms, namely the Biter iterator (see next subsec- tion) that can replace each record in a stream by an RID- pointer pair or vice versa.</p>
<p>下一条记录结构只能指向一条记录。 当前实现的所有查询处理算法都在运算符之间传递完整的记录，例如，连接通过复制两个输入记录中的字段来创建新的完整记录。 可以说，创建完整的新记录并在运营商之间传递它们的成本过高。 另一种方法是将原始记录保留在缓冲区中，因为它们是从存储的数据中检索的，并组合下一个记录对、三元组等作为中间结果。 虽然这种替代方案会减少内存到内存的复制，但它并没有明确实现，因为 Volcano 已经提供了必要的机制，即 Biter 迭代器（参见下一小节），它可以用 RID 指针替换流中的每个记录 配对或反之亦然。</p>
<p> In summary, demand-driven dataflow is implemented by encoding operators as iterators, i.e., with open, next, and close procedures, since this scheme promises gener- ality , extensibility, efficiency, and low overhead. The next few sections describe some of Volcano’s existing iterators in more detail. In very few modules, the described operators provide much of the functionality of other query evaluation systems through generality and separation of mechanisms and policies. Furthermore, the separation of set processing control (iteration) from item interpretation and manipulation provides this functionality independently from any data model.</p>
<p>总之，需求驱动的数据流是通过将运算符编码为迭代器来实现的，即使用 open、next 和 close 过程，因为该方案具有通用性、可扩展性、高效性和低开销。 接下来的几节将更详细地描述 Volcano 的一些现有迭代器。 在极少数模块中，所描述的运算符通过机制和策略的通用性和分离性提供了其他查询评估系统的许多功能。 此外，集合处理控制（迭代）与项目解释和操作的分离提供了独立于任何数据模型的功能。</p>
<ol>
<li><p>Scans, Functional Join, and Filter: The first scan interface was discussed with the file system. The second interface to scans, both file scans and B+-tree scans, pro- vides an iterator interface suitable for query processing. The open procedures open the file or B+-tree and initiate a scan using the scan procedures of the file system level. The file name or closed file descriptor are given in the state record as are an optional predicate and bounds for B+-tree scans. Thus, the two scan interfaces are function- ally equivalent. Their difference is that the file system scan interface is used by various internal modules, e.g., by the device module for the device table of contents, while the iterator interface is used to provide leaf operators for query evaluation plans.</p>
<p>扫描、功能连接和过滤：第一个扫描接口是与文件系统一起讨论的。 第二个扫描接口（文件扫描和 B+ 树扫描）提供了适合查询处理的迭代器接口。 打开过程打开文件或 B+ 树并使用文件系统级别的扫描过程启动扫描。 文件名或关闭文件描述符在状态记录中给出，作为 B+ 树扫描的可选谓词和边界。 因此，两个扫描接口在功能上是等效的。 它们的区别在于，文件系统扫描接口由各种内部模块使用，例如，由用于设备内容表的设备模块使用，而迭代器接口用于为查询评估计划提供叶运算符。</p>
<p>Typically, Bf-tree indices hold keys and RID’s in their leaves. In order to use B+-tree indices, the records in the data file must be retrieved. In Volcano, this look-up op- eration is split from the B+-tree scan iterator and is performed by the functional join operator. This operator re- quires a stream of records containing RID’s as input and either outputs the records retrieved using the RID’s or it composes new records from the input records and the re- trieved records, thus “joining” the B+-tree entries and their corresponding data records.</p>
<p>通常，B+ 树索引在其叶子中保存键和 RID。 为了使用 B+ 树索引，必须检索数据文件中的记录。 在 Volcano 中，此查找操作从 B+ 树扫描迭代器中分离出来，并由函数连接运算符执行。 该运算符需要包含 RID 的记录流作为输入，并且输出使用 RID 检索到的记录，或者从输入记录和检索到的记录组成新记录，从而“连接”B+ 树条目及其对应的记录。 数据记录。</p>
<p>B+-tree scan and functional join are separated for a number of reasons. First, it is not clear that storing data in B+-tree leaves never is a good idea. At times, it may be desirable to experiment with having other types of in- formation associated with look-up keys. Second, this sep- aration allows experimentation with manipulation of RID- lists for complex queries. Third, while functional join is currently implemented rather naively, this operation can be made more intelligent to assemble complex objects recursively. In summary, separating index search and record retrieval is another example for porviding mechanisms in Volcano to allow for experiments with policies, a design principle employed to ensure that the Volcano software would be flexible and extensible.</p>
<p>由于多种原因，B+ 树扫描和功能连接是分开的。 首先，目前尚不清楚将数据存储在 B+ 树叶子中永远不是一个好主意。 有时，可能需要尝试使用与查找键相关联的其他类型的信息。 其次，这种分离允许对复杂查询的 RID 列表进行实验操作。 第三，虽然函数式连接目前的实现相当简单，但可以使该操作更加智能，以递归地组装复杂对象。 总之，分离索引搜索和记录检索是 Volcano 中提供允许进行策略实验的机制的另一个例子，这是用于确保 Volcano 软件灵活且可扩展的设计原则.</p>
<p>The filter opeartor used in the example above performs three functions, depeding on presence or absence of corresponding support functions in the state record. The predicate function apples a selection predicate.e.g., to implement bit vector filtering. The transform function creates a new record,typically of a new type, from each old record.An example would be a relational projection (without duplicate elimination). More complex examples include compression and decompression, other changes in codes and representations, and reducing a stream of rec- ords to RID-pointer pairs. Finally, the apply function is invoked once on each record for the venefit of its side effects. Typical examples are updates and printing. Notice that updates are done within streams and query evaluation plans. Thus, Volcano plans are not only retrieval but also update plans. The filter operator is also called the ‘ ‘side-effect operator. ’ ’ Another example is creating a fil- ter for bit vector filtering. In other words, the filter op- erator is a very versatile single-input single-output oper- ator that can be used for a variety of purposes. Bit vector filtering is an example for a special version of separationof policy and mechanism, namely the rule not to provide an operation that can be composed easily and efficiently using existing operations.</p>
<p>上面示例中使用的过滤器运算符执行三个功能，具体取决于状态记录中是否存在相应的支持功能。 谓词函数应用选择谓词，例如，实现位向量过滤。 变换函数从每个旧记录创建一个新记录，通常是新类型。一个例子是关系投影（没有重复消除）。 更复杂的示例包括压缩和解压缩、代码和表示的其他更改以及将记录流减少为 RID 指针对。 最后，apply 函数在每条记录上调用一次，以消除其副作用。 典型的例子是更新和打印。 请注意，更新是在流和查询评估计划内完成的。 因此，Volcano计划不仅是检索计划，而且是更新计划。 过滤运算符也称为“副作用运算符”。 ” 另一个例子是创建一个用于位向量过滤的过滤器。 换句话说，过滤器运算符是一种非常通用的单输入单输出运算符，可用于多种目的。 位向量过滤是策略和机制分离的特殊版本的示例，即不提供可以使用现有操作轻松且高效地组合的操作的规则。</p>
</li>
<li><p>One-to-One Match: Together with the filter opera- tor, the one-to-one match operator will probably be among the most frequently used query processing operators in Volcano as it implements a variety of set-matching func- tions. In a single operator, it realizes join, semi-join, outer joint, anti-joint, intersection, union, difference, anti-dif- ference, aggregation, and duplicate elimination. The one- to-one match operator is a physical operator like sort, i.e., part of the executable algebra, not a logical operator like the operators of relational algebra. It is the operator that implements all operations in which an item is included in the output depending on the result of a comparison be- tween a pair of items.</p>
<p>一对一匹配：与过滤运算符一起，一对一匹配运算符可能是 Volcano 中最常用的查询处理运算符之一，因为它实现了各种集合匹配功能。 在单个算子中，它实现了连接、半连接、外连接、反连接、交集、并集、差分、反差分、聚合和重复消除。 一对一匹配运算符是像排序这样的物理运算符，即可执行代数的一部分，而不是像关系代数运算符那样的逻辑运算符。 该运算符实现所有操作，其中根据一对项目之间的比较结果将项目包含在输出中。</p>
<p>Fig. 3 shows the basic principle underlying the one-to- one match operator for binary operations, namely sepa- ration of the matching and non-matching components of two sets, called R and S in the Fig. 3, and producing ap- propriate subsets, possibly after some transformation and combination as in the case of a join. Since all these operations require basically the same steps, it was logical to mplement them in one general and efficient module. The main difference between the unary and binary operations, e.g., aggregate functions and equi-join, is that the former require comparing items of the same input while the latter require comparing items of two different inputs.</p>
<p>图 3 显示了二元运算的一对一匹配运算符的基本原理，即分离两个集合的匹配和不匹配分量（在图 3 中称为 R 和 S），并生成 ap- 适当的子集，可能是在一些转换和组合之后，如连接的情况。 由于所有这些操作都需要基本相同的步骤，因此在一个通用且高效的模块中实现它们是合乎逻辑的。 一元和二元运算（例如聚合函数和等连接）之间的主要区别在于，前者需要比较相同输入的项目，而后者需要比较两个不同输入的项目。</p>
<p><img src="/img/volcano-pic/image-20230720155442652.png"></p>
<p>Since the implementation of Volcano’s one-to-one match is data-model-independent and all operations on data items are imported via support functions, the module s not restricted to the relational model but can perform set matching functions for arbitrary data types. Furthermore, the hash-based version provides recursive hash table overflow avoidance [121and resolution similar to hybrid hash join [31] and can therefore handle very large nput sizes. The sort-based version of one-to-one match is based on an external sort operator and can also operate on arbitrarily large inputs.</p>
<p>由于Volcano的一对一匹配的实现是与数据模型无关的，并且对数据项的所有操作都是通过支持函数导入的，因此该模块不限于关系模型，而是可以对任意数据类型执行集合匹配函数。 此外，基于散列的版本提供了递归散列表溢出避免[121]和类似于混合散列连接[31]的解决方案，因此可以处理非常大的输入大小。 一对一匹配的基于排序的版本基于外部排序运算符，并且还可以对任意大的输入进行操作。</p>
<p>While there seems to be an abundance <strong>of</strong> join algo- rithms, our design goals of extensibility and limited sys- tem size led to the choice of only two algorithms (at the current time) to be implemented in Volcano, namely merge join and hybrid hash join. This choice will also allow experimental research into the duality and trade-offs between sort- and hash-based query processing algo- rithms.</p>
<p>虽然连接算法似乎很丰富，但我们的可扩展性和有限系统规模的设计目标导致我们只选择了两种算法（目前）在 Volcano 中实现，即合并连接和混合哈希 加入。 这种选择还允许对基于排序和基于散列的查询处理算法之间的二元性和权衡进行实验研究。</p>
<p>The classic hashjoin algorithm (which is the in-mem- ory component of hybrid hash join) proceeds in two phases. In the first phase, a hash table is built from one input; it is therefore called the build phase. In the second phase, the hash table is probed using tuples from the other input to determine matches and to compose output tuples; it is called the probe phase. After the probe phase, the hash table and its entries are discarded. Instead, our one- to-one match operator uses a third phase called thejush phase, which is needed for aggregate functions and some other operations.</p>
<p>经典的哈希连接算法（混合哈希连接的内存组件）分两个阶段进行。 在第一阶段，根据一个输入构建哈希表； 因此，它被称为构建阶段。 在第二阶段，使用来自其他输入的元组来探测哈希表以确定匹配并组成输出元组； 它被称为探测阶段。 探测阶段结束后，哈希表及其条目将被丢弃。 相反，我们的一对一匹配运算符使用称为 thejush 阶段的第三阶段，这是聚合函数和其他一些操作所需要的。</p>
<p>Since the one-to-one match operator is an interator like all Volcano operators, the three phases are assigned to the <strong>open, next,</strong> and <strong>close</strong> functions. <strong>Open</strong> includes the build phase, while the other two phases are included in the <strong>next</strong> function. Successive invocations of the <strong>next</strong> function au- tomatically switch from the probe phase to the flush phase when the second input is exhausted.</p>
<p>由于一对一匹配运算符像所有 Volcano 运算符一样是一个迭代器，因此三个阶段被分配给 open、next 和 close 函数。 Open 包含构建阶段，而其他两个阶段包含在 next 函数中。 当第二个输入耗尽时，连续调用下一个函数会自动从探测阶段切换到刷新阶段。</p>
<p>The build phase can be used to eliminate duplicates or to perform an aggregate function in the build input. The one-to-one match module does not require a probe input; if only an aggregation is required without subsequent join, the absence of the probe input in the state record signals to the module that the probe phase should be skipped. For aggregation, instead of inserting a new tuple into the hash table as in the classic hash join, an input tuple is first matched with the tuples in its prospective hash bucket. If a match is found, the new tuple is discarded or its values are aggregated into the existing tuple.</p>
<p>构建阶段可用于消除重复项或在构建输入中执行聚合函数。 一对一匹配模块不需要探针输入； 如果仅需要聚合而无需后续连接，则状态记录中缺少探测输入会向模块发出应跳过探测阶段的信号。 对于聚合，不是像经典哈希连接那样将新元组插入到哈希表中，而是首先将输入元组与其预期哈希桶中的元组进行匹配。 如果找到匹配项，则丢弃新元组或将其值聚合到现有元组中。</p>
<p>While hash tables in main memory are usually quite fast, a severe problem occurs if the build input does not fit in main memory. This situation is called <strong>hash table overjlow.</strong> There are two ways to deal with hash table over- flow. First, if a query optimizer is used and can anticipate overflow, it can be avoided by partitioning the input(s). This <strong>overjlow avoidance</strong> technique is the basis for the hash join algorithm used in the Grace database machine [121. Second, overflow files can be created using <strong>overfrow res- olution</strong> after the problem occurs.</p>
<p>虽然主内存中的哈希表通常非常快，但如果构建输入不适合主内存，则会出现严重问题。 这种情况称为哈希表溢出。 有两种方法可以处理哈希表溢出。 首先，如果使用查询优化器并且可以预见溢出，则可以通过对输入进行分区来避免溢出。 这种过度避免技术是 Grace 数据库机中使用的哈希连接算法的基础 [121。 其次，可以在问题发生后使用溢出解决方案创建溢出文件。</p>
<p>For Volcano’s one-to-one match, we have adopted hy- brid hash join. Compared to the hybrid hash algorithm used in GAMMA, our overflow resolution scheme has several improvements. Items can be inserted into the hash table without copying, i.e., the hash table points directly to records in the buffer as produced by one-to-one match’s build input. If input items are not densely packed, how- ever, the available buffer memory can fill up very quickly. Therefore, the one-to-one match operator has an argu- ment called the packing threshold. When the number of items in the hash table reaches this threshold, items are packed densely into overflow files. However, the clusters (pages) of these overflow files are not yet unfixed in the buffer, i.e., no t &#x2F; O is performed as yet. Only when the number of items in the hash table reaches a second thresh- old called spilling threshold is the first of the partition files unfixed. The clusters of this file are written to disk and the count of items in the hash table accordingly re- duced. When this number reaches the spilling threshold again, the next partition is unfixed, etc. If necessary, par- titioning is performed recursively, with automatically ad- justed packing and spilling thresholds. The unused por- tions of the hash table, i.e., the portions corresponding to spilled buckets, are used for bit vector filtering to save I&#x2F;O to overflow files.</p>
<p>对于 Volcano 的一对一匹配，我们采用了混合哈希连接。 与 GAMMA 中使用的混合哈希算法相比，我们的溢出解决方案有一些改进。 可以将项目插入到哈希表中而无需复制，即哈希表直接指向由一对一匹配的构建输入生成的缓冲区中的记录。 然而，如果输入项不是密集排列的，则可用缓冲存储器很快就会被填满。 因此，一对一匹配运算符有一个称为打包阈值的参数。 当哈希表中的项目数量达到此阈值时，项目将被密集地打包到溢出文件中。 然而，这些溢出文件的簇(页)在缓冲区中尚未被固定，即尚未执行t&#x2F;O。 仅当哈希表中的项目数达到称为溢出阈值的第二个阈值时，第一个分区文件才会被取消修复。 该文件的簇被写入磁盘，哈希表中的项目数相应减少。 当这个数字再次达到溢出阈值时，下一个分区将不固定，等等。如果有必要，分区会递归执行，并自动调整打包和溢出阈值。 哈希表的未使用部分，即与溢出桶相对应的部分，用于位向量过滤，以节省溢出文件的 I&#x2F;O。</p>
<p>The fan-out of the first partitioning step is determined by the total available memory minus the memory required to reach the packing threshold. By choosing the packing and spilling thresholds, a query optimizer can avoid re- cord copying entirely for small build inputs, specify overflow avoidance (and the maximum fan-out) for very large build inputs, or determine packing and spilling thresholds based on the expected build input size. In fact, because the input sizes cannot be estimated precisely if the inputs are produced by moderately complex expres- sions, the optimizer can adjust packing and spilling thresholds based on the esimated probability distributions of input sizes. For example, if overflow is very unlikely, it might be best to set the packing threshold quite high such that, with high probability, the operation can pro- ceed without copying. On the other hand, if overflow is more likely, the packing threshold should be set lower to obtain a larger partitioning fan-out.</p>
<p>第一个分区步骤的扇出由总可用内存减去达到打包阈值所需的内存确定。 通过选择打包和溢出阈值，查询优化器可以完全避免小型构建输入的记录复制，为非常大的构建输入指定溢出避免（和最大扇出），或者根据预期确定打包和溢出阈值 构建输入大小。 事实上，由于如果输入是由中等复杂的表达式生成的，则无法精确估计输入大小，因此优化器可以根据输入大小的估计概率分布来调整打包和溢出阈值。 例如，如果溢出的可能性很小，则最好将打包阈值设置得相当高，以便很有可能操作可以继续进行而无需复制。 另一方面，如果溢出的可能性更大，则应将打包阈值设置得较低以获得更大的分区扇出。</p>
<p>The initial packing and spilling thresholds can be set to zero; in that case, Volcano’s one-to-one match performs overflow avoidance very similar to the join algorithm used in the Grace database machine. Beyond this parameter- ization of overflow avoidance and resolution, Volcano’s one-to-one match algorithm also permits optimizations of cluster size and recursion depth similar to the ones used for sorting [4], <strong>[21]</strong> and for nonuniform hash value dis- tributions, and it can operate on inputs with variable- length records.</p>
<p>初始打包和溢出阈值可以设置为零； 在这种情况下，Volcano 的一对一匹配执行溢出避免，与 Grace 数据库机器中使用的连接算法非常相似。 除了溢出避免和解析的参数化之外，Volcano 的一对一匹配算法还允许优化簇大小和递归深度，类似于用于排序 [4]、[21] 和非均匀哈希值分布的优化 ，并且它可以对具有可变长度记录的输入进行操作。</p>
<p>The extension of the module described so far to set op- erations started with the observation that the intersection of two union-compatible relations is the same as the nat- ural join of these relations, and can be best implemented as semi-join. The union is the (double-sided) outer join of union-compatible relations. Difference and anti-differ- ence of two sets can be computed using special settings of the algorithm’s bells and whistles. Finally, a Cartesian product can be implemented by matching successfully all possible pairs of items from the two inputs.</p>
<p>到目前为止描述的模块对设置操作的扩展始于观察到两个并集兼容关系的交集与这些关系的自然连接相同，并且可以最好地实现为半连接。 联合是联合兼容关系的（双面）外部联接。 可以使用算法花哨的特殊设置来计算两个集合的差异和反差异。 最后，可以通过成功匹配两个输入中所有可能的项目对来实现笛卡尔积。</p>
<p><strong>A</strong> second version of one-to-one match is based on sort- ing. Its two modules are a disk-based merge-sort and the actual merge-join. Merge-join has been generalized sim- ilarly to hash-join to support semi-join, outer join, anti-join, and set operations. The sort operator has been im- plemented in such a way that it uses and provides the it- erator interface. Opening the <strong>sort</strong> iterator prepares sorted runs for merging. If the number of runs is larger than the maximal fan-in, runs are merged into larger runs until the remaining runs can be merged in a single step. The final merge is performed on demand by the <strong>next</strong> function. If the entire input fits into the sort buffer, it is kept there until demanded by the <strong>next</strong> function. The sort operator also supports aggregation and duplicate elimination. It can perform these operations early, i.e., while writing tem- porary files [2]. The sort algorithm is described and eval- uated in detail in <strong>[2****11.</strong></p>
<p>一对一匹配的第二个版本基于排序。 它的两个模块是基于磁盘的合并排序和实际的合并连接。 合并连接已被推广为类似于散列连接，以支持半连接、外连接、反连接和集合操作。 排序运算符的实现方式是使用并提供迭代器接口。 打开排序迭代器为合并排序运行做好准备。 如果运行数量大于最大扇入，运行将合并为更大的运行，直到可以在单个步骤中合并剩余运行。 最终合并由下一个函数按需执行。 如果整个输入适合排序缓冲区，则它将保留在那里，直到下一个函数需要为止。 排序运算符还支持聚合和重复消除。 它可以尽早执行这些操作，即在写入临时文件时[2]。 排序算法在[211.</p>
<p>In summary, Volcano’s one-to-one match operators are very powerful parts of Volcano’s query execution alge- bra. By separating the control required to operate on sets of items and the interpretation and manipulation of indi- vidual items it can perform a variety of set matching tasks frequently used in database query processing, and can perform these tasks for arbitrary data types and data models. The separation of mechanisms and policies for overflow management supports overflow avoidance as well as hybrid hash overflow resolution, both recursively if required. Implementing sort- and hash-based algo- rithms in a comparable fashion will allow meaningful ex- perimental research into the duality and trade-offs be- tween sort- and hash-based query processing algorithms. The iterator interface guarantees that the one-to-one match operator can easily be combined with other operations, including new iterators yet to be designed.</p>
<p>总之，Volcano 的一对一匹配运算符是 Volcano 查询执行代数中非常强大的部分。 通过分离对项目集进行操作所需的控制以及对单个项目的解释和操作，它可以执行数据库查询处理中经常使用的各种集合匹配任务，并且可以对任意数据类型和数据模型执行这些任务。 溢出管理机制和策略的分离支持溢出避免以及混合散列溢出解决，如果需要的话可以递归地解决。 以类似的方式实现基于排序和散列的算法将允许对基于排序和散列的查询处理算法之间的二元性和权衡进行有意义的实验研究。 迭代器接口保证一对一匹配运算符可以轻松地与其他操作组合，包括尚未设计的新迭代器。</p>
</li>
<li><p><strong>One-to-Many Match:</strong> While the one-to-one match operator includes an item in its output depending on a comparison of two items with one another, the one-to- many match operator compares each item with a number of other items to determine whether a new item is to be produced. <strong>A</strong> typical example is relational division, the re- lational algebra operator corresponding to universal quan- tification in relational calculus. There are two versions of relational division in Volcano. The first version, called <strong>native division,</strong> is based on sorting. The second version, called <strong>hash-division,</strong> utilizes two hash tables, one on the divisor and one on the quotient. An exact description of the two algorithms and altemative algorithms based on aggregate functions can be found in <strong>[16]</strong> along with ana- lytical and experimental performance comparisons and detailed discussions of two partitioning strategies for hash table overflow and multiprocessor implementations. W e are currently studying how to generalize these algo- rithms in a way comparable with the generalizations of aggregation and join, e.g., for a <strong>majority</strong> function.</p>
<p>一对多匹配：一对一匹配运算符根据两个项目之间的比较在其输出中包含一个项目，而一对多匹配运算符将每个项目与多个其他项目进行比较 以确定是否要生产新产品。 一个典型的例子是关系除法，关系代数运算符对应于关系微积分中的通用量化。 Volcano 中的关系划分有两个版本。 第一个版本称为本机划分，基于排序。 第二种版本称为哈希除法，它使用两个哈希表，一个用于除数，一个用于商。 两种算法和基于聚合函数的替代算法的准确描述可以在[16]中找到，以及分析和实验性能比较以及哈希表溢出和多处理器实现的两种分区策略的详细讨论。 我们目前正在研究如何以与聚合和连接的泛化相当的方式来泛化这些算法，例如，对于多数函数.</p>
</li>
</ol>
<h3 id="IV-EXTENSIBILITY"><a href="#IV-EXTENSIBILITY" class="headerlink" title="IV. EXTENSIBILITY"></a>IV. EXTENSIBILITY</h3><p>A number of database research efforts strive for exten- sibility, e.g., EXODUS, GENESIS, Postgres, Starburst, DASDBS <strong>[30],</strong> Cactis <strong>[24],</strong> and others. Volcano is a very open query evaluation architecture that provides easy ex- tensibility. Let us consider a number of frequently pro- posed database extensions and how they can be accom- modated in Volcano.</p>
<p>许多数据库研究工作都致力于可扩展性，例如 EXODUS、GENESIS、Postgres、Starburst、DASDBS [30]、Cactis [24] 等。 Volcano 是一个非常开放的查询评估架构，提供简单的可扩展性。 让我们考虑一些经常提出的数据库扩展以及如何将它们容纳在 Volcano 中。</p>
<p>First, when extending the object type system, e.g., with a new abstract data type (ADT) like date or box, the Volcano software is not affected at all because it does not provide a type system for objects. All manipulation of and calculation based on individual objects is performed by support functions. To a certain extent, Volcano is incomplete (it is not a database system), but by separating set processing and instance interpretation and providing a well-defined interface between them, Volcano is inherently extensible on the level of instance types and semantics.</p>
<p>首先，当扩展对象类型系统时，例如使用日期或框等新的抽象数据类型（ADT），Volcano 软件根本不受影响，因为它不提供对象的类型系统。 所有基于单个对象的操作和计算都是由支持函数执行的。 在某种程度上，Volcano 是不完整的（它不是一个数据库系统），但是通过将集合处理和实例解释分开并在它们之间提供定义良好的接口，Volcano 在实例类型和语义层面上具有本质上的可扩展性。</p>
<p>As a rule, data items that are transferred between operators using some next iterator procedure are records. For an extensible or object-oriented database system, this would be an unacceptable problem and limitation. The solution to be used in Volcano is to pass only the root component (record) between operators after loading and fixing necessary component records in the buffer and suit- ably swizzling inter-record pointers. Very simple objects can be assembled in Volcano with the functional join op- erator. Generalizations of this operator are necessary for object-oriented o r non-first-normal-form database sys- tems, but can be included in Volcano without difficulty. In fact, a prototype for such an assembly operator has been built [ 2 6 ] for use in the revelation object - oriented database systems project [151.</p>
<p>通常，使用某个下一个迭代器过程在运算符之间传输的数据项是记录。 对于可扩展或面向对象的数据库系统，这将是一个不可接受的问题和限制。 Volcano 中使用的解决方案是在缓冲区中加载和修复必要的组件记录并适当混合记录间指针后，仅在运算符之间传递根组件（记录）。 非常简单的对象可以通过函数连接运算符在 Volcano 中组装。 该运算符的推广对于面向对象或非第一范式数据库系统是必要的，但可以毫无困难地包含在 Volcano 中。 事实上，这样一个汇编操作符的原型已经被构建出来了[2 6]，用于revelation 面向对象的数据库系统项目[151]。</p>
<p>Second, in order to add new functions on individual objects or aggregate functions, e.g., geometric mean, to the database and query processing system, the appropriate support function is required and passed to a query pro- cessing routine. In other words, the query processing rou- tines are not affected by the semantics of the support func- tions as long as interface and return values are correct. The reason Volcano software is not affected by extensions of the functionality on individual objects is that Volcano’s software only provides abstractions and implementations for dealing with and sequencing <strong>sets</strong> of objects using streams, whereas the capabilities for interpreting and ma- nipulating individual objects are <strong>imported</strong> in the form of support functions.</p>
<p>其次，为了向数据库和查询处理系统添加单个对象或聚合函数的新函数（例如几何平均值），需要适当的支持函数并将其传递给查询处理例程。 换句话说，只要接口和返回值正确，查询处理例程就不会受到支持函数语义的影响。 Volcano 软件不受单个对象功能扩展影响的原因是，Volcano 软件仅提供使用流处理和排序对象集的抽象和实现，而解释和操作单个对象的功能是在 Volcano 软件中导入的。 支持功能的形式。</p>
<p>Third, in order to incorporate a new access method, e.g., multidimensional indices in form of R-trees [22], appropriate iterators have to be defined. Notice that it makes sense to perform not only retrieval but also main- tenance of storage structures in the form of iterators. For example, if a set of items defined via a predicate (selec- tion) needs to be updated, the iterator or query tree im- plementing the selection can “feed” its data into a main- tenance iterator. The items fed into the maintenance operator should include a reference to the part of the storage structure to be updated, e.g., a RID or a key, and appropriate new values if they have been computed in the selection, e.g., new salaries from old salaries. Updating multiple structures (multiple indices) can be organized and executed very efficiently using nested iterators, i.e., a query evaluation plan. Furthermore, if ordering makes maintenance more efficient as for B-trees, an ordering or sort iterator can easily be included in the plan. In other words, it makes sense to think of plans not only as query plans used in retrieval but also as “update plans” or com- binations of retrieval and update plans. The stream con- cept is very open; in particular, anonymous inputs shield existing query processing modules and the new iterators from one another.</p>
<p>第三，为了合并新的访问方法，例如 R 树 [22] 形式的多维索引，必须定义适当的迭代器。 请注意，不仅以迭代器的形式执行检索，而且还执行存储结构的维护都是有意义的。 例如，如果需要更新通过谓词（选择）定义的一组项目，则实现选择的迭代器或查询树可以将其数据“馈送到”维护迭代器中。 馈入维护操作符的项目应包括对要更新的存储结构部分的引用，例如，RID或密钥，以及适当的新值（如果它们已在选择中计算），例如，从旧工资中计算出新工资 。 使用嵌套迭代器（即查询评估计划）可以非常有效地组织和执行更新多个结构（多个索引）。 此外，如果排序使 B 树的维护更加高效，则可以轻松地将排序或排序迭代器包含在计划中。 换句话说，将计划不仅视为检索中使用的查询计划，而且视为“更新计划”或检索和更新计划的组合是有意义的。 流的概念非常开放； 特别是，匿名输入可以屏蔽现有查询处理模块和新迭代器之间的相互影响。</p>
<p>Fourth, to include a new query processing algorithm in Volcano, e.g., an algorithm for transitive closure or <strong>nest</strong> and <strong>unnest</strong> operations for nested relations, the algorithm needs to be coded in the iterator paradigm. In other words, the algorithm implementation must provide <strong>open, next,</strong> and <strong>close</strong> procedures, and must use these procedures for its input stream or streams. After an algorithm has been brought into this form, its integration with Volcano is trivial. In fact, as the Volcano query processing software became more complex and complete, this was done a number of times. For example, the one-to-many match or division operators [16] were added without regard to the other operators, and when the early in-memory-only ver- sion of hash-based one-to-one match was replaced by the version with overflow management described above, none of the other operators or meta-operators had to be changed. Finally, a complex object assembly operator was added recently to Volcano <strong>[26].</strong></p>
<p>第四，要在 Volcano 中包含新的查询处理算法，例如用于嵌套关系的传递闭包或嵌套和取消嵌套操作的算法，该算法需要以迭代器范例进行编码。 换句话说，算法实现必须提供 open、next 和 close 过程，并且必须将这些过程用于其一个或多个输入流。 当算法采用这种形式后，它与 Volcano 的集成就很简单了。 事实上，随着 Volcano 查询处理软件变得更加复杂和完整，这种情况已经被完成了很多次。 例如，添加了一对多匹配或除法运算符[16]，而不考虑其他运算符，并且当基于哈希的一对一匹配的早期仅内存版本被替换为 在上述具有溢出管理的版本中，无需更改任何其他运算符或元运算符。 最后，Volcano 最近添加了一个复杂的对象组装操作符 [26]。</p>
<p>Extensibility can also be considered in a different con- text. In the long run, it clearly is desirable to provide an interactive front-end to make using Volcano easier. We are currently working on a two front-end, a nonoptimized command interpreter based on Volcano’s executable al- gebra and an optimized one based on a logical algebra or calculus language including query optimization imple- mented with a new optimizer generator. The translation between plans as produced by an optimizer and Volcano will be accomplished using a module that “walks” query evaluation plans produced by the optimizer and Volcano plans, i.e., state records, support functions, etc. We will also use the optimizing front-end as a vehicle for experi- mentation with <strong>dynamic</strong> <em>query</em> <strong>evaluation plans</strong> that are outlined in the next section.</p>
<p>还可以在不同的上下文中考虑可扩展性。 从长远来看，显然需要提供一个交互式前端以使 Volcano 的使用更加容易。 我们目前正在开发一种双前端，一种是基于 Volcano 可执行代数的非优化命令解释器，另一种是基于逻辑代数或微积分语言的优化命令解释器，包括使用新的优化器生成器实现的查询优化。 优化器和 Volcano 生成的计划之间的转换将使用一个模块来完成，该模块“遍历”优化器和 Volcano 计划生成的查询评估计划，即状态记录、支持函数等。我们还将使用优化前端 最终作为动态查询评估计划实验的工具，这些计划将在下一节中概述。</p>
<p>In summary, since Volcano is very modular in its de- sign, extensibility is provided naturally. It could be ar- gued that this is the case only because Volcano does not address the hard problems in extensibility. However, this argument does not hold. Rather, Volcano is only one component of a database system, namely the query exe- cution engine. Therefore, it addresses only a subset of the extensibility problems and ignores a different subset. As a query processing engine, it provides extensibility of its set of query processing algorithms, and it does so in a way that matches well with the extensibility as provided by query optimizer generators. It does not provide other da- tabase services and abstractions like a type system and type checking for the support functions since it is not an extensible database system. The Volcano routines assume that query evaluation plans and their support functions are correct. Their correctness has to be ensured before Vol- cano is invoked, which is entirely consistent with the gen- eral database systems concept to ensure correctness at the highest possible level, i.e., as soon as possible after a user query is parsed. The significance of Volcano as an exten- sible query evaluation system is that it provides a simple but very useful and powerful set of mechanisms for effi- cient query processing and that it can and has been used as a flexible research tool. Its power comes not only from the fact that it has been implemented following a few con- sistent design principles but also from its two meta-op- erators described in the next two sections.</p>
<p>总而言之，由于 Volcano 的设计非常模块化，因此自然地提供了可扩展性。 可以说，出现这种情况只是因为 Volcano 没有解决可扩展性方面的难题。 然而，这个论点并不成立。 相反，Volcano 只是数据库系统的一个组件，即查询执行引擎。 因此，它仅解决可扩展性问题的一个子集并忽略不同的子集。 作为查询处理引擎，它提供了查询处理算法集的可扩展性，并且它的实现方式与查询优化器生成器提供的可扩展性很好地匹配。 它不提供其他数据库服务和抽象，例如类型系统和支持功能的类型检查，因为它不是可扩展的数据库系统。 Volcano 例程假设查询评估计划及其支持函数是正确的。 在调用 Volcano 之前必须确保它们的正确性，这与一般数据库系统的概念完全一致，以确保尽可能高的级别的正确性，即在解析用户查询后尽快确保正确性。 Volcano 作为一个可扩展的查询评估系统的重要性在于，它为高效查询处理提供了一套简单但非常有用且强大的机制，并且它可以并且已经被用作灵活的研究工具。 它的力量不仅来自于它是按照一些一致的设计原则实现的，而且还来自于接下来两节中描述的两个元运算符。</p>
<h3 id="V-DYNAMIC-QUERY-EVALUATION-PLANS"><a href="#V-DYNAMIC-QUERY-EVALUATION-PLANS" class="headerlink" title="V. DYNAMIC QUERY EVALUATION PLANS"></a>V. DYNAMIC QUERY EVALUATION PLANS</h3><p>In most database systems, a query embedded in a program written in a conventional programming language <strong>is</strong> opti- mized when the program is compiled. The query opti- mizer must make assumptions about the values of the pro- gram variables that appear as constants in the query and the data in the database. These assumptions include that the query can be optimized realistically using guessed “typical” values for the program variables and that the database will not change significantly between query op- timization and query evaluation. The optimizer must also anticipate the resources that can be committed to query evaluation, e.g., the size of the buffer or the number of processors. The optimality of the resulting query evalua- tion plan depends on the validity of these assumptions. If a query evaluation plan is used repeatedly over an ex- tended period of time, it is important to determine when reoptimization is necessary. We are working on a scheme in which reoptimization can be avoided by using a new technique called <strong>dynamic query evaluation plans</strong> <strong>[I</strong> **71.**’</p>
<p>在大多数数据库系统中，嵌入在用传统编程语言编写的程序中的查询在程序编译时被优化。 查询优化器必须对查询中作为常量出现的程序变量的值和数据库中的数据做出假设。 这些假设包括可以使用程序变量的猜测“典型”值来实际优化查询，并且数据库在查询优化和查询评估之间不会发生显着变化。 优化器还必须预测可用于查询评估的资源，例如缓冲区的大小或处理器的数量。 结果查询评估计划的最优性取决于这些假设的有效性。 如果在较长时间内重复使用查询评估计划，则确定何时需要重新优化非常重要。 我们正在研究一种方案，通过使用一种称为动态查询评估计划的新技术来避免重新优化[I 71。”</p>
<p>Volcano includes a <strong>choose-plan</strong> operator that allows re- alization of both multiplan access modules and dynamic plans. In some sense, it is not an operator as it does not perform any data manipulations. Since it provides control for query execution it is a <strong>metu-operator.</strong> This operator provides the same <strong>open-next-close</strong> protocol as the other operators and can therefore be inserted into a query plan at any location. The <strong>open</strong> operation decides which of sev- eral equivalent query plans to use and invokes the <strong>open</strong> operation for this input. <strong>Open</strong> calls upon a support func- tion for this policy decision, passing it the <strong>bindings</strong> pa- rameter described above. The <strong>next</strong> and <strong>close</strong> operations simply call the appropriate operation for the input chosen during <strong>open.</strong></p>
<p>Volcano 包括一个选择计划运算符，允许实现多计划访问模块和动态计划。 从某种意义上说，它不是一个运算符，因为它不执行任何数据操作。 由于它提供对查询执行的控制，因此它是一个元运算符。 该运算符提供与其他运算符相同的打开-下一个-关闭协议，因此可以插入到查询计划的任何位置。 open 操作决定使用几个等效查询计划中的哪一个，并为此输入调用 open 操作。 Open 调用此策略决策的支持函数，并向其传递上述绑定参数。 接下来和关闭操作只需为打开期间选择的输入调用适当的操作。</p>
<p>Fig. 4 shows a very simple dynamic plan. Imagine a selection predicate controlled by a program variable. The index scan and functional join can be much faster than the file scan, but not when the index is nonclustering and a large number of items must be retrieved. Using the plan of Fig. 4, however, the optimizer can prepare effectively for both cases, and the application program using this dy- namic plan will perform well for any predicate value.</p>
<p>图 4 显示了一个非常简单的动态计划。 想象一下由程序变量控制的选择谓词。 索引扫描和函数连接可以比文件扫描快得多，但当索引是非聚集的并且必须检索大量项目时则不然。 然而，使用图 4 的计划，优化器可以有效地为这两种情况做好准备，并且使用此动态计划的应用程序对于任何谓词值都将表现良好。</p>
<p><img src="/img/volcano-pic/image-20230720160212513.png"></p>
<p>The <strong>choose-plan</strong> operator allows considerable flexibil- ity. If only one <strong>choose-plan</strong> operator is used as the top of a query evaluation plan, it implements a multiplan access module. If multiple <strong>choose-plan</strong> operators are included in a plan, they implement a dynamic query evaluation plan. Thus, all forms of dynamic plans identified in [17] can be realized with one simple and effective mechanism. Note that the <strong>choose-plan</strong> operator does not make the policy decision concerning which of several plans to execute; it only provides the mechanism. The policy is imported us- ing a support function. Thus, the decision can be made depending on bindings for query variables (e.g., program variables used as constants in a query predicate), on the resource and contention situation (e.g., the availability of processors and memory), other considerations such as user priority, or all of the above.</p>
<p>选择计划运算符具有相当大的灵活性。 如果仅使用一个选择计划运算符作为查询评估计划的顶部，则它会实现多计划访问模块。 如果一个计划中包含多个选择计划运算符，它们将实现动态查询评估计划。 因此，[17]中确定的所有形式的动态计划都可以通过一种简单而有效的机制来实现。 请注意，选择计划操作员并不做出有关执行多个计划中哪一个的策略决定； 它仅提供机制。 该策略是使用支持功能导入的。 因此，可以根据查询变量的绑定（例如，在查询谓词中用作常量的程序变量）、资源和争用情况（例如，处理器和内存的可用性）、其他考虑因素（例如用户优先级）来做出决定 ，或以上全部。</p>
<p>The <strong>choose-plan</strong> operator provides significant new freedom in query optimization and evaluation with an ex- tremely small amount of code. Since it is compatible with the query processing paradigm, its presence does not af- fect the other operators at all, and it can be used in a very flexible way. The operator is another example for Vol- cano’s design principle to provide mechanisms to imple- ment a multitude of policies. We used the same philoso- phy when designing and implementing a scheme for parallel query evaluation.</p>
<p>选择计划运算符以极少量的代码为查询优化和评估提供了显着的新自由度。 由于它与查询处理范例兼容，因此它的存在根本不会影响其他运算符，并且可以以非常灵活的方式使用。 运营商是 Volcano 设计原则的另一个例子，它提供了实施多种策略的机制。 在设计和实现并行查询评估方案时，我们使用了相同的理念。</p>
<h3 id="VI-MULTIPROCESSOR-QOURERY-EVALUATION"><a href="#VI-MULTIPROCESSOR-QOURERY-EVALUATION" class="headerlink" title="VI. MULTIPROCESSOR  QOURERY  EVALUATION"></a>VI. MULTIPROCESSOR  QOURERY  EVALUATION</h3><p>A large number of research and development projects have shown over the last decade that query processing in relational database systems can benefit significantly from parallel algorithms. The main reasons parallelism is rel- atively easy to exploit in relational query processing sys- tems are 1) query processing is performed using a tree of operators that can be executed in separate processes and processors connected with pipelines (inter-operator par- allelism) and 2) each operator consumes and produces sets that can be partitioned or fragmented into disjoint subsets to be processed in parallel (intra-operator parallelism). Fortunately, the reasons parallelism is easy to exploit in relational systems does not require the relational data model per se, only that queries be processed as sets of data items in a tree of operators. These are exactly the assumptions made in the design of Volcano, and it was therefore logical to parallelize extensible query process- ing in Volcano.</p>
<p>过去十年中的大量研究和开发项目表明，关系数据库系统中的查询处理可以从并行算法中受益匪浅。 在关系查询处理系统中并行性相对容易利用的主要原因是：1）查询处理是使用运算符树执行的，该运算符树可以在与管道连接的单独进程和处理器中执行（运算符间并行性） 2) 每个运算符消耗并生成可以被分区或分段为不相交子集以进行并行处理的集合（运算符内并行性）。 幸运的是，并行性在关系系统中易于利用的原因并不需要关系数据模型本身，只需要将查询作为运算符树中的数据项集进行处理即可。 这些正是 Volcano 设计中所做的假设，因此在 Volcano 中并行化可扩展查询处理是合乎逻辑的。</p>
<p>When Volcano was ported to a multiprocessor machine, it was desirable to use all single-process query processing code existing at that point without any change.The result is very clean, self-scheduling parallel processing. We call this novel approach the operator model of parallelizing a query evaluation engine [20].*In this model, all parallelism issues are localized in one operator that uses and provides the standard iterator interface to the operators above and below in a query tree.</p>
<p>当 Volcano 被移植到多处理器机器上时，希望使用当时存在的所有单进程查询处理代码而不进行任何更改。结果是非常干净的、自调度的并行处理。 我们将这种新颖的方法称为并行化查询评估引擎的运算符模型[20]。*在此模型中，所有并行问题都集中在一个运算符中，该运算符使用标准迭代器接口并向查询树中的上方和下方运算符提供标准迭代器接口。</p>
<p>The module responsible for parallel execution and syn- chronization is called the <strong>exchange</strong> iterator in Volcano. Notice that it is an iterator with <strong>open, next,</strong> and <strong>close</strong> pro- cedures; therefore, it can be inserted at any one place or at multiple places in a complex query tree. Fig. <em>5</em> shows a complex query execution plan that includes data pro- cessing operators, i.e., file scans and joins, and exchange operators. The next two figures will show the processes created when this plan is executed.</p>
<p>负责并行执行和同步的模块在 Volcano 中称为交换迭代器。 请注意，它是一个具有 open、next 和 close 过程的迭代器； 因此，它可以插入到复杂查询树中的任意一处或多处。 图 5 显示了一个复杂的查询执行计划，其中包括数据处理运算符，即文件扫描和连接以及交换运算符。 接下来的两幅图将显示执行该计划时创建的流程。</p>
<p><img src="/img/volcano-pic/image-20230720160833887.png"></p>
<p>This section describes how the <strong>exchange</strong> iterator im- plements vertical and horizontal parallelism followed by discussions of alternative modes of operation of Vol- cano’s <strong>exchange</strong> operator and modifications to the file system required for multiprocess query evaluation. The description goes into a fair amount of detail since the <strong>ex- change</strong> operator adds significantly to the power of Vol- cano. In fact, it represents a new concept in parallel query execution that is likely to prove useful in parallelizing both existing commercial database products and extensible sin- gle-process systems. It is described here for shared-mem- ory systems only; considerations for the distributed-mem- ory version are outlined as future work in the last section of this paper.</p>
<p>本节介绍交换迭代器如何实现垂直和水平并行性，然后讨论 Volcano 交换运算符的替代操作模式以及对多进程查询评估所需的文件系统的修改。 由于交易所运营商显着增强了 Volcano 的功能，因此描述非常详细。 事实上，它代表了并行查询执行的一个新概念，可能在现有商业数据库产品和可扩展单进程系统的并行化方面很有用。 此处仅针对共享内存系统进行描述； 本文最后一部分概述了分布式内存版本的考虑因素作为未来的工作。</p>
<h4 id="A-Vertical-Parallelism-垂直并行性"><a href="#A-Vertical-Parallelism-垂直并行性" class="headerlink" title="A. Vertical Parallelism 垂直并行性"></a><strong>A. Vertical Parallelism</strong> 垂直并行性</h4><p>The first function of exchange is to provide <strong>vertical parallelism</strong> or pipelining between processes. The <strong>open</strong> procedure creates a new process after creating a data structure in shared memory called a <strong>port</strong> for synchroni- zation and data exchange. The child process is an exact duplicate of the parent process. The exchange operator then takes different paths in the parent and child pro- cesses.</p>
<p>交换的第一个功能是在进程之间提供<strong>垂直并行性</strong>或流水线。 <strong>open</strong> 过程在共享内存中创建称为<strong>端口</strong> 的数据结构后创建一个新进程，用于同步和数据交换。 子进程是父进程的精确副本。 然后交换操作符在父进程和子进程中采取不同的路径。</p>
<p>The parent process serves as the consumer and the child process as the producer in Volcano. The exchange oper- ator in the consumer process acts as a normal iterator, the only difference from other iterators is that it receives its input via inter-process communication rather than iterator (procedure) calls. After creating the child process, open-exchange in the consumer is done. Next-exchange waits for data to arrive via the port and returns them a record at a time. Close-exchange informs the producer that it can close, waits for an acknowledgment, and re- turns.</p>
<p>在 Volcano 中，父进程充当消费者，子进程充当生产者。 消费者进程中的交换运算符充当普通迭代器，与其他迭代器的唯一区别是它通过进程间通信而不是迭代器（过程）调用接收输入。 创建子进程后，消费者中的开放交换就完成了。 Next-exchange 等待数据通过端口到达并一次返回一条记录。 关闭交换通知生产者它可以关闭，等待确认并返回。</p>
<p>Fig. <strong>6</strong> shows the processes created for vertical paral- lelism or pipelining by the exchange operators in the query plan of the previous figure. The exchange operators have created the processes, and are executing on both sides of the process boundaries, hiding the existence of process boundaries from the “work” operators. The fact that the join operators are executing within the same process, i.e., the placement of the exchange operators in the query tree, was arbitrary. The exchange operator provides only the mechanisms for parallel query evaluation, and many other choices (policies) would have been possible. In fact, the mechanisms provided in the operator model tend to be more flexible and amenable to more different policies than in the alternative bracket model <strong>[20].</strong></p>
<p>图 6 显示了上图查询计划中的交换操作符为垂直并行或流水线创建的进程。 交换操作员创建了流程，并在流程边界的两侧执行，向“工作”操作员隐藏了流程边界的存在。 事实上，连接运算符在同一进程中执行，即交换运算符在查询树中的放置是任意的。 交换运算符仅提供并行查询评估的机制，并且许多其他选择（策略）也是可能的。 事实上，与替代支架模型相比，运营商模型中提供的机制往往更加灵活并且能够适应更多不同的政策[20]。</p>
<p><img src="/img/volcano-pic/image-20230720160942516.png"></p>
<p>In the producer process, the exchange operator be- comes the <strong>driver</strong> for the query tree below the exchange operator using <strong>open, next,</strong> and <strong>close</strong> on its input. The out- put of <strong>next</strong> is collected in <strong>packets,</strong> which are arrays of <strong>Next-Record</strong> structures. The packet size is an argument in the exchange iterator’s state record, and can be set be- tween l and <strong>32</strong> 000 records. When a packet is filled, it is inserted into a linked list originating in the <strong>port</strong> and a semaphore is used to inform the consumer about the new packet. Records in packets are fixed in the shared buffer and must be unfixed by a consuming operator.</p>
<p>在生产者进程中，交换运算符成为交换运算符下方查询树的驱动程序，在其输入上使用 open、next 和 close。 next 的输出被收集在数据包中，这些数据包是 Next-Record 结构的数组。 数据包大小是交换迭代器状态记录中的一个参数，可以设置在 1 到 32 000 条记录之间。 当数据包被填满时，它将被插入到源自端口的链表中，并使用信号量来通知消费者有关新数据包的信息。 数据包中的记录固定在共享缓冲区中，并且必须由消费操作员取消固定。</p>
<p>When its input is exhausted, the exchange operator in the producer process marks the last packet with an end- of-stream tag, passes it to the consumer, and waits until the consumer allows closing all open files. This delay is necessary in Volcano because files on virtual devices must not be closed before all their records are unpinned in the buffer. In other words, it is a peculiarity due to other de- sign decisions in Volcano rather than inherent in the ex- change iterator on the operator model of parallelization.</p>
<p>当其输入耗尽时，生产者进程中的交换操作符会使用流结束标记标记最后一个数据包，将其传递给消费者，并等待消费者允许关闭所有打开的文件。 这种延迟在 Volcano 中是必要的，因为虚拟设备上的文件在其所有记录都在缓冲区中取消固定之前不得关闭。 换句话说，这是由于 Volcano 中的其他设计决策而产生的特性，而不是并行化操作模型上交换迭代器固有的特性。</p>
<p>The alert reader has noticed that the exchange module uses a different dataflow paradigm than all other operators. While all other modules are based on demand-driven dataflow (iterators, lazy evaluation), the producer-con- sumer relationship of exchange uses data-driven dataflow (eager evaluation). There are two reasons for this change in paradigms. First, we intend to use the exchange oper- ator also for horizontal parallelism, to be described be- low, which is easier to implement with data-driven data- flow. Second, this scheme removes the need for request messages. Even though a scheme with request messages, e.g., using a semaphore, would probably perform accept- ably on a shared-memory machine, it would create un- necessary control overhead and delays. Since very-high degrees of parallelism and very-high-performance query evaluation require a closely tied network, e.g., a hyper- cube, of shared-memory machines, we decided to use a paradigm for data exchange that has been proven to per- form well in a “shared-nothing” database machine [111.<br>A run-time switch of exchange enablesjow control or back pressure using an additional semaphore. If the pro- ducer is significantly faster than the consumer, the pro- ducer may pin a significant portion of the buffer, thus impeding overall system performance. If flow control is enabled, after a producer has inserted a new packet into the port, it must request the flow control semaphore. After a consumer has removed a packet from the port, it re- leases the flow control semaphore. The initial value of the flow control semaphore determines how many packets the producers may get ahead of the consumers.</p>
<p>细心的读者已经注意到，交换模块使用与所有其他运算符不同的数据流范例。 虽然所有其他模块都基于需求驱动的数据流（迭代器、惰性求值），但生产者-消费者交换关系使用数据驱动的数据流（热切求值）。 这种范式的改变有两个原因。 首先，我们打算将交换运算符也用于水平并行性，如下所述，这更容易通过数据驱动的数据流来实现。 其次，该方案消除了对请求消息的需要。 尽管带有请求消息的方案（例如使用信号量）可能在共享内存机器上表现良好，但它会产生不必要的控制开销和延迟。 由于非常高的并行度和非常高性能的查询评估需要一个紧密相连的网络，例如共享内存机器的超立方体，因此我们决定使用一种已经被证明可以满足以下条件的数据交换范例： 在“无共享”数据库机器中很好地形成[111。<br>交换的运行时开关可以使用附加信号量进行流控制或背压。 如果生产者明显快于消费者，则生产者可能会固定缓冲区的很大一部分，从而影响整体系统性能。 如果启用了流量控制，则在生产者将新数据包插入端口后，它必须请求流量控制信号量。 当消费者从端口移除数据包后，它会释放流量控制信号量。 流量控制信号量的初始值决定了生产者可以领先于消费者多少个数据包。</p>
<p>Notice that flow control and demand-driven dataflow are not the same. One significant difference is that flow control allows some “slack” in the synchronization of producer and consumer and therefore truly overlapped ex- ecution, while demand-driven dataflow is a rather rigid structure of request and delivery in which the consumer waits while the producer works on its next output. The second significant difference is that data-driven dataflow is easier to combine efficiently with horizontal parallelism and partitioning.</p>
<p>请注意，流控制和需求驱动的数据流并不相同。 一个显着的区别是，流控制允许生产者和消费者的同步存在一定的“松弛”，因此真正实现了重叠执行，而需求驱动的数据流是一种相当严格的请求和交付结构，其中消费者在生产者工作时等待 关于其下一个输出。 第二个显着区别是数据驱动的数据流更容易与水平并行性和分区有效地结合。</p>
<h4 id="B-Horizontal-Parallelism-水平并行性"><a href="#B-Horizontal-Parallelism-水平并行性" class="headerlink" title="B. Horizontal Parallelism 水平并行性"></a><strong>B. Horizontal Parallelism</strong> 水平并行性</h4><p>There are two forms of horizontal parallelism, which we call <strong>bushy parallelism</strong> and <strong>intra-operator</strong> parallelism. In bushy parallelism, different CPU’s execute different subtrees of a complex query tree. Bushy parallelism and vertical parallelism are forms of <strong>inter-operator</strong> parallel- ism. Intra-operator parallelism means that several CPU’s perform the same operator on different subsets of a stored dataset or an intermediate result.</p>
<p>水平并行有两种形式，我们称之为浓密并行和运算符内并行。 在密集并行中，不同的 CPU 执行复杂查询树的不同子树。 Bushy 并行和垂直并行是运算符间并行的形式。 运算符内并行性意味着多个 CPU 对存储数据集或中间结果的不同子集执行相同的运算符。</p>
<p>Bushy parallelism can easily be implemented by insert- ing one or two exchange operators into a query tree. For example, in order to sort two inputs into a merge-join in parallel, the first or both inputs are separated from the merge-join by an exchange operation. The parent process turns to the second sort immediately after forking the child process that will produce the first input in sorted order. Thus, the two sort operations are working in parallel.</p>
<p>通过将一两个交换运算符插入到查询树中，可以轻松实现密集并行。 例如，为了并行地将两个输入排序到合并连接中，第一个或两个输入通过交换操作与合并连接分开。 父进程在分叉子进程后立即转向第二种排序，该子进程将按排序顺序生成第一个输入。 因此，两个排序操作是并行工作的。</p>
<p>Intra-operator parallelism requires data partitioning. Partitioning of stored datasets is achieved by using mul- tiple files, preferably on different devices. Partitioning of intermediate results is implemented by including multiple queues in a port. If there are multiple consumer pro- cesses, each uses its own input queue. The producers use a support function to decide into which of the queues (or actually, into which of the packets being filled by the pro- ducer) an output record must go. Using a support function allows implementing round-robin-, key-range-, or hash- partitioning.</p>
<p>运算符内并行性需要数据分区。 存储数据集的分区是通过使用多个文件来实现的，最好是在不同的设备上。 中间结果的分区是通过在端口中包含多个队列来实现的。 如果有多个消费者进程，则每个进程都使用自己的输入队列。 生产者使用支持函数来决定输出记录必须进入哪个队列（或者实际上，进入由生产者填充的哪个数据包）。 使用支持函数可以实现循环、键范围或散列分区。</p>
<p>Fig. 7 shows the processes created for horizontal par- allelism or partitioning by the exchange operators in the query plan shown earlier. The join operators are executed by three processes while the file scan operators are exe- cuted by one or two processes each, typically scanning file partitions on different devices. To obtain this group- ing of processes, the only difference to the query plan used for the previous figure is that the “degree of parallelism” arguments in the exchange state records have to be set to 2 or <strong>3,</strong> respectively, and that partitioning support func- tions must be provided for the exchange operators that transfer file scan output to the joint processes. All file scan processes can transfer data to all join processes; however, data transfer between the join operators occurs only within each of the join processes. Unfortunately, this restriction renders this parallelization infeasible if the two joins are on different attributes and partitioning-based parallel join methods are used. For this case, a variant of exchange is supported in Volcano exchange operator called <strong>inrer- change,</strong> which is described in the next section.</p>
<p>图 7 显示了前面显示的查询计划中交换运算符为水平并行或分区创建的进程。 连接运算符由三个进程执行，而文件扫描运算符分别由一两个进程执行，通常扫描不同设备上的文件分区。 为了获得这个进程分组，与上图使用的查询计划的唯一区别是交换状态记录中的“并行度”参数必须分别设置为 2 或 3，并且分区支持 必须为交换操作员提供将文件扫描输出传输到联合进程的功能。 所有文件扫描进程都可以向所有连接进程传输数据； 然而，连接运算符之间的数据传输仅发生在每个连接进程内。 不幸的是，如果两个连接位于不同的属性上并且使用基于分区的并行连接方法，则此限制使得这种并行化不可行。 对于这种情况，Volcano 交换操作符支持一种称为 inrer-change 的交换变体，这将在下一节中进行描述。</p>
<p><img src="/img/volcano-pic/image-20230720161343962.png"></p>
<p>​	If an operator or an operator subtree is executed in par- allel by a group of processes, one of them is designated the master. When a query tree is opened, only one process is running, which is naturally the master. When a master forks a child process in a producer-consumer relation- ship, the child process becomes the master within its group. The first action of the master producer is to determine how many slaves are needed by calling an appro- priate support function. If the producer operation is to run in parallel, the master producer forks the other producer processes.</p>
<p>如果一个运算符或一个运算符子树由一组进程并行执行，则其中一个被指定为主进程。 当一棵查询树被打开时，只有一个进程在运行，这自然是主进程。 当主进程在生产者-消费者关系中分叉子进程时，子进程就成为其组内的主进程。 主生产者的第一个动作是通过调用适当的支持函数来确定需要多少个从设备。 如果生产者操作要并行运行，则主生产者会分叉其他生产者进程。</p>
<p>After all producer processes are forked, they run with- out further synchronization among themselves, with two exceptions. First, when accessing a shared data structure, e.g., the port to the consumers or a buffer table, short- term locks must be acquired for the duration of one linked- list insertion. Second, when a producer group is also a consumer group, i.e., there are at least two exchange op- erators and three process groups involved in a vertical pipeline, the processes that are both consumers and pro- ducers synchronize twice. During the (very short) interval between synchronizations, the master of this group cre- ates a port that serves all processes in its group.</p>
<p>在所有生产者进程被分叉后，它们之间的运行不会进一步同步，但有两个例外。 首先，当访问共享数据结构时，例如消费者的端口或缓冲表，必须在一次链表插入期间获取短期锁。 其次，当生产者组也是消费者组时，即垂直管道中至少涉及两个交换操作符和三个进程组时，既是消费者又是生产者的进程会同步两次。 在同步之间的（非常短的）间隔期间，该组的主设备创建一个为其组中的所有进程提供服务的端口。</p>
<p>When a <strong>close</strong> request is propagated down the tree and reaches the first exchange operator, the master consum- er’s <strong>close-exchange</strong> procedure informs all producer pro- cesses that they are allowed to close down using the semaphore mentioned above in the discussion on vertical parallelism. If the producer processes are also consumers, the master of the process group informs its producers, etc. <strong>In</strong> this way, all operators are shut down in an orderly fash- ion, and the entire query evaluation is self-scheduling.</p>
<p>当关闭请求沿着树传播并到达第一个交换操作符时，主消费者的关闭交换过程通知所有生产者进程，它们可以使用上面在垂直并行性讨论中提到的信号量来关闭。 如果生产者进程也是消费者，则进程组的主进程会通知其生产者等。这样，所有操作符都会以有序的方式关闭，并且整个查询评估是自我调度的。</p>
<h4 id="C-Variants-of-the-Exchange-Operator-Exchange-Operator-的变体"><a href="#C-Variants-of-the-Exchange-Operator-Exchange-Operator-的变体" class="headerlink" title="C. Variants of the Exchange Operator Exchange Operator 的变体"></a><strong>C. Variants</strong> <strong>of</strong> <strong>the Exchange Operator</strong> Exchange Operator 的变体</h4><p>There are a number of situations for which the <strong>ex-</strong> <strong>change</strong> operator described so far required some modifi- cations or extensions. In this section, we outline addi- tional capabilities implemented in Volcano’s exchange operator. All of these variants have been implemented in the <strong>exchange</strong> operator and are controlled by arguments in the state record.</p>
<p>到目前为止所描述的交换运营商在许多情况下都需要进行一些修改或扩展。 在本节中，我们概述了 Volcano 交易所运营商实现的其他功能。 所有这些变体都已在交换运算符中实现，并由状态记录中的参数控制。</p>
<p>For some operations, it is desirable to replicate or broadcast a stream to all consumers. For example, one of the two partitioning methods for hash-division [16] re- quires that the divisor be replicated and used with each partition of the dividend. Another example are fragment- and-replicate parallel join algorithms in which one of the two input relations is not moved at all while the other relation is sent to all processors. To support these algo- rithms, the exchange operator can be directed to send all records to all consumers, after pinning them appropriately multiple times in the buffer pool. Notice that it is not nec- essary to copy the records since they reside in a shared buffer pool; it is sufficient to pin them such that each con- sumer can unpin them as if it were the only process using them.</p>
<p>对于某些操作，需要将流复制或广播给所有消费者。 例如，哈希除法[16]的两种分区方法之一要求复制除数并与被除数的每个分区一起使用。 另一个例子是片段复制并行连接算法，其中两个输入关系之一根本不移动，而另一个关系被发送到所有处理器。 为了支持这些算法，可以指示交换操作员将所有记录发送给所有消费者，然后将它们适当地固定在缓冲池中多次。 请注意，没有必要复制记录，因为它们驻留在共享缓冲池中； 固定它们就足够了，这样每个消费者都可以取消固定它们，就好像它是唯一使用它们的进程一样。</p>
<p>During implementation and benchmarking of parallel sorting <strong>[18],</strong> [21], we added two more features to <strong>ex- change.</strong> First, we wanted to implement a merge network in which some processors produce sorted streams merge concurrently by other processors. Volcano’s <strong>sort</strong> iterator can be used to generate a sorted stream. <strong>A</strong> <strong>merge</strong> iterator was easily derived from the sort module. It uses a single level merge, instead of the cascaded merge of runs used in sort. The input of <strong>a</strong> <strong>merge</strong> iterator is an <strong>exchange,</strong> Dif- ferently from other operators, the merge iterator requires to distinguish the input records by their producer. <strong>As</strong> an example, for a join operation it does not matter where the input records were created, and all inputs can be accu- mulated in a single input stream. For a merge operation, it is crucial to distinguish the input records by their pro- ducer in order to merge multiple sorted streams correctly.</p>
<p>在并行排序[18]、[21]的实现和基准测试期间，我们添加了另外两个特征来交换。 首先，我们想要实现一个合并网络，其中一些处理器生成排序流，同时由其他处理器进行合并。 Volcano 的排序迭代器可用于生成排序流。 合并迭代器很容易从排序模块中派生出来。 它使用单级合并，而不是排序中使用的级联合并。 合并迭代器的输入是一个交换，与其他运算符不同，合并迭代器需要通过其生产者来区分输入记录。 例如，对于连接操作，输入记录在哪里创建并不重要，所有输入都可以累积在单个输入流中。 对于合并操作，区分输入记录的生产者至关重要，以便正确合并多个排序流。</p>
<p>We modified the <strong>exchange</strong> module such that it can keep the input records separated according to their producers. <strong>A</strong> third argument to <strong>next-exchange</strong> is used to communi- cate the required producer from the <strong>merge</strong> to the <strong>exchange</strong> iterator. Further modifications included increasing the number of input buffers used by <strong>exchange,</strong> the number of semaphores (including for flow control) used between producer and consumer part of <strong>exchange,</strong> and the logic for <strong>end-of-stream.</strong> All these modifications were imple- mented in such a way that they support multilevel merge trees, e.g., a parallel binary merge tree as used in <strong>[3].</strong> The merging paths are selected automatically such that the load is distributed as evenly as possible in each level.</p>
<p>我们修改了交换模块，以便它可以根据输入记录的生产者将输入记录分开。 next-exchange 的第三个参数用于将合并所需的生产者传递给交换迭代器。 进一步的修改包括增加交换使用的输入缓冲区的数量、交换的生产者和消费者部分之间使用的信号量（包括用于流量控制的）数量以及流结束的逻辑。 所有这些修改都是以支持多级合并树的方式实现的，例如[3]中使用的并行二元合并树。 合并路径是自动选择的，以便负载尽可能均匀地分布在每个级别中。</p>
<p>Second, we implemented a sort algorithm that sorts data randomly partitioned (or “striped” [29]) over multiple disks into a range-partitioned file with sorted partitions, i.e., a sorted file distributed over multiple disks. When using the same number of processors and disks, two pro- cesses per CPU were required, one to perform the file scan and partition the records and another one to sort them. Creating and running more processes than proces- sors can inflict a significant cost since these processes compete for the CPU’s and therefore require operating system scheduling.</p>
<p>其次，我们实现了一种排序算法，将多个磁盘上随机分区（或“条带化”[29]）的数据排序为具有排序分区的范围分区文件，即分布在多个磁盘上的排序文件。 当使用相同数量的处理器和磁盘时，每个 CPU 需要两个进程，一个执行文件扫描并对记录进行分区，另一个对记录进行排序。 创建和运行比处理器更多的进程可能会造成巨大的成本，因为这些进程会竞争 CPU，因此需要操作系统调度。</p>
<p>In order to make better use of the available processing power, we decided to redue the number of processes by half, effectively moving to one process per CPU. This required modifications to the exchange operator. Until then, the exchange operator could “live” only at the top or the bottom of the operator tree in a process. Since the modification, the exchange operator can also be in the middle of a process’ operator tree. When the exchange operator is opened, it does not fork any processes but es- tablishes a communication port for data exchange. The next operation requests records from its input tree, pos- sibly sending them off to other processes in the group, until a record for its own partition is found. This mode of operation was termed interchange, and was referred to earlier in the discussion of Fig. 7.</p>
<p>为了更好地利用可用的处理能力，我们决定将进程数量减少一半，有效地改为每个 CPU 一个进程。 这需要对交换运营商进行修改。 在此之前，交换运算符只能“生存”在进程中运算符树的顶部或底部。 修改后，交换运算符也可以位于进程运算符树的中间。 当交换操作符打开时，它不会派生任何进程，而是建立一个用于数据交换的通信端口。 下一个操作从其输入树请求记录，可能将它们发送到组中的其他进程，直到找到其自己的分区的记录。 这种操作模式称为交换，并在前面图 7 的讨论中提到过。</p>
<p>This mode of operation also makes flow control obso- lete. A process runs a producer (and produces input for the other processes) only if it does not have input for the consumer. Therefore, if the producers are in danger of overrunning the consumers, none of the producer opera- tors gets scheduled, and the consumers consume the available records.</p>
<p>这种操作模式也使得流量控制变得过时。 仅当进程没有消费者的输入时，才会运行生产者（并为其他进程生成输入）。 因此，如果生产者面临超出消费者的危险，则不会调度任何生产者操作员，并且消费者会消耗可用记录。</p>
<h4 id="D-File-System-Modijications-修改"><a href="#D-File-System-Modijications-修改" class="headerlink" title="D. File System Modijications 修改"></a><strong>D. File System Modijications</strong> 修改</h4><p>The file system required some modifications to serve several processes concurrently. In order to restrict the ex- tent of such modifications, Volcano currently does not in- clude protection of files and records other than each disk’s volume table of contents. Furthermore, typically nonre- petitive actions like mounting a device must be invoked by the query root process before or after a query is eval- uated by multiple processes.</p>
<p>文件系统需要进行一些修改才能同时服务多个进程。 为了限制此类修改的范围，Volcano 目前不包括对每个磁盘卷内容表之外的文件和记录的保护。 此外，在多个进程评估查询之前或之后，查询根进程必须调用通常的非重复操作（例如安装设备）。</p>
<p>The most intricate changes were required for the <strong>bufer</strong> module. In fact, making sure the buffer manager would not be a bottleneck in a shared-memory machine was an interesting subproject independent of database query pro- cessing [<strong>181.</strong> Concurrency control in the buffer manager was designed to provide a testbed for future research with effective and efficient mechanisms, and not to destroy the separation of policies and mechanisms.</p>
<p>缓冲区模块需要进行最复杂的更改。 事实上，确保缓冲区管理器不会成为共享内存机器中的瓶颈是一个独立于数据库查询处理的有趣的子项目[181。 缓冲区管理器中的并发控制旨在为未来研究提供有效且高效的机制的测试平台，而不是破坏策略和机制的分离。</p>
<p>Using one exclusive lock is the simplest way to protect a buffer pool and its internal data structures. However, decreased concurrency would have removed most or all advantages of parallel query processing. Therefore, the buffer uses a two-level scheme. There is a lock for each buffer pool and one for each descriptor (page or cluster resident in the buffer). The buffer pool lock must be held while searching or updating the hash tables and bucket chains. It is never held while doing <strong>Z&#x2F;O;</strong> thus, it is never held for a long period of time. <strong>A</strong> descriptor or cluster lock must be held while doing <strong>I&#x2F;O</strong> or while updating a descrip- tor in the buffer, e.g., to decrease its fix count.</p>
<p>使用一个独占锁是保护缓冲池及其内部数据结构的最简单方法。 然而，并发性的降低会消除并行查询处理的大部分或全部优势。 因此，缓冲器采用两级方案。 每个缓冲池都有一把锁，每个描述符（驻留在缓冲区中的页或簇）都有一把锁。 在搜索或更新哈希表和桶链时必须持有缓冲池锁。 做 Z&#x2F;O 时绝不会持有它； 因此，它不会被长期持有。 在执行 I&#x2F;O 或更新缓冲区中的描述符时，必须保持描述符或簇锁，例如，为了减少其修复计数。</p>
<p>If a process finds a requested cluster in the buffer, it uses an atomic test-and-lock operation to lock the descrip- tor. If this operation fails, the pool lock is released, the operation delayed and restarted. It is necessary to restart the buffer operation including the hash table lookup be- cause the process that holds the lock might be replacing the requested cluster. Therefore, the requesting process must wait to determine the outcome of the prior operation. Using this restart-scheme for descriptor locks has the ad- ditional benefit of avoiding deadlocks. The four condi- tions for deadlock are mutual exclusion, hold-and-wait no preemption, and circular wait; Volcano’s restart scheme does not satisfy the second condition. On the other hand, starvation is theoretically possible but has become extremely unlikely after buffer modifications that basi- cally eliminated buffer contention.</p>
<p>如果进程在缓冲区中找到请求的簇，它就会使用原子测试和锁定操作来锁定描述符。 如果此操作失败，则释放池锁，操作延迟并重新启动。 有必要重新启动缓冲区操作（包括哈希表查找），因为持有锁的进程可能正在替换所请求的簇。 因此，请求进程必须等待才能确定先前操作的结果。 使用描述符锁的重新启动方案还有避免死锁的额外好处。 死锁的四个条件是互斥、保持等待无抢占、循环等待； Volcano的重启方案不满足第二个条件。 另一方面，饥饿在理论上是可能的，但在缓冲区修改基本上消除了缓冲区争用之后，这种情况变得极不可能发生。</p>
<p>In summary, the exchange module encapsulates paral- lel query processing in Volcano. It provides a large set of mechanisms useful in parallel query evaluation. Only very few changes had to be made in the buffer manager and the other file system modules to accommodate parallel exe- cution. The most important properties of the exchange module are that it implements three forms of parallel pro- cessing within a single module, that it makes parallel query processng entirely self-scheduling, supports a va- riety of policies, e.g., partitioning schemes or packet sizes, and that it did not require any changes in the exist- ing query processing modules, thus leveraging signifi- cantly the time and effort spent on them and allowing easy parallel implementation of new algorithms. It entirely separates data selection, manipulation, derivation, etc. from all parallelism issues, and may therefore prove use- ful in parallelizing other systems, both relational com- mercial and extensible research systems.</p>
<p>总之，交换模块封装了 Volcano 中的并行查询处理。 它提供了大量可用于并行查询评估的机制。 只需对缓冲区管理器和其他文件系统模块进行很少的更改即可适应并行执行。 交换模块最重要的属性是它在单个模块内实现了三种形式的并行处理，它使并行查询处理完全自我调度，支持各种策略，例如分区方案或数据包大小 ，并且它不需要对现有查询处理模块进行任何更改，从而显着地利用在这些模块上花费的时间和精力，并允许轻松并行实现新算法。 它将数据选择、操作、推导等与所有并行问题完全分开，因此可能在并行其他系统（关系商业系统和可扩展研究系统）时有用。</p>
<h3 id="VII-SUMMARAYN-D-CONCLUSIONS"><a href="#VII-SUMMARAYN-D-CONCLUSIONS" class="headerlink" title="VII. SUMMARAYN D CONCLUSIONS"></a>VII. <strong>SUMMAR</strong>A<strong>Y</strong>N D CONCLUSIONS</h3><p>We have described Volcano, a new query evaluation system that combines compactness, efficiency, extensi- bility, and parallelism in a dataflow query evaluation sys- tem. Compactness is achieved by focusing on few general algorithms. For example, the one-to-one match operator implements join, semi-join, our join, anti-join, duplica- tion elimination, aggregation, intersection, union, differ- ence, and anti-difference. Extensibility is achieved by im- plementing only one essential abstraction, streams, and by relying on imported <strong>support functions</strong> for object inter- pretation and manipulation. The details of streams, e.g., type and structure of their elements, are not part of the stream definition and its implementation, and can be de- termined at will, making Volcano a <strong>data-model-inde- pendent set processor.</strong> The separation of set processing control in <strong>iterators</strong> and object interpretation and manip- ulation through support functions contributes significantly to Volcano’s extensibility.</p>
<p>我们描述了 Volcano，一种新的查询评估系统，它在数据流查询评估系统中结合了紧凑性、效率、可扩展性和并行性。 紧凑性是通过关注少数通用算法来实现的。 例如，一对一匹配运算符实现连接、半连接、我们的连接、反连接、重复消除、聚合、交集、并集、差分和反差分。 可扩展性是通过仅实现一种基本抽象（流）并依靠导入的支持函数来进行对象解释和操作来实现的。 流的细节，例如其元素的类型和结构，不是流定义及其实现的一部分，可以随意确定，使 Volcano 成为数据模型独立的集合处理器。 迭代器中的集合处理控制与通过支持函数进行的对象解释和操作的分离极大地提高了 Volcano 的可扩展性。</p>
<p>The Volcano design and implementation was guided by a few simple but generally useful principles. First, Vol- cano implements <strong>mechanisms</strong> to support <strong>policies</strong> that can be determined by a human experimenter or a query opti- mizer. Second, operators are implemented as iterators to allow efficient transfer of data and control within a single process. Third, a uniform operator interface allows for integration of new query processing operators and algo- rithms. Fourth, the interpretation and manipulation of stream elements is consistently left open to allow sup- porting any data model and processing items of any type, shape, and representation. Finally, the encapsulated im- plementation of parallelism allows developing query pro- cessing algorithms in a single-process environment but executing them in parallel. These principles have led to a very flexible, extensible, and powerful query processing engine.</p>
<p>Volcano 的设计和实施遵循一些简单但普遍有用的原则。 首先，Volcano 实现了支持可由人类实验者或查询优化器确定的策略的机制。 其次，运算符被实现为迭代器，以允许在单个进程内有效地传输数据和控制。 第三，统一的运算符接口允许集成新的查询处理运算符和算法。 第四，流元素的解释和操作始终保持开放，以允许支持任何数据模型和处理任何类型、形状和表示的项。 最后，并行性的封装实现允许在单进程环境中开发查询处理算法，但并行执行它们。 这些原则导致了非常灵活、可扩展且强大的查询处理引擎。</p>
<p>Volcano introduces two novel <strong>meta-operators.</strong> Dy- namic query evaluation plans are a new concept intro- duced in [171that allow efficientevaluation of queries with free variables. The <strong>choose-plan</strong> meta-operator at the top of a plan or a subplan makes an efficient decision which alternative plan to use when the plan is invoked. Dynamic plans have the potential of increasing the performance of embedded and repetitive queries significantly.</p>
<p>Volcano 引入了两个新颖的元运算符。 动态查询评估计划是[171]中引入的一个新概念，它允许使用自由变量对查询进行有效评估。 计划或子计划顶部的选择计划元运算符可以有效地决定在调用计划时使用哪个替代计划。 动态计划有可能显着提高嵌入式和重复查询的性能。</p>
<p>Dataflow techniques are used within processes as well as between processes. Within a process, demand-driven dataflow is implemented by means of streams and itera- tors. Streams and iterators represent the most efficient ex- ecution model in terms of time and space for single-pro- cess query evaluation. Between processes, data-driven dataflow is used to pass data between producers and con- sumers efficiently. If necessary, Volcano’s data-driven dataflow can be augmented with flow control or back pressure. Horizontal partitioning can be used both on stored and intermediate datasets to allow intra-operator parallelism. The design of the <strong>exchange</strong> meta-operator encapsulates the parallel execution mechanism for verti- cal, bushy, and intra-operator parallelism, and it performs the translations from demand-driven to data-driven data- flow and back [20].</p>
<p>数据流技术在进程内以及进程之间使用。 在流程中，需求驱动的数据流是通过流和迭代器来实现的。 流和迭代器代表了单进程查询评估的时间和空间上最有效的执行模型。 在进程之间，数据驱动的数据流用于在生产者和消费者之间有效地传递数据。 如有必要，可以通过流量控制或背压来增强 Volcano 的数据驱动数据流。 水平分区可用于存储数据集和中间数据集，以允许运算符内并行性。 交换元操作符的设计封装了垂直、密集和操作符内并行性的并行执行机制，并执行从需求驱动到数据驱动的数据流的转换[20]。</p>
<p>Encapsulating all issues of parallelism control in one operator and thus orthogonalizing data manipulation and parallelism offers important extensibility and portability advantages. All data manipulation operators are shielded from parallelism issues, and have been designed, de- bugged, tuned, and preliminarily evaluated in a single- process environment. To parallelize a new operator, it only has to be combined with the exchange operator in a query evaluation plan. T o port all V olcano operators to a new parallel machine, only the exchange operator re- quires appropriate modifications. At the current time, the exchange operator supports parallelism only on shared- memory machines. We are currently working on extend- ing this operator to support query processing on distrib- uted-memory machines while maintaining its encapsula- tion properties. However, we do not want to give up the advantages of shared memory, namely fast communica- tion and synchronization. <strong>A</strong> recent investigation demon- strated that shared-memory architectures can deliver near- linear speed-up for limited degrees of parallelism; we ob- served a speed-up of 14.9 with 16 CPU’s for parallel sort- ing in Volcano [18]. To combine the bets of both worlds, we are building our software such that it runs on a closely- tied group, e.g., a hypercube or mesh architecture, of shared-memory parallel machines. Once this version of Volcano’s exchange operator and therefore all of Volcano runs on such machines, we can investigate query process- ing on hierarchical architectures and heuristics of how CPU and <strong>U0</strong> power as well as memory can best be placed and exploited in such machines.</p>
<p>将并行控制的所有问题封装在一个运算符中，从而使数据操作和并行正交化，提供了重要的可扩展性和可移植性优势。 所有数据操作运算符都不受并行性问题的影响，并且已在单进程环境中进行设计、调试、调整和初步评估。 要并行化新的运算符，只需将其与查询评估计划中的交换运算符组合即可。 要将所有 Volcano 操作器移植到新的并行机，仅交换操作器需要进行适当的修改。 目前，交换运算符仅在共享内存机器上支持并行性。 我们目前正在努力扩展该运算符以支持分布式内存机器上的查询处理，同时保持其封装属性。 然而，我们不想放弃共享内存的优点，即快速通信和同步。 最近的一项调查表明，共享内存架构可以为有限的并行度提供近线性的加速； 我们观察到在 Volcano 中使用 16 个 CPU 进行并行排序时速度提高了 14.9 [18]。 为了结合两个世界的赌注，我们正在构建我们的软件，使其运行在一个紧密联系的组上，例如共享内存并行机器的超立方体或网状架构。 一旦这个版本的 Volcano 交换运算符以及所有 Volcano 在此类机器上运行，我们就可以研究分层架构上的查询处理以及如何在此类机器中最好地放置和利用 CPU 和 U0 功率以及内存的启发式方法。</p>
<p>Most of today’s parallel machines are built as one of the two extreme cases of this hierarchical design: a dis- tributed-memory machine uses single-CPU nodes, while a shared-memory machine consists of a single node. Soft- ware designed for this hierarchical architecture will run on either conventional design as well as a genuinely hi- erarchical machine, and will allow exploring trade-offs in the range of altematives in between. Thus, the operator model of parallelization also offers the advantage of ar- chitecture- and topology-independent parallel query eval- uation [191.</p>
<p>当今大多数并行机都是作为这种分层设计的两个极端情况之一构建的：分布式内存机器使用单 CPU 节点，而共享内存机器由单个节点组成。 为这种分层架构设计的软件将运行在传统设计以及真正的分层机器上，并且允许探索之间的替代方案范围的权衡。 因此，并行化的运算符模型还提供了独立于架构和拓扑的并行查询评估的优点[191。</p>
<p>A number of features make Volcano an interesting ob- ject for further performance studies. First, the LRU&#x2F;MRU buffer replacement strategy switched by a keep-or-toss hint needs to be evaluated. Second, using clusters of different sizes on a single device and avoiding buffer shuf- fling by allocating buffer space dynamically instead of statically require careful evaluation. Third, the duality and trade-offs between sort- and hash-based query processing algorithms and their implementations will be explored further. Fourth, Volcano allows measuring the perfor- mance of parallel algorithms and identifying bottlenecks on a shared-memory architecture, as demonstrated for in- stance in [ 181. W e intend to perform similar studies on distributed-memory and, as they become available, hier- archical architectures, Fifth, the advantages and disad- vantages of a separate scheduler process in distributed- memory query processing (as used in GAMMA) will be evaluated. Finally, after data-driven dataflow has been shown to work well on a shared-nothing database machine [111, the combination of demand- and data-driven data- flow should be explored on a network on shared-memory computers.</p>
<p>许多特征使 Volcano 成为进一步性能研究的有趣对象。 首先，需要评估由 keep-or-toss 提示切换的 LRU&#x2F;MRU 缓冲区替换策略。 其次，在单个设备上使用不同大小的集群并通过动态而不是静态分配缓冲区空间来避免缓冲区改组需要仔细评估。 第三，将进一步探讨基于排序和基于散列的查询处理算法及其实现之间的二元性和权衡。 第四，Volcano 允许测量并行算法的性能并识别共享内存架构上的瓶颈，如[181]中所示。我们打算对分布式内存进行类似的研究，并且当它们可用时， 第五，将评估分布式内存查询处理（如 GAMMA 中使用的）中单独的调度程序进程的优点和缺点。 最后，在数据驱动的数据流被证明在无共享数据库机器上运行良好之后[111，应该在共享内存计算机的网络上探索需求驱动和数据驱动的数据流的组合。</p>
<p>While Volcano is a working system in its current form, we are considering several extensions and improvements. First, Volcano currently does very extensive error detec- tion, but it does not encapsulate errors in <strong>fail-fast</strong> mod- ules. It would be desirable to modify all modules such that they have all-or-nothing semantics for all requests. This might prove particularly tricky for the exchange module, even more so in a distributed-memory environment. Sec- ond, for a more complete performance evaluation, Vol- cano should be enhanced to a multiuser system that allows inter-query parallelism. Third, to make it a complete data manager and query processor, transactions semantics in- cluding recovery should be added.</p>
<p>虽然 Volcano 是目前形式的工作系统，但我们正在考虑进行一些扩展和改进。 首先，Volcano 目前进行了非常广泛的错误检测，但它没有将错误封装在快速失败模块中。 最好修改所有模块，使它们对所有请求都具有“全有或全无”语义。 这对于交换模块来说可能特别棘手，在分布式内存环境中更是如此。 其次，为了更完整的性能评估，Volcano 应该增强为允许查询间并行的多用户系统。 第三，为了使其成为一个完整的数据管理器和查询处理器，应该添加包括恢复在内的事务语义。</p>
<p>Volcano <strong>is</strong> the first operational query evaluation system that combines extensibility and parallelism. We believe that Volcano is a powerful tool for database systems re- search and education. We are making it available for stu- dent use, e . g . , for implementation and performance stud- ies, and have given copies to selected outside organizations. We intend to use it in a number of further research projects, including research on the optimization and evaluation of dynamic query evaluation plans [ 171 and the REVELATIOpNroject on query optimization and exe- cution in object-oriented database systems with encapsu- lated behavior [151.</p>
<p>Volcano是第一个结合了可扩展性和并行性的操作查询评估系统。 我们相信 Volcano 是数据库系统研究和教育的强大工具。 我们将其提供给学生使用，例如。 G 。 ，用于实施和绩效研究，并向选定的外部组织提供了副本。 我们打算在许多进一步的研究项目中使用它，包括动态查询评估计划的优化和评估的研究[171，以及关于具有封装行为的面向对象数据库系统中的查询优化和执行的REVELATIOpNroject研究[151] 。</p>
<h3 id="ACKNOWLEDGM"><a href="#ACKNOWLEDGM" class="headerlink" title="ACKNOWLEDGM"></a>ACKNOWLEDGM</h3><p>The one-to-one match operators were implemented by Tom Keller starting with existing hash join, hash aggregate, merge join, and sort code. Hash table overflow man- agement was added by Mark Swain. Dynamic query eval- uation plans and the choose-plan operator were designed and implemented by Karen Ward. We are also very much indebted to all members of the GAMMA and EXODUS projects. Leonard Shapiro contributed to the quality and clarity of the exposition with many insightful comments. David DeWitt, Jim Gray, David Maier, Bill McKenna, Marguerite Murphy, and Mike Stonebraker gave very helpful comments on earlier drafts of this paper. The anonymous referees gave some further helpful sugges- tions.</p>
<p>一对一匹配运算符是由 Tom Keller 从现有的哈希连接、哈希聚合、合并连接和排序代码开始实现的。 哈希表溢出管理是由 Mark Swain 添加的。 动态查询评估计划和选择计划运算符由 Karen Ward 设计和实现。 我们也非常感谢 GAMMA 和 EXODUS 项目的所有成员。 伦纳德·夏皮罗 (Leonard Shapiro) 发表了许多富有洞察力的评论，为展览的质量和清晰度做出了贡献。 David DeWitt、Jim Gray、David Maier、Bill McKenna、Marguerite Murphy 和 Mike Stonebraker 对本文的早期草稿给出了非常有帮助的评论。 匿名审稿人还提出了一些进一步的有益建议。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>执行器</tag>
      </tags>
  </entry>
  <entry>
    <title>Generalized Isolation Level Definitions</title>
    <url>/2023/11/06/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/1-Generalized%20Isolation%20Level%20Definitions/</url>
    <content><![CDATA[<p><em>Commercial databases support different isolation levels to allow programmers to trade off consistency for a poten- tial gain in performance. The isolation levels are defined in the current ANSI standard, but the definitions are ambigu- ous and revised definitions proposed to correct the problem are too constrained since they allow only pessimistic (lock- ing) implementations. This paper presents new specifica- tions for the ANSI levels. Our specifications are</em> portable*; they apply not only to locking implementations, but also to optimistic and multi-version concurrency control schemes. Furthermore, unlike earlier definitions, our new specifica- tions handle predicates in a correct and flexible manner at all levels.*</p>
<p>商业数据库支持不同的隔离级别，使程序员可以牺牲一致性来获得潜在的性能提升。 隔离级别在当前的 ANSI 标准中进行了定义，但这些定义是不明确的，并且为纠正该问题而提出的修订定义过于受限，因为它们只允许悲观（锁定）实现。 本文提出了 ANSI 级别的新规范。 我们的规格是便携式的； 它们不仅适用于锁定实现，还适用于乐观和多版本并发控制方案。 此外，与早期的定义不同，我们的新规范在各个级别以正确且灵活的方式处理谓词。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><strong>1. Introduction</strong></h2><p>This paper gives new, precise definitions of the ANSI- SQL isolation levels [6]. Unlike previous proposals [13, 6, 8], the new definitions are both correct (they rule out all bad histories) and implementation-independent. Our spec- ifications allow a wide range of concurrency control tech- niques, including locking, optimistic techniques [20, 2, 5], and multi-version mechanisms [9, 24]. Thus, they meet the goals of ANSI-SQL and could be used as an isolation standard.</p>
<p>本文给出了 ANSI-SQL 隔离级别的新的、精确的定义 [6]。 与之前的提案 [13,6,8] 不同，新定义既正确（它们排除了所有不良历史）又独立于实现。 我们的规范允许广泛的并发控制技术，包括锁定、乐观技术[20,2,5]和多版本机制[9,24]。 因此，它们满足 ANSI-SQL 的目标，并且可以用作隔离标准。</p>
<p>The concept of isolation levels was first introduced in [13] under the name <em>Degrees of Consistency</em>. The goal of this work was to provide improved concurrency for workloads by sacrificing the guarantees of perfect isolation. The work in [13] and some refinements suggested by [11] set the stage for the ANSI&#x2F;ISO SQL-92 definitions for isolation levels [6], where the goal was to develop a standard that was implementation-independent. However, a subsequent paper [8] showed that the definitions provided in [6] were ambiguous. That paper proposed different definitions that avoided the ambiguity problems, but, as stated in [8], these definitions were simply “disguised versions of locking” and therefore disallow optimistic and multi-version mech- anisms. Thus, these definitions fail to meet the goals of ANSI-SQL with respect to implementation-independence.</p>
<p>隔离级别的概念首次在[13]中以“一致性程度”的名称引入。 这项工作的目标是通过牺牲完美隔离的保证来提高工作负载的并发性。 [13] 中的工作和 [11] 建议的一些改进为隔离级别的 ANSI&#x2F;ISO SQL-92 定义奠定了基础 [6]，其目标是开发一个独立于实现的标准。 然而，随后的论文[8]表明[6]中提供的定义是不明确的。 该论文提出了避免歧义问题的不同定义，但是，正如[8]中所述，这些定义只是“锁定的变相版本”，因此不允许乐观和多版本机制。 因此，这些定义无法满足 ANSI-SQL 在实现独立性方面的目标。</p>
<p>Thus, we have a problem: the standard is intended to be implementation-independent, but lacks a precise definition that meets this goal. Implementation-independence is im- portant since it provides flexibility to implementors, which can lead to better performance. Optimism can outperform locking in some environments, such as large scale, wide- area distributed systems [2, 15]; optimistic mechanisms are the schemes of choice for mobile environments; and Gem- stone [22] and Oracle [24] provide serializability and Snap- shot Isolation, respectively, using multi-version optimistic implementations. It is undesirable for the ANSI standard to rule out these implementations. For example, Gemstone provides serializability even though it does not meet the locking-based rules given in [8].</p>
<p>因此，我们遇到了一个问题：该标准旨在独立于实现，但缺乏满足此目标的精确定义。 实现独立性很重要，因为它为实现者提供了灵活性，从而可以带来更好的性能。 在某些环境中，乐观可以胜过锁定，例如大规模、广域分布式系统 [2, 15]； 乐观机制是移动环境的首选方案； Gemstone [22] 和 Oracle [24] 使用多版本乐观实现分别提供可序列化和快照隔离。 ANSI 标准排除这些实现是不可取的。 例如，Gemstone 提供了可序列化性，即使它不满足 [8] 中给出的基于锁定的规则。</p>
<p>This paper presents new implementation-independent specifications that correct the problems with the existing definitions. Our definitions cover the weaker isolation lev- els that are in everyday use: Most database vendors and database programmers take advantage of levels below se- rializability levels to achieve better performance; in fact, READ COMMITTED is the default for some database products and database vendors recommend using this level instead of serializability if high performance is desired. Our defi- nitions also enable database vendors to develop innovative implementations of the different levels using a wide variety of concurrency control mechanisms including locking, op- timistic and multi-version mechanisms. Furthermore, our specifications handle predicate-based operations correctly at all isolation levels.</p>
<p>本文提出了新的独立于实现的规范，纠正了现有定义的问题。 我们的定义涵盖了日常使用的较弱隔离级别：大多数数据库供应商和数据库程序员利用低于可串行性级别的级别来实现更好的性能； 事实上，READ COMMITTED 是某些数据库产品的默认设置，如果需要高性能，数据库供应商建议使用此级别而不是串行化。 我们的定义还使数据库供应商能够使用各种并发控制机制（包括锁定、乐观和多版本机制）开发不同级别的创新实现。 此外，我们的规范在所有隔离级别正确处理基于谓词的操作。</p>
<p>Thus, the paper makes the following contributions:</p>
<p>因此，本文做出以下贡献：</p>
<ul>
<li>It specifies the existing ANSI isolation levels in an implementation-independent manner. The definitions are correct (they rule out all bad histories). They are also complete (they allow all good histories) for serializability; in particular, they provide conflict- serializability [9]. It is difficult to prove completeness for lower isolation levels, but we can easily show that our definitions are more permissive than those given in [8].</li>
</ul>
<p>  它以独立于实现的方式指定现有的 ANSI 隔离级别。 定义是正确的（它们排除了所有不好的历史）。 它们对于可序列化也是完整的（它们允许所有好的历史记录）； 特别是，它们提供冲突可串行化[9]。 很难证明较低隔离级别的完整性，但我们可以轻松证明我们的定义比[8]中给出的定义更宽松。</p>
<ul>
<li><p>Our specifications also handle predicates correctly in a flexible manner; earlier definitions were either lock- based or incomplete [8].</p>
<p>我们的规范还以灵活的方式正确处理谓词； 早期的定义要么是基于锁的，要么是不完整的[8]。</p>
</li>
</ul>
<p>Our approach can be used to define additional levels as well, including commercial levels such as Cursor Stability [11], and Oracle’s Snapshot Isolation and Read Consistency [24], and new levels; for example, we have developed an ad- ditional isolation level called PL-2+, which is the weakest level that guarantees consistent reads and causal consistency with respect to transactions. Details can be found in [1].</p>
<p>我们的方法也可用于定义其他级别，包括商业级别，例如游标稳定性 [11]、Oracle 的快照隔离和读取一致性 [24]，以及新级别； 例如，我们开发了一个名为 PL-2+ 的附加隔离级别，这是保证事务一致性读取和因果一致性的最弱级别。 详细信息可以参见[1]。</p>
<p>Our definitions are given using a combination of con- straints on transaction histories and graphs; we proscribe different types of cycles in a serialization graph at each isolation level. Our graphs are similar to those that have been used before for specifying serializability [9, 19, 14], semantics-based correctness criterion [4], and for defining extended transaction models [10]. Our approach is the first that applies these techniques to defining ANSI and commer- cial isolation levels. Our specifications are different from the multi-version theory presented in [9] since that work only describes conditions for serializability whereas we specify all ANSI&#x2F;SQL-92 and other commercial isolation levels for multi-version systems. Furthermore, unlike our specifica- tions, their definitions do not take predicates into account. Our work is also substantially different from the definitions presented in [8] since our specifications handle multi-version systems, optimistic systems and also deal with predicates in a correct and flexible manner at all isolation levels.</p>
<p>我们的定义是结合交易历史和图表的约束给出的； 我们在每个隔离级别的序列化图中禁止不同类型的循环。 我们的图与之前用于指定可序列化性 [9,19,14]、基于语义的正确性标准 [4] 以及定义扩展事务模型 [10] 的图类似。 我们的方法是第一个将这些技术应用于定义 ANSI 和商业隔离级别的方法。 我们的规范与 [9] 中提出的多版本理论不同，因为该工作仅描述了可串行性的条件，而我们为多版本系统指定了所有 ANSI&#x2F;SQL-92 和其他商业隔离级别。 此外，与我们的规范不同，它们的定义没有考虑谓词。 我们的工作也与[8]中提出的定义有很大不同，因为我们的规范处理多版本系统、乐观系统，并且还在所有隔离级别以正确和灵活的方式处理谓词。</p>
<p>Relaxed correctness conditions based on semantics and extended transaction models have been suggested in the past [10, 4, 17, 7]. By contrast, our work focuses on specifying existing ANSI and commercial isolation levels that are being used by large numbers of application programmers.</p>
<p>过去已经提出了基于语义和扩展事务模型的宽松正确性条件[10,4,17,7]。 相比之下，我们的工作重点是指定大量应用程序员正在使用的现有 ANSI 和商业隔离级别。</p>
<p>The rest of this paper is organized as follows. Section 2 discusses prior work in more detail. Section 3 shows that the current definitions are inadequate and motivates the need for our work. Section 4 describes our database model. Section 5 provides our definitions for the existing ANSI isolation lev- els. We close in Section 6 with a discussion of what we have accomplished.</p>
<p>本文的其余部分安排如下。 第 2 节更详细地讨论了之前的工作。 第 3 节表明当前的定义是不充分的，并激发了我们工作的需要。 第 4 节描述了我们的数据库模型。 第 5 节提供了我们对现有 ANSI 隔离级别的定义。 我们在第 6 节结束时讨论了我们所取得的成就。</p>
<h2 id="2-Previous-Work"><a href="#2-Previous-Work" class="headerlink" title="2. Previous Work"></a><strong>2. Previous Work</strong></h2><p>The original proposal for isolation levels [13] introduced four degrees of consistency, degrees 0, 1, 2 and 3, where de- gree 3 was the same as serializability. That paper, however, was concerned with locking schemes, and as a consequence the definitions were not implementation-independent.</p>
<p>隔离级别的最初提议[13]引入了四种一致性程度：0、1、2 和 3 度，其中 3 度与可串行性相同。 然而，该论文关注的是锁定方案，因此定义并不是独立于实现的。</p>
<p>However, that work, together with the refinement of the levels provided by Date [11], formed the basis for the ANSI&#x2F;ISO SQL-92 isolation level definitions [6]. The ANSI standard had implementation-independence as a goal and the definitions were supposed to be less constraining than ear- lier ones. The approach taken was to proscribe certain types of bad behavior called <em>phenomena</em>; more restrictive consis- tency levels disallow more phenomena and serializability does not permit any phenomenon. The isolation levels were named READ UNCOMMITTED, READ COMMITTED, REPEAT- ABLE READ, and SERIALIZABLE; some of these levels were intended to correspond to the degrees of [13].</p>
<p>然而，这项工作与 Date [11] 提供的级别的细化一起，构成了 ANSI&#x2F;ISO SQL-92 隔离级别定义 [6] 的基础。 ANSI 标准以实现独立性为目标，并且其定义应该比早期的定义更少限制。 采取的方法是禁止某些类型的不良行为，称为现象； 更严格的一致性级别不允许更多的现象，并且可串行化不允许任何现象。 隔离级别被命名为 READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE； 其中一些级别旨在与[13]的程度相对应。</p>
<p>The work in [8] analyzed the ANSI-SQL standard and demonstrated several problems in its isolation level defini- tions: some phenomena were ambiguous, while others were missing entirely. It then provided new definitions. As with the ANSI-SQL standard, various isolation levels are defined by having them disallow various phenomena. The phenom- ena proposed by [8] are:</p>
<p>[8] 中的工作分析了 ANSI-SQL 标准并展示了其隔离级别定义中的几个问题：一些现象是不明确的，而另一些现象则完全缺失。 然后它提供了新的定义。 与 ANSI-SQL 标准一样，各种隔离级别是通过禁止各种现象来定义的。 [8]提出的现象是：</p>
<ol>
<li>P0:  w1[x] … w2[x] … (c1 or a1)</li>
<li>P1:  w1[x] … r2[x] … (c1 or a1)</li>
<li>P2:  r1[x] … w2[x] … (c1 or a1)</li>
<li>P3:  r1[P] … w2[y in P] … (c1 or a1)</li>
</ol>
<p>Proscribing P0 (which was missing in the ANSI-SQL defi- nitions) requires that a transaction T2 cannot write an object <em>x</em> if an uncommitted transaction T1 has already modified <em>x</em>. This is simply a disguised locking definition, requiring T1 and T2 to acquire long write-locks. (Long-term locks are held until the transaction taking them commits; short- term locks are released immediately after the transaction completes the read or write that triggered the lock attempt.) Similarly, proscribing P1 requires T1 to acquire a long write- lock and T2 to acquire (at least) a short-term read-lock, and proscribing P2 requires the use of long read and write locks.</p>
<p>禁止 P0（ANSI-SQL 定义中缺少）要求如果未提交的事务 T1 已经修改了 x，则事务 T2 不能写入对象 x。 这只是一个变相的锁定定义，要求T1和T2获取长写锁。 （长期锁会一直保持到获取它们的事务提交为止；短期锁会在事务完成触发锁尝试的读取或写入后立即释放。）类似地，禁止 P1 需要 T1 获取长写锁，并且 T2 获取（至少）短期读锁，而禁止 P2 需要使用长期读写锁。</p>
<p>Phenomenon P3 deals with the queries based on predi- cates. Proscribing P3 requires that a transaction T2 cannot modify a predicate P by inserting, updating, or deleting a row such that its modification changes the result of a query executed by an uncommitted transaction T1 based on pred- icate P; to avoid this situation, T1 acquires a long phantom read-lock [14] on predicate P.</p>
<p>现象 P3 处理基于谓词的查询。 禁止 P3 要求事务 T2 不能通过插入、更新或删除行来修改谓词 P，从而使其修改改变未提交事务 T1 基于谓词 P 执行的查询的结果； 为了避免这种情况，T1 在谓词 P 上获取长幻影读锁 [14]。</p>
<p>Thus, these definitions only allow histories that would occur in a system using long&#x2F;short read&#x2F;write item&#x2F;predicate locks. Since locking serializes transactions by preventing certain situations (e.g., two concurrent transactions both modifying the same object), <font color="red">we refer to this approach as the preventative approach.</font></p>
<p>因此，这些定义仅允许使用长&#x2F;短读&#x2F;写项&#x2F;谓词锁的系统中发生的历史记录。 由于锁定通过防止某些情况（例如，两个并发事务都修改同一对象）来序列化事务，因此我们将这种方法称为预防性方法。</p>
<p>Figure 1 summarizes the isolation levels as defined in [8] and relates them to a lock-based implementation. Thus the READ UNCOMMITTED level proscribes P0; READ COM- MITTED proscribes P0 and P1; the REPEATABLE READ level proscribes P0 - P2; and SERIALIZABLE proscribes P0 - P3.</p>
<p>图 1 总结了 [8] 中定义的隔离级别，并将它们与基于锁的实现相关联。 因此 READ UNCOMMITTED 级别禁止 P0； READ COMMITTED 禁止 P0 和 P1； 可重复读取级别规定 P0 - P2； 并且 SERIALIZABLE 禁止 P0 - P3。</p>
<p><strong>Figure 1. Consistency Levels and Locking ANSI-92 Isolation Levels</strong></p>
<table>
<thead>
<tr>
<th>Locking Isolation Level</th>
<th>Proscribed Phenomena</th>
<th>Read Locks on Data Items and Phantoms (same unless noted)</th>
<th>Write Locks on Data Items and Phantoms (always the same)</th>
</tr>
</thead>
<tbody><tr>
<td>Degree 0</td>
<td>none</td>
<td>none</td>
<td>Short write locks</td>
</tr>
<tr>
<td>Degree 1 &#x3D; Locking READ UNCOMMITTED</td>
<td>P0</td>
<td>none</td>
<td>Long write locks</td>
</tr>
<tr>
<td>Degree 2 &#x3D; Locking READ COMMITTED</td>
<td>P0, P1</td>
<td>Short read locks</td>
<td>Long write locks</td>
</tr>
<tr>
<td>Locking REPEATABLE READ</td>
<td>P0, P1, P2</td>
<td>Long data-item read locks,Short phantom read locks</td>
<td>Long write locks</td>
</tr>
<tr>
<td>Degree 3 &#x3D; Locking SERIALIZABLE</td>
<td>P0, P1, P2, P3</td>
<td>Long read locks</td>
<td>Long write locks</td>
</tr>
</tbody></table>
<h2 id="3-Restrictiveness-of-Preventative-Approach"><a href="#3-Restrictiveness-of-Preventative-Approach" class="headerlink" title="3. Restrictiveness of Preventative Approach"></a><strong>3. Restrictiveness of Preventative Approach</strong></h2><p>预防方法的限制性</p>
<p>We now show that the preventative approach is overly restrictive since it rules out optimistic and multi-version implementations. As mentioned, this approach disallows all histories that would not occur in a locking scheme and <em>prevents</em> conflicting operations from executing concurrently.</p>
<p>我们现在表明，预防性方法过于严格，因为它排除了乐观和多版本的实现。 如前所述，此方法不允许在锁定方案中不会发生的所有历史记录，并防止同时执行冲突的操作。</p>
<p>The authors in [8] wanted to ensure that multi-object con- straints (e.g., constraints like x + y &#x3D; 10) are not observed as violated by transactions that request an isolation level such as serializability. They showed that histories such as H1 and H2 are allowed by one interpretation of the ANSI standard (at the SERIALIZABLE isolation level) even though they are non-serializable:</p>
<p>[8] 中的作者希望确保请求隔离级别（例如可序列化性）的事务不会违反多对象约束（例如 x + y &#x3D; 10 等约束）。 他们表明，诸如 H1 和 H2 之类的历史记录是 ANSI 标准的一种解释所允许的（在 SERIALIZABLE 隔离级别），即使它们是不可序列化的：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">H1: r1(x, <span class="number">5</span>) w1(x, <span class="number">1</span>) r2(x, <span class="number">1</span>) r2(y, <span class="number">5</span>) c2 r1(y, <span class="number">5</span>) w1(y, <span class="number">9</span>) c1 </span><br><span class="line">H2: r2(x, <span class="number">5</span>) r1(x, <span class="number">5</span>) w1(x, <span class="number">1</span>) r1(y, <span class="number">5</span>) w1(y, <span class="number">9</span>) c1 r2(y, <span class="number">9</span>) c2</span><br></pre></td></tr></table></figure>

<p>In both cases, T2 observes an inconsistent state (it observes invariant x + y &#x3D; 10 to be violated). These histories are not allowed by the preventative approach; H1 is ruled out by P1 and H2 is ruled out by P2.</p>
<p>在这两种情况下，T2 都会观察到不一致的状态（它观察到不变量 x + y &#x3D; 10 被违反）。 预防性方法不允许出现这些历史； H1被P1排除，H2被P2排除。</p>
<p>Optimistic and multi-version mechanisms [2, 5, 9, 20, 22] that provide serializability also disallow non-serializable histories such as H1 and H2. However, they allow many legal histories that are not permitted by P0, P1, P2, and P3. Thus, the preventative approach disallows such implemen- tations. Furthermore, it rules out histories that really occur in practical implementations.</p>
<p>提供可序列化性的乐观和多版本机制 [2,5,9,20,22] 也不允许不可序列化的历史，例如 H1 和 H2。 然而，它们允许许多 P0、P1、P2 和 P3 不允许的法律历史记录。 因此，预防性方法不允许此类实施。 此外，它排除了实际实施中真正发生的历史。</p>
<p>Phenomenon P0 can occur in optimistic implementations since there can be many uncommitted transactions modify- ing local copies of the same object concurrently; if neces- sary, some of them will be forced to abort so that serializ- ability can be provided. Thus, disallowing P0 can rule out optimistic implementations.</p>
<p>现象 P0 可能发生在乐观实现中，因为可能有许多未提交的事务同时修改同一对象的本地副本； 如果有必要，其中一些将被迫中止，以便提供可串行性。 因此，禁止 P0 可以排除乐观的实现。</p>
<p>Condition P1 precludes transactions from reading up- dates by uncommitted transactions. Such reads are disal- lowed by many optimistic schemes, but they are desirable in mobile environments, where commits may take a long time if clients are disconnected from the servers [12, 16]; furthermore, reads from uncommitted transactions may be desirable in high traffic hotspots [23]. For example, in his- tory H1, if T2 reads T1’s values for both x and y, it can be serialized after T1 :</p>
<p>条件 P1 阻止事务读取未提交事务的更新。 许多乐观方案都不允许这种读取，但它们在移动环境中是可取的，如果客户端与服务器断开连接，提交可能需要很长时间[12, 16]； 此外，在高流量热点中可能需要读取未提交的事务[23]。 例如，在历史 H1 中，如果 T2 读取 T1 的 x 和 y 值，则可以在 T1 之后序列化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">H10 : r1(x, <span class="number">5</span>) w1(x, <span class="number">1</span>) r1(y, <span class="number">5</span>) w1(y, <span class="number">9</span>) r2(x, <span class="number">1</span>) r2(y, <span class="number">9</span>) c1 c2</span><br></pre></td></tr></table></figure>

<p>The above history can occur in a mobile system, but P1 disallows it. In such a system, commits can be assumed to have happened “tentatively” at client machines [12, 16]; later transactions may observe modifications of those tentative transactions. When the client reconnects with the servers, its work is checked to determine if consistency has been violated and the relevant transactions are aborted. Of course, if dirty reads are allowed, cascading aborts can occur, e.g., in history H10 , T2 must abort if T1 aborts; this problem can be alleviated by using compensating actions [18, 26, 19].</p>
<p>上述历史可以在移动系统中发生，但P1不允许。 在这样的系统中，可以假设提交“暂时”发生在客户端计算机上 [12, 16]； 后续交易可能会观察到这些暂定交易的修改。 当客户端重新与服务器连接时，将检查其工作以确定是否违反了一致性并中止相关事务。 当然，如果允许脏读，则可能会发生级联中止，例如，在历史记录H10中，如果T1中止，则T2必须中止； 这个问题可以通过使用补偿措施来缓解[18,26,19]。</p>
<p>Proscribing phenomenon P2 disallows a modification to an object that has been read by an uncommitted transaction (P3 rules out a similar situation with respect to predicates). As with P0, uncommitted transactions may read&#x2F;write the same object concurrently in an optimistic implementation. There is no harm in allowing phenomenon P2 if transactions commit in the right order. For example, in history H2 given above, if T2 reads the old values of <em>x</em> and <em>y</em>, the transactions can be serialized in the order T2; T1:</p>
<p>禁止现象 P2 不允许对已由未提交事务读取的对象进行修改（P3 排除了谓词方面的类似情况）。 与 P0 一样，未提交的事务可以在乐观实现中同时读&#x2F;写同一对象。 如果事务以正确的顺序提交，那么允许 P2 现象并没有什么坏处。 例如，在上面给出的历史H2中，如果T2读取x和y的旧值，则交易可以按照T2的顺序序列化； T1：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">H20 : r2(x, <span class="number">5</span>) r1(x, <span class="number">5</span>) w1(x, <span class="number">1</span>) r1(y, <span class="number">5</span>) r2(y, <span class="number">5</span>) w1(y, <span class="number">9</span>) c2 c1</span><br></pre></td></tr></table></figure>

<p>The real problem with the preventative approach is that the phenomena are expressed in terms of single-object his- tories. However, the properties of interest are often multi- object constraints. To avoid problems with such constraints, the phenomena need to restrict what can be done with indi- vidual objects more than is necessary. Our approach avoids this difficulty by using specifications that capture constraints on multiple objects directly. Furthermore, the definitions in the preventative approach are not applicable to multi-version systems since they are described in terms of objects rather than in terms of versions. On the other hand, our specifica- tions deal with multi-version and single-version histories.</p>
<p>预防性方法的真正问题在于，这些现象是用单一对象历史来表达的。 然而，感兴趣的属性通常是多对象约束。 为了避免此类约束带来的问题，现象需要对单个对象所能做的事情进行超出必要的限制。 我们的方法通过使用直接捕获多个对象的约束的规范来避免这个困难。 此外，预防性方法中的定义不适用于多版本系统，因为它们是根据对象而不是版本来描述的。 另一方面，我们的规范涉及多版本和单版本历史。</p>
<p>The approach in [8] only allows schemes that provide the same guarantees for running and committed transac- tions (a lock-based implementation does indeed have this property). However, many optimistic mechanisms provide weak guarantees to transactions as they run while provid- ing strong guarantees such as serializability for committed transactions. Our definitions allow different isolation guar- antees for committed and running transactions; in this paper, we only present guarantees for committed transactions.</p>
<p>[8]中的方法只允许为运行和提交的事务提供相同保证的方案（基于锁的实现确实具有此属性）。 然而，许多乐观机制在事务运行时为事务提供弱保证，同时提供强保证，例如已提交事务的可串行性。 我们的定义允许对已提交和正在运行的事务提供不同的隔离保证； 在本文中，我们仅对已提交的交易提供担保。</p>
<h2 id="4-Database-Model-and-Transaction-Histories"><a href="#4-Database-Model-and-Transaction-Histories" class="headerlink" title="4. Database Model and Transaction Histories"></a><strong>4. Database Model and Transaction Histories</strong></h2><p>We now describe our database model, transaction histories, and serialization graphs. We use a multi-version model similar to the one presented in [9]. However, unlike [9], our model incorporates predicates also. Furthermore, we al- low predicate behavior that is possible in non-locking based systems.</p>
<p>我们现在描述我们的数据库模型、事务历史和序列化图。 我们使用类似于[9]中提出的多版本模型。 然而，与[9]不同的是，我们的模型也包含谓词。 此外，我们允许在基于非锁定的系统中可能出现的谓词行为。</p>
<h3 id="4-1-Database-Model"><a href="#4-1-Database-Model" class="headerlink" title="4.1. Database Model"></a><strong>4.1. Database Model</strong></h3><p>The database consists of objects that can be read or writ- ten by transactions; in a relational database system, each row or tuple is an object. Each transaction reads and writes objects and indicates a total order in which these operations occur.</p>
<p>数据库由可以通过事务读取或写入的对象组成； 在关系数据库系统中，每一行或元组都是一个对象。 每个事务读取和写入对象并指示这些操作发生的总顺序。</p>
<p>An object has one or more versions. However, trans- actions interact with the database only in terms of objects; the system maps each operation on an object to a specific version of that object. A transaction may read versions created by committed, uncommitted, or even aborted trans- actions; constraints imposed by some isolation levels will prevent certain types of reads, e.g., reading versions created by aborted transactions.</p>
<p>一个对象有一个或多个版本。 然而，事务仅以对象的形式与数据库进行交互。 系统将对象上的每个操作映射到该对象的特定版本。 事务可以读取由已提交、未提交甚至中止的事务创建的版本； 某些隔离级别施加的约束将阻止某些类型的读取，例如读取由中止事务创建的版本。</p>
<p>When a transaction writes an object <em>x</em>, it creates a new version of <em>x</em>. A transaction Ti can modify an object multiple times; its first modification of object <em>x</em> is denoted by xi:1, the second by xi:2, and so on. Version xi denotes the final modification of <em>x</em> performed by Ti before it commits or aborts. A transaction’s last operation, <em>commit</em> or <em>abort</em>, indicates whether its execution was successful or not; there is at most one commit or abort operation for each transaction.</p>
<p>当事务写入对象 x 时，它会创建 x 的新版本。 一个事务Ti可以多次修改一个对象； 它对对象 x 的第一次修改用 xi:1 表示，第二次用 xi:2 表示，依此类推。 版本 xi 表示 Ti 在提交或中止之前对 x 执行的最终修改。 事务的最后一个操作，提交或中止，表明其执行是否成功； 每个事务至多有一次提交或中止操作。</p>
<p>The <em>committed state</em> reflects the modifications of committed transactions. When transaction Ti commits, each version xi created by Ti becomes a part of the committed state and we say that Ti <em>installs</em> xi ; the system determines the ordering of xi relative to other committed version of x. If Ti aborts,x does not become part of the committed state.</p>
<p><em>提交状态</em>反映了已提交事务的修改。 当事务 Ti 提交时，Ti 创建的每个版本 xi 都成为已提交状态的一部分，我们说 Ti <em>安装</em> xi ； 系统确定 xi 相对于其他已提交版本的 x 的顺序。 如果 Ti 中止，xi 不会成为提交状态的一部分。</p>
<p>Conceptually, the initial committed state comes into existence as a result of running a special initialization transaction, T(init). Transaction T(init) creates all objects that will ever exist in the database; at this point, each object <em>x</em> has an initial version, x(init) , called the <em>unborn</em> version. When an application transaction creates an object <em>x</em> (e.g., by in- serting a tuple in a relation), we model it as the creation of a <em>visible</em> version for <em>x</em>. Thus, a transaction that loads thedatabase creates the initial visible versions of the objects being inserted. When a transaction Ti deletes an object <em>x</em> (e.g., by deleting a tuple from some relation), we model it as the creation of a special <em>dead</em> version, i.e., in this case, xi is a dead version. Thus, object versions can be of three kinds — unborn, visible, and dead; the ordering relationship between these versions is discussed in Section 4.2.</p>
<p>从概念上讲，初始提交状态是由于运行特殊初始化事务 T(init) 而产生的。 事务 T(init) 创建数据库中存在的所有对象； 此时，每个对象 <em>x</em> 都有一个初始版本 x(init) ，称为 <em>unborn</em> 版本。 当应用程序事务创建对象<em>x</em>（例如，通过在关系中插入元组）时，我们将其建模为创建<em>x</em>的<em>可见</em>版本。 因此，加载数据库的事务创建所插入的对象的初始可见版本。 当事务 Ti 删除对象 <em>x</em> 时（例如，通过从某个关系中删除元组），我们将其建模为创建特殊的 <em>dead</em> 版本，即在这种情况下，xi 是一个dead 版本。 因此，对象版本可以分为三种：未出生的、可见的和死亡的。 这些版本之间的顺序关系将在 4.2 节中讨论。</p>
<p>If an object <em>x</em> is deleted from the committed database state and inserted later, we consider the two incarnations of <em>x</em> to be distinct objects. When a transaction Ti performs an insert operation, the system selects a <em>unique</em> object <em>x</em> that has never been selected for insertion before and Ti creates a visible version of <em>x</em> if it commits.</p>
<p>如果一个对象 x 从已提交的数据库状态中删除并稍后插入，我们认为 x 的两个化身是不同的对象。 当事务 Ti 执行插入操作时，系统会选择一个之前从未选择插入的唯一对象 x，如果提交，Ti 将创建 x 的可见版本。</p>
<p>We assume object versions exist forever in the committed state to simplify the handling of inserts and deletes, i.e., we simply treat inserts&#x2F;deletes as write (update) operations. An implementation only needs to maintain visible versions of objects, and a single-version implementation can maintain just one visible version at a time. Furthermore, application transactions in a real system access only visible versions.</p>
<p>我们假设对象版本永远以提交状态存在，以简化插入和删除的处理，即我们简单地将插入&#x2F;删除视为写（更新）操作。 一种实现只需要维护对象的可见版本，而单版本实现一次只能维护一个可见版本。 此外，真实系统中的应用程序事务仅访问可见版本。</p>
<h3 id="4-2-Transaction-Histories"><a href="#4-2-Transaction-Histories" class="headerlink" title="4.2. Transaction Histories"></a><strong>4.2. Transaction Histories</strong></h3><p>We capture what happens in an execution of a database system by a history. A <em>history H</em> over a set of transactions consists of two parts — a partial order of events <em>E</em> that reflects the operations (e.g., read, write, abort, commit) of those transactions, and a version order, , that is a total order on committed versions of each object.</p>
<p>我们通过历史记录来捕获数据库系统执行过程中发生的情况。 一组事务的历史记录 H 由两部分组成：事件 E 的部分顺序，反映这些事务的操作（例如读、写、中止、提交），以及版本顺序 ，即全顺序 每个对象的提交版本。</p>
<p>Each event in a history corresponds to an operation of some transaction, i.e., read, write, commit, or abort. A write operation on object <em>x</em> by transaction Ti is denoted by wi (xi ) (or wi (xi:m )); if it is useful to indicate the value <em>v</em> being written into xi , we use the notation, wi (xi , v). When a transaction Tj reads a version of <em>x</em> that was created by Ti , we denote this as rj (xi ) (or rj (xi:a )). If it is useful to indicate the value <em>v</em> being read, we use the notation rj (xi , v).</p>
<p>历史记录中的每个事件对应于某个事务的操作，即读、写、提交或中止。 事务 Ti 对对象 x 的写操作记为 wi (xi ) （或 wi (xi:m )）； 如果需要指示将值 v 写入 xi ，我们使用符号 wi (xi , v)。 当事务 Tj 读取由 Ti 创建的 x 版本时，我们将其表示为 rj (xi ) （或 rj (xi:a )）。 如果指示正在读取的值 v 有用，我们使用符号 rj (xi , v)。</p>
<p>The partial order of events <em>E</em> in a history obeys the fol- lowing constraints:</p>
<p>历史中事件 E 的偏序遵循以下约束：</p>
<ul>
<li><p>It preserves the order of all events within a transaction including the commit and abort events.</p>
<p>它保留事务中所有事件的顺序，包括提交和中止事件。</p>
</li>
<li><p>If an event rj (xi:m ) exists in E, it is preceded by wi (xi:m ) in E, i.e., a transaction Tj cannot read ver- sion xi:m of object x before it has been produced by Ti . Note that the version read by Tj is not necessarily the most recently installed version in the committed database state; also, Ti may be uncommitted when rj (xi:m ) occurs.</p>
<p>如果 E 中存在事件 rj (xi:m)，则 E 中的事件 rj (xi:m) 前面有 wi (xi:m)，即事务 Tj 在 Ti 生成对象 x 之前无法读取对象 x 的版本 xi:m。 注意，Tj读取的版本不一定是提交数据库状态下最近安装的版本； 另外，当 rj (xi:m ) 发生时，Ti 可能未提交。</p>
</li>
<li><p>If an event wi (xi:m ) is followed by ri (xj ) without an intervening event wi (xi:n ) in <em>E</em>, xj must be xi:m . This condition ensures that if a transaction modifies object <em>x</em> and later reads <em>x</em>, it will observe its last update to <em>x</em>.</p>
<p>如果事件 wi (xi:m ) 后面跟着 ri (xj )，而 E 中没有中间事件 wi (xi:n )，则 xj 必须是 xi:m 。 此条件确保如果事务修改对象 x 并且稍后读取 x，它将观察其对 x 的最后更新。</p>
</li>
<li><p>The history must be <em>complete</em>: if <em>E</em> contains a read or write event that mentions a transaction Ti , <em>E</em> must contains a commit or abort event for Ti .</p>
<p>历史记录必须完整：如果 E 包含提及事务 Ti 的读取或写入事件，则 E 必须包含 Ti 的提交或中止事件。</p>
</li>
</ul>
<p>A history that is not complete can be completed by append- ing abort events for uncommitted transactions in <em>E</em>. Adding these events is intuitively correct since any implementation that allows a commit of a transaction that reads from an uncommitted transaction Ti can do so only if it is legal for Ti to abort later.</p>
<p>不完整的历史记录可以通过在 E 中附加未提交事务的中止事件来完成。添加这些事件直观上是正确的，因为任何允许提交从未提交事务 Ti 读取的事务的实现只有在以下情况下才可以这样做： Ti 稍后中止是合法的。</p>
<p>For convenience, we will present event histories in ex- amples as a total order (from left to right) that is consistent with the partial order.</p>
<p>为了方便起见，我们将在示例中将事件历史呈现为与部分顺序一致的全顺序（从左到右）。</p>
<p>The second part of a history H is the version order, , that specifies a total order on versions of each object created by <em>committed</em> transactions in H; there is no ordering of versions due to uncommitted or aborted transactions. We also refer to versions due to committed transactions in H as <em>committed versions</em>. We impose two constraints on a history’s version order for different kinds of committed versions:</p>
<p>历史记录 H 的第二部分是版本顺序 ，它指定由 H 中已提交事务创建的每个对象的版本的总顺序； 由于未提交或中止的事务，不会对版本进行排序。 我们还将 H 中已提交事务的版本称为已提交版本。 我们对不同类型的提交版本的历史版本顺序施加两个约束：</p>
<ul>
<li><p>the version order of each object <em>x</em> contains exactly one initial version, xinit , and at most one committed dead version, xdead .</p>
<p>每个对象 <em>x</em> 的版本顺序恰好包含一个初始版本 xinit 和至多一个已提交的死版本 xdead 。</p>
</li>
<li><p>xinit is <em>x</em>’s first version in its version order and xdead is its last version (if it exists); all committed visible versions are placed between xinit and xdead .</p>
<p>xinit 是 <em>x</em> 按其版本顺序排列的第一个版本，xdead 是其最后一个版本（如果存在）； 所有提交的可见版本都放置在 xinit 和 xdead 之间。</p>
</li>
</ul>
<p>We additionally constrain the system to allow reads only of visible versions:</p>
<p>我们还限制系统只允许读取可见版本：</p>
<ul>
<li>if rj (xi ) occurs in a history, then xi is a visible version.</li>
</ul>
<p>  如果 rj (xi ) 出现在历史记录中，则 xi 是可见版本。</p>
<p>For convenience, we will only show the version order for visible versions in our example histories; in cases where unborn or dead versions help in illustrating an issue, we will show some of these versions as well.</p>
<p>为了方便起见，我们将仅在示例历史记录中显示可见版本的版本顺序； 如果未出生或已死亡的版本有助于说明问题，我们也会展示其中的一些版本。</p>
<p>The version order in a history H can be different from the order of write or commit events in H. This flexibility is needed to allow certain optimistic and multi-version imple- mentations where it is possible that a version xi is placed before version xj in the version order (xi  xj ) even though xi is installed in the committed state <em>after</em> version xj was installed. For example, in history Hw r iteor der ,</p>
<p>历史记录 H 中的版本顺序可能与 H 中写入或提交事件的顺序不同。需要这种灵活性来允许某些乐观和多版本实现，其中版本 xi 可能放置在版本 xj 之前 版本顺序 (xi xj )，即使 xi 在安装版本 xj 之后以已提交状态安装。 例如，在历史 Hw r ite order 中，</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库SQL总结</title>
    <url>/2023/11/07/2-%E6%95%B0%E6%8D%AE%E5%BA%93/4-%E8%AE%BA%E6%96%87/1-Cascades/</url>
    <content><![CDATA[<p><em>This paper describes a new extensible query optimization framework that resolves many of the short- comings of the EXODUS and Volcano optimizer generators. In addition to extensibility, dynamic pro- gramming, and memorization based on and extended from the EXODUS and Volcano prototypes, this new optimizer provides (i) manipulation of operator arguments using rules or functions, (ii) operators that are both logical and physical for predicates etc., (iii) schema-specific rules for materialized views, (iv) rules to insert ”enforcers” or ”glue operators,” (v) rule-specific guidance, permitting grouping of rules, (vi) basic facilities that will later permit parallel search, partially ordered cost measures, and dy- namic plans, (vii) extensive tracing support, and (viii) a clean interface and implementation making full use of the abstraction mechanisms of C++. We describe and justify our design choices for each of these issues. The optimizer system described here is operational and will serve as the foundation for new query optimizers in Tandem’s NonStop SQL product and in Microsoft’s SQL Server product.</em></p>
<p>本文描述了一种新的可扩展查询优化框架，它解决了 EXODUS 和 Volcano 优化器生成器的许多缺点。 除了基于 EXODUS 和 Volcano 原型并从其扩展的可扩展性、动态编程和记忆之外，这个新的优化器还提供（i）使用规则或函数操作运算符参数，（ii）逻辑和物理运算符 谓词等，(iii) 物化视图的特定于模式的规则，(iv) 插入“执行者”或“粘合操作符”的规则，(v) 特定于规则的指导，允许对规则进行分组，(vi) 将 后来允许并行搜索、部分有序成本测量和动态计划，(vii) 广泛的跟踪支持，以及 (viii) 充分利用 C++ 抽象机制的干净接口和实现。 我们针对每个问题描述并证明我们的设计选择。 这里描述的优化器系统是可操作的，并将作为 Tandem NonStop SQL 产品和 Microsoft SQL Server 产品中新查询优化器的基础。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a><strong>1 Introduction</strong></h2><p>Following our experiences with the EXODUS Optimizer Generator [GrD87], we built a new optimizer generator as part of the Volcano project [GrM93]. The main contributions of the EXODUS work were the optimizer gener- ator architecture based on code generation from declarative rules, logical and physical algebra’s, the division of a query optimizer into modular components, and interface definitions for support functions to be provided by the database implementor (DBI), whereas the Volcano work combined improved extensibility with an efficient search engine based on dynamic programming and memorization. By using the Volcano Optimizer Generator in two applications, a object-oriented database systems [BMG93] and a scientific database system prototype [WoG93], we identified a number of flaws in its design. Overcoming these flaws is the goal of a completely new extensi- ble optimizer developed in the Cascades project, a new project applying many of the lessons learned from the Volcano project on extensible query optimization, parallel query execution, and physical database design. Com- pared to the Volcano design and implementation, the new Cascades optimizer has the following advantages. In their entirety, they represent a substantial improvement over our own earlier work as well as other related work in functionality, ease-of-use, and robustness.</p>
<p>根据我们使用 EXODUS 优化器生成器 [GrD87] 的经验，我们构建了一个新的优化器生成器作为 Volcano 项目 [GrM93] 的一部分。 EXODUS 工作的主要贡献是基于声明性规则、逻辑和物理代数的代码生成的优化器生成器体系结构、将查询优化器划分为模块化组件以及由数据库实现者提供的支持功能的接口定义 （DBI），而 Volcano 工作将改进的可扩展性与基于动态编程和记忆的高效搜索引擎结合起来。 通过在两个应用程序（面向对象的数据库系统 [BMG93] 和科学数据库系统原型 [WoG93]）中使用 Volcano Optimizer Generator，我们发现了其设计中的许多缺陷。 克服这些缺陷是 Cascades 项目中开发的全新可扩展优化器的目标，该新项目应用了从 Volcano 项目中获得的有关可扩展查询优化、并行查询执行和物理数据库设计的许多经验教训。 与Volcano的设计和实现相比，新的Cascades优化器具有以下优点。 总的来说，它们比我们早期的工作以及其他相关工作在功能、易用性和稳健性方面有了实质性的改进。</p>
<ul>
<li>Abstract interface classes defining the DBI-optimizer interface and permitting DBI-defined subclass hier- archies</li>
</ul>
<p>  定义 DBI 优化器接口并允许 DBI 定义的子类层次结构的抽象接口类</p>
<ul>
<li>Rules as objects</li>
</ul>
<p>  规则作为对象</p>
<ul>
<li>Facilities for schema- and even query-specific rules</li>
</ul>
<p>  模式甚至查询特定规则的设施</p>
<ul>
<li>Simple rules requiring minimal DBI support</li>
</ul>
<p>  简单的规则需要最少的 DBI 支持</p>
<ul>
<li>Rules with substitutes consisting of a complex expression</li>
</ul>
<p>  包含复杂表达式的替代规则</p>
<ul>
<li>Rules that map an input pattern to a DBI-supplied function</li>
</ul>
<p>  将输入模式映射到 DBI 提供的函数的规则</p>
<ul>
<li>Rules to place property enforcers such as sort operations</li>
</ul>
<p>  放置属性执行器的规则，例如排序操作</p>
<ul>
<li>Operators that may be both logical and physical, e.g., predicates</li>
</ul>
<p>  既可以是逻辑运算符也可以是物理运算符，例如谓词</p>
<ul>
<li>Patterns that match an entire subtree, e.g., a predicate</li>
</ul>
<p>  匹配整个子树的模式，例如谓词</p>
<ul>
<li>Optimization tasks as data structures</li>
</ul>
<p>  作为数据结构的优化任务</p>
<ul>
<li>Incremental enumeration of equivalent logical expressions</li>
</ul>
<p>  等价逻辑表达式的增量枚举</p>
<ul>
<li>Guided or exhaustive search</li>
</ul>
<p>  引导式或详尽的搜索</p>
<ul>
<li>Ordering of moves by promise</li>
</ul>
<p>  按承诺排序动作</p>
<ul>
<li>Rule-specific guidance</li>
</ul>
<p>  特定规则的指导</p>
<ul>
<li>Incremental improvement of estimated logical properties</li>
</ul>
<p>  估计逻辑属性的增量改进</p>
<p>The points in the list above and their effects will be discussed in this paper. While the system is operational, we have not performed any performance studies and the system is not fully tuned yet. Detailed analysis and focused improvement of the Cascades optimizer’s efficiency is left for further work.</p>
<p>本文将讨论上述要点及其影响。 虽然系统正在运行，但我们尚未进行任何性能研究，并且系统尚未完全调整。 Cascades优化器效率的详细分析和重点提升有待进一步工作。</p>
<h2 id="2-Optimization-Algorithm-and-Tasks"><a href="#2-Optimization-Algorithm-and-Tasks" class="headerlink" title="2 Optimization Algorithm and Tasks"></a><strong>2 Optimization Algorithm and Tasks</strong></h2><p>The optimization algorithm is broken into several parts, which we call ”tasks.” While each task could easily be implemented as a procedure, we chose to realize tasks as objects that, among other methods, have a ”perform” method defined for them. Task objects offer significantly more flexibility than procedure invocations, in particu- lar with respect to search algorithm and search control. A task object exists for each task that has yet to be done; all such task objects are collected in a task structure. The task structure is currently realized as a last-in-first-out stack; however, other structures can easily be envisioned. In particular, task objects can be reordered very easily at any point, enabling very flexible mechanisms for heuristic guidance. Moreover, we plan on representing the task structure by a graph that captures dependencies or the topological ordering among tasks and permit efficient parallel search (using shared memory). However, in order to obtain a working system fast, the current implemen- tation is restricted to a LIFO stack, and scheduling a task is very similar to invoking a function, with the exception that any work to be done after a sub-task completes must be scheduled as a separate task.</p>
<p>优化算法分为几个部分，我们称之为“任务”。 虽然每个任务都可以很容易地实现为一个过程，但我们选择将任务实现为对象，除了其他方法之外，还为它们定义了一个“执行”方法。 任务对象比过程调用提供了更大的灵活性，特别是在搜索算法和搜索控制方面。 每个尚未完成的任务都存在一个任务对象； 所有此类任务对象都收集在任务结构中。 任务结构目前实现为后进先出堆栈； 然而，可以很容易地设想其他结构。 特别是，任务对象可以在任何时候非常容易地重新排序，从而实现非常灵活的启发式指导机制。 此外，我们计划用一个图来表示任务结构，该图捕获任务之间的依赖关系或拓扑顺序，并允许高效的并行搜索（使用共享内存）。 然而，为了快速获得一个工作系统，当前的实现仅限于后进先出堆栈，并且调度任务与调用函数非常相似，除了子任务完成后要完成的任何工作 必须安排为单独的任务。</p>
<p>Figure 1 shows the tasks that make up the optimizer’s search algorithm. Arrows indicate which type of task schedules (invokes) which other type; dashed arrows indicate where invocations pertain to inputs, i.e., subqueries or subplans. Brief pseudo-code for the tasks is also given in the appendix. The ”optimize()” procedure first copies the original query into the internal ”memo” structure and then triggers the entire optimization process with a task to optimize the class corresponding to the root node of the original query tree, which in turn triggers optimization of smaller and smaller subtrees.</p>
<p>图 1 显示了构成优化器搜索算法的任务。 箭头表示哪种类型的任务调度（调用）哪种其他类型； 虚线箭头表示调用与输入相关的位置，即子查询或子计划。 附录中还给出了任务的简短伪代码。 “optimize()”过程首先将原始查询复制到内部“memo”结构中，然后触发整个优化过程，其中一个任务是优化原始查询树的根节点对应的类，进而触发优化 子树越来越小。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">----------&gt; Optimize Group &lt;-------- Optimize Inputs</span></span><br><span class="line">									<span class="operator">|</span>                        <span class="operator">|</span></span><br><span class="line">									<span class="operator">|</span> d                      <span class="operator">|</span> up</span><br><span class="line">						Optimize Experssion            <span class="operator">|</span></span><br><span class="line">            			<span class="operator">|</span>            \  \up      <span class="operator">|</span></span><br><span class="line">            			<span class="operator">|</span>  d         d\   \ </span><br><span class="line">            Explore <span class="keyword">Group</span>              Apply Rule</span><br><span class="line">            	<span class="operator">|</span>     <span class="operator">|</span>             d <span class="operator">/</span> <span class="operator">/</span></span><br><span class="line">            	<span class="operator">|</span>d    <span class="operator">|</span> up          <span class="operator">/</span> <span class="operator">/</span>   up</span><br><span class="line">						Explore Expression</span><br></pre></td></tr></table></figure>



<p>A task to optimize a group or an expression represents what was called an ”optimization goal” in the Volcano optimizer generator: it combines a group or expression with a cost limit and with required and excluded physical properties. Performing such a task results either in a plan or a failure. Optimizing a group means finding the best plan for any expression in the group and therefore applies rules to all expressions, whereas optimizing an expression starts with a single expression. The former is realized by invoking the latter for each expression. The latter results in transitive rule applications and therefore, if the rule set is complete, finds the best plan within the starting expression’s group. The distinction between the two task types is made purely for pragmatic reasons. On the one hand, there must be a task to find the best plan for any expression in a group in order to initiate optimization of an entire query tree or a subtree after an implementation rule has been applied; on the other hand, there must be a task to optimize a single (new) expression after applying a transformation rule.</p>
<p>优化组或表达式的任务代表了 Volcano 优化器生成器中所谓的“优化目标”：它将组或表达式与成本限制以及所需和排除的物理属性相结合。 执行这样的任务要么会导致计划成功，要么会失败。 优化组意味着为组中的任何表达式找到最佳计划，因此将规则应用于所有表达式，而优化表达式则从单个表达式开始。 前者是通过为每个表达式调用后者来实现的。 后者导致传递规则应用，因此，如果规则集完整，则会在起始表达式组中找到最佳计划。 两种任务类型之间的区别纯粹是出于实用原因。 一方面，必须有一个任务来为组中的任何表达式找到最佳计划，以便在应用实现规则后启动整个查询树或子树的优化； 另一方面，在应用转换规则后必须有一个任务来优化单个（新）表达式。</p>
<p>The task to optimize a group also implements dynamic programming and memorization. Before initiating optimization of all a group’s expressions, it checks whether the same optimization goal has been pursued already; if so, it simply returns the plan found in the earlier search. Reusing plans derived earlier is the crucial aspect of dynamic programming and memorization. Exploring a group or an expression is an entirely new concept that has no equivalent in the Volcano optimizer generator. In the Volcano search strategy, a first phase applied all transformation rules to create all possible logical expressions for a query and all its subtrees. The second phase, which performed the actual optimization, navigated within that network of equivalence classes and expressions, applied implementation rules to obtain plans, and determined the best plan.</p>
<p>优化群体的任务也实现了动态规划和记忆。 在开始优化所有组的表达式之前，它会检查是否已经追求相同的优化目标； 如果是这样，它只是返回在先前搜索中找到的计划。 重用先前导出的计划是动态编程和记忆的关键方面。 探索组或表达式是一个全新的概念，在 Volcano 优化器生成器中没有等效概念。 在火山搜索策略中，第一阶段应用所有转换规则来为查询及其所有子树创建所有可能的逻辑表达式。 第二阶段执行实际的优化，在等价类和表达式的网络中导航，应用实现规则来获取计划，并确定最佳计划。</p>
<p>In the Cascades optimizer, this separation into two phases is abolished, because it is not useful to derive all logically equivalent forms of all expressions, e.g., of a predicate. A group is explored using transformation rules only on demand, and it is explored only to create all members of the group that match a given pattern. Thus, exploring a group or an expression (the distinction between these two mirrors the distinction between optimizing a group or an expression) means deriving all logical expressions that match a given pattern. The pattern, which is part of the task definition, is a subtree of the rule’s antecedent or ”before”-pattern.</p>
<p>在 Cascades 优化器中，取消了这种分为两个阶段的做法，因为派生所有表达式（例如谓词）的所有逻辑等效形式是没有用的。 仅根据需要使用转换规则来探索组，并且仅探索创建与给定模式匹配的组的所有成员。 因此，探索组或表达式（这两者之间的区别反映了优化组或表达式之间的区别）意味着导出与给定模式匹配的所有逻辑表达式。 该模式是任务定义的一部分，是规则的先行词或“之前”模式的子树。</p>
<p>As do optimization tasks, exploration tasks also avoid duplicate work. Before exploring a group’s expres- sions, the task to explore a group checks whether the same pattern has already been explored for the given group. If so, the task terminates immediately without spawning other tasks. Thus, the overall effort to expand logical expressions is also reduced by dynamic programming, i.e., retaining and reusing results of earlier search effort. The decision whether or not a pattern has already been explored is made using a ”pattern memory” initialized and administered by the DBI.</p>
<p>与优化任务一样，探索任务也避免了重复工作。 在探索组的表达式之前，探索组的任务会检查是否已经为给定组探索了相同的模式。 如果是这样，该任务将立即终止，而不会生成其他任务。 因此，动态编程也减少了扩展逻辑表达式的总体工作，即保留和重用早期搜索工作的结果。 使用由 DBI 初始化和管理的“模式记忆”来决定是否已经探索过某个模式。</p>
<p>In order to make this discussion more concrete, consider a join associativity rule. In Volcano, all equivalence classes are completely expanded to contain all equivalent logical expressions before the actual optimization phase begins. Thus, during the optimization phase, when a join operator matches the top join operator in the rule, all join expressions for the rule’s lower join are readily available so the rule can immediately applied with all possible bindings. In Cascades, these expressions are not immediately available and must be derived before the rule is applied. The exploration tasks provide this functionality; they are invoked not during a pre-optimization phase as in Volcano but on demand for a specific group and a specific pattern.</p>
<p>为了使此讨论更加具体，请考虑连接关联性规则。 在 Volcano 中，在实际优化阶段开始之前，所有等价类都被完全扩展为包含所有等价逻辑表达式。 因此，在优化阶段，当连接运算符与规则中的顶部连接运算符匹配时，该规则的较低连接的所有连接表达式都随时可用，因此该规则可以立即应用于所有可能的绑定。 在级联中，这些表达式不是立即可用的，必须在应用规则之前导出。 探索任务提供了此功能； 它们不是像 Volcano 那样在预优化阶段调用，而是根据特定组和特定模式的需求调用。</p>
<p>One might ask which of the Volcano technique and the Cascades technique is more efficient and more effec- tive. The Volcano technique generates all equivalent logical expressions exhaustively in the first phase. Even if the actual optimization phase uses a greedy search algorithm, this first phase in Volcano must still be exhaustive. In the Cascades technique, this represents the worst case. If there is no guidance indicating which rule might lead to expressions matching the given pattern, exhaustive enumeration of all equivalent logical expressions cannot be avoided. On the other hand, if there is some guidance, some of that effort can be avoided, and the Cascades search strategy seems superior. On the other hand, the same group might have to be explored multiple times for different patterns ; if so, redundant rule applications and derivations might occur. In order to avoid that, each expression in the ”memo” structure includes a bit map that indicates which transformation rules have already been applied to it and thus should not be re-applied. Thus, we believe that the Cascades search strategy is more efficient because it explores groups only for truly useful patterns. In the worst case, i.e., without any guidance, the efficiency of the Cascades search will equal that of the Volcano search strategy.</p>
<p>有人可能会问火山技术和级联技术哪一种更高效、更有效。 火山技术在第一阶段详尽地生成所有等效逻辑表达式。 即使实际的优化阶段使用贪婪搜索算法，Volcano 中的第一阶段仍然必须是详尽的。 在级联技术中，这代表最坏的情况。 如果没有指导指示哪个规则可能导致表达式与给定模式匹配，则无法避免对所有等效逻辑表达式的详尽枚举。 另一方面，如果有一些指导，则可以避免一些工作，并且级联搜索策略似乎更优越。 另一方面，同一组可能需要多次探索不同的模式； 如果是这样，可能会出现冗余的规则应用和推导。 为了避免这种情况，“memo”结构中的每个表达式都包含一个位图，该位图指示哪些转换规则已经应用于它，因此不应重新应用。 因此，我们认为级联搜索策略更有效，因为它只探索真正有用的模式的组。 在最坏的情况下，即没有任何指导，Cascades 搜索的效率将等于 Volcano 搜索策略的效率。</p>
<p>On the other hand, if such guidance is incorrect, incorrect pruning of the search space may occur and the Cascades optimizer’s effectiveness might suffer. Thus, it is very important that such guidance be correct. We plan on using two techniques for guidance, which are not implemented yet. First, by inspecting the entire rule set, in particular the top operators of each rule’s antecedent (”before”-pattern) and consequent (”after”-pattern, substitute), we can identify which operators can be mapped to which other operators in a single rule application. By taking the transitive closure of this reachability relationship, we can exclude some rules from consideration. Note that this transitive closure can be computed when the optimizer is generated from the rule set, i.e., only once. Second, we plan on implementing mechanisms for guidance by the DBI.</p>
<p>另一方面，如果此类指导不正确，则可能会错误地修剪搜索空间，并且 Cascades 优化器的有效性可能会受到影响。 因此，这种指导的正确性非常重要。 我们计划使用两种技术进行指导，但尚未实施。 首先，通过检查整个规则集，特别是每个规则的前件（“before”模式）和后件（“after”模式，替换）的顶级运算符，我们可以识别哪些运算符可以映射到其中的哪些其他运算符 单个规则应用程序。 通过采用这种可达性关系的传递闭包，我们可以排除一些规则。 请注意，当从规则集生成优化器时，可以计算此传递闭包，即仅计算一次。 其次，我们计划落实DBI的指导机制。</p>
<p>Applying a rule creates a new expression; notice that the new expression can be complex (consisting of mul- tiple operators, as in a join associativity rule) and may be either a transformation rule (creating a new logical expression) or an implementation rule (creating a new physical expression or plan). In fact, since an operator can be both logical and physical, one rule may be both a transformation and an implementation rule. Correct rule application for such rules is guaranteed, although we expect such operators and rules to be exceptions rather than the norm.</p>
<p>应用规则会创建一个新的表达式； 请注意，新表达式可以很复杂（由多个运算符组成，如连接关联性规则），并且可以是转换规则（创建新的逻辑表达式）或实现规则（创建新的物理表达式或计划） 。 事实上，由于运算符既可以是逻辑的，也可以是物理的，因此一个规则可以既是转换规则，又是实现规则。 尽管我们希望此类运算符和规则是例外而不是规范，但可以保证此类规则的正确应用。</p>
<p>Performing an ”apply rule” task is fairly complex. It may roughly be broken into four components. First, all bindings for the rule’s pattern are derived and iterated over one by one. Second, for each binding, the rule is used to create a new expression. Note that for function rules, there may be multiple new expressions for each binding. Third, the new expressions are integrated in the ”memo” structure. Within this process, exact replicas of expressions that already exist in ”memo” are identified and removed from further consideration. Fourth, each expression that is not a duplicate of an earlier one is optimized or explored with the same goal and context that triggered the current rule application. Let us discuss these four components in turn.</p>
<p>执行“应用规则”任务相当复杂。 它大致可以分为四个部分。 首先，规则模式的所有绑定都是逐一派生和迭代的。 其次，对于每个绑定，该规则用于创建一个新表达式。 请注意，对于函数规则，每个绑定可能有多个新表达式。 第三，新的表达方式被整合到“备忘录”结构中。 在此过程中，“备忘录”中已存在的表达式的精确副本将被识别并从进一步考虑中删除。 第四，每个与先前表达式不重复的表达式都会以触发当前规则应用程序的相同目标和上下文进行优化或探索。 让我们依次讨论这四个组成部分。</p>
<p>Since each rule’s antecedent (”before”-pattern) may be complex, the Cascades optimizer employs a complex procedure to identify all possible bindings for a rule. This procedure is recursive, with each recursive invocation for each node in the pattern. Most of its complexity serves to obtain all possible bindings for a rule’s pattern. In fact, the procedure is realized as an iterator that produces the next feasible binding with each invocation. The state of this iteration is captured in the ”BINDING” class with one instance of that class for each node in the pattern. Once a binding is found, it is translated into a tree consisting of ”EXPR” nodes (note that this class is part of the DBI interface, whereas the optimizer’s internal data structures are not). This copy step represents some effort, but it isolates the optimizer from the DBI methods that may be invoked for this tree. For each binding, the rule’s con- dition function is invoked and qualifying bindings are then translated into the rule’s consequent (”after”-pattern, substitute). For some rules, this is very easy and entirely left to the optimizer. For other rules, the DBI specified a function to create the substitute, and this function is invoked repeatedly to create as many substitute as possible. In other words, this function may be an iterator producing multiple substitutes in consecutive invocations. Thus, the effort of extracting a binding from the ”memo” is leveraged for multiple transformations if possible.</p>
<p>由于每个规则的先行词（“之前”模式）可能很复杂，因此 Cascades 优化器采用复杂的过程来识别规则的所有可能的绑定。 此过程是递归的，每次递归调用都会针对模式中的每个节点。 它的大部分复杂性是为了获取规则模式的所有可能的绑定。 事实上，该过程被实现为一个迭代器，每次调用都会生成下一个可行的绑定。 这一迭代的状态在“BINDING”类中捕获，模式中的每个节点都有该类的一个实例。 一旦找到绑定，它就会被转换为由“EXPR”节点组成的树（请注意，此类是 DBI 接口的一部分，而优化器的内部数据结构则不是）。 此复制步骤需要付出一定的努力，但它将优化器与可能为此树调用的 DBI 方法隔离开来。 对于每个绑定，都会调用规则的条件函数，然后将限定绑定转换为规则的结果（“after”模式，替换）。 对于某些规则，这非常简单，完全留给优化器。 对于其他规则，DBI指定一个函数来创建替代品，并且重复调用该函数以创建尽可能多的替代品。 换句话说，该函数可以是在连续调用中产生多个替代的迭代器。 因此，如果可能的话，从“备忘录”中提取绑定的努力可用于多次转换。</p>
<p>Each substitute expression is then integrated into the ”memo” structure. This process includes search for and detection of duplicates, i.e., expression that have been derived earlier in the optimization. This process is very similar to duplicate expression detection in both the EXODUS and Volcano optimizer generators. It is a recursive process that starts at the leaves of the substitute, which may be either query or plan tree leaves (i.e., scans) or leaf operators that denote the scope of a rewrite operation (as described as part of the DBI interface), and works upwards in the substitute towards the substitute’s root; this direction is required for correct duplicate detection. The search for duplicates is very fast as it employs a hash table using an operator and the groups of its inputs as keys.</p>
<p>然后将每个替换表达式集成到“备忘录”结构中。 该过程包括搜索和检测重复项，即先前在优化中导出的表达式。 此过程与 EXODUS 和 Volcano 优化器生成器中的重复表达检测非常相似。 这是一个从替代的叶子开始的递归过程，替代的叶子可以是查询或计划树叶子（即扫描）或表示重写操作范围的叶子运算符（如 DBI 接口的一部分所述）， 并在替代物中向上工作至替代物的根； 正确的重复检测需要这个方向。 搜索重复项的速度非常快，因为它使用哈希表，使用运算符及其输入组作为键。</p>
<p>Finally, if a substitute’s root is a new expression, follow-on tasks may be initiated. If the substitute was created as part of an exploration, a task is created to explore the substitute for the same pattern. If the substitute was created as part of an optimization, the follow-on tasks depend on whether the rule was a transformation or an implementation rule, i.e., whether the substitute’s root operator is a logical or a physical operator. Note, again, that an operator can be both logical and physical; thus, a rule can be both a transformation or an implementation rule. In that case, both types of follow-on tasks are created. For a logical root operator, an optimization task is created to optimize the substitute, keeping the same optimization goal. For a physical root operator, a new task is scheduled to optimize the operator’s inputs and to calculated processing costs. The ”optimize inputs” task is different from all other tasks. While all other tasks schedule their follow-on tasks and then vanish, this sixth task type become active multiple times. In other words, it schedules a follow-on task, waits for its completion, resumes and schedules the next follow-on task, etc. The follow-on tasks are all of the same type, which is optimizing input groups for a suitable optimization goal. Thus, like the Volcano search strategy, the Cascades search engine guarantees that only those subtrees and interesting properties are optimized that could indeed participate in a query evaluation plan. Each time after an input has been optimized, the optimize inputs task obtains the best execution cost derived, and derives a new cost limit for optimizing the next input. Thus, pruning is as tight as possible.</p>
<p>最后，如果替换的根是一个新的表达式，则可以启动后续任务。 如果替代品是作为探索的一部分创建的，则会创建一个任务来探索相同模式的替代品。 如果替换是作为优化的一部分创建的，则后续任务取决于规则是转换规则还是实现规则，即替换的根运算符是逻辑运算符还是物理运算符。 再次注意，运算符可以是逻辑运算符，也可以是物理运算符； 因此，规则既可以是转换规则，也可以是实现规则。 在这种情况下，将创建两种类型的后续任务。 对于逻辑根算子，创建优化任务来优化替代项，保持相同的优化目标。 对于物理根算子，计划一项新任务来优化算子的输入并计算处理成本。 “优化输入”任务不同于所有其他任务。 当所有其他任务安排其后续任务然后消失时，第六种任务类型会多次激活。 换句话说，它会调度一个后续任务，等待其完成，恢复并调度下一个后续任务，等等。后续任务都是同一类型，正在优化输入组以进行适当的优化 目标。 因此，与 Volcano 搜索策略一样，Cascades 搜索引擎保证只有那些确实可以参与查询评估计划的子树和有趣的属性才会被优化。 每次优化输入后，优化输入任务都会获得导出的最佳执行成本，并导出用于优化下一个输入的新成本限制。 因此，修剪尽可能严格。</p>
<h2 id="3-Data-Abstraction-and-the-User-Interface"><a href="#3-Data-Abstraction-and-the-User-Interface" class="headerlink" title="3 Data Abstraction and the User Interface"></a><strong>3 Data Abstraction and the User Interface</strong></h2><p>Developing the Cascades optimizer system required pursuing three different activities in rapid alternation. First, designing the interface between database implementor and optimizer had to focus on minimal, functional, and clean abstractions. Second, implementing a prototype optimizer as our own DBI was an exercise in exploiting the interface as effectively as possible. Third, design and implementation of an efficient search strategy was based on lessons learned during the EXODUS and Volcano projects, combined with the requirements set forth by a workshop of academic and industrial query optimization researchers and by the first user group of this software. Each of these three activities had different goals and required a different mind-set; in our internal discussions, we constantly alternated among these perspectives in order to design and develop a truly extensible and useful tool. In this section, we describe the data structuring decisions made for the interface between database implementor and the optimizer.</p>
<p>开发 Cascades 优化器系统需要快速交替执行三种不同的活动。 首先，设计数据库实现者和优化器之间的接口必须关注最小化、功能性和简洁的抽象。 其次，实现原型优化器作为我们自己的 DBI 是尽可能有效地利用接口的练习。 第三，高效搜索策略的设计和实施是基于 EXODUS 和 Volcano 项目期间吸取的经验教训，并结合学术和工业查询优化研究人员研讨会以及该软件的第一个用户组提出的要求。 这三项活动都有不同的目标，需要不同的心态； 在我们的内部讨论中，我们不断地交替使用这些观点，以设计和开发一个真正可扩展且有用的工具。 在本节中，我们描述为数据库实现者和优化器之间的接口做出的数据结构化决策。</p>
<p>Users of the EXODUS and Volcano optimizer generator generators made it very clear that the interface of these system could bear improvement. Feedback from users of the Volcano optimizer generator matches our own analysis [BMG93]; therefore, we focused on (i) clean abstractions for support functions in order to enable an optimizer generator to create them from specification, (ii) rule mechanisms that permit the DBI to choose rules or functions to manipulate operator arguments (such as predicates), and (iii) more concise and complete interface specifications, both in the code and in the written documentation. Following these guidelines, we designed the following interface.</p>
<p>EXODUS 和 Volcano 优化器生成器的用户明确表示，这些系统的界面需要改进。 Volcano 优化器生成器用户的反馈与我们自己的分析相符 [BMG93]； 因此，我们专注于（i）支持函数的干净抽象，以便使优化器生成器能够根据规范创建它们，（ii）允许 DBI 选择规则或函数来操作运算符参数（例如谓词）的规则机制， (iii) 代码和书面文档中的接口规范更加简洁和完整。 遵循这些准则，我们设计了以下界面。</p>
<p>Each of the classes that make up the interface between the Cascades optimizer and the DBI is designed to become the root of a subclass hierarchy. Thus, creation of new objects of one of these classes is associated with another class. For example, creation a new ”guidance” structure is associated with a ”rule” object. The rule object can be of some DBI-defined subclass of the interface class ”RULE,” and the newly created guidance structure can be of any DBI-defined subclass of the interface class ”GUIDANCE.” The optimizer relies only on the method defined in this interface; the DBI is free to add additional methods when defining subclasses.</p>
<p>构成 Cascades 优化器和 DBI 之间接口的每个类都被设计为成为子类层次结构的根。 因此，这些类之一的新对象的创建与另一个类相关联。 例如，创建新的“指导”结构与“规则”对象相关联。 规则对象可以是接口类“RULE”的某个DBI定义的子类，并且新创建的指导结构可以是接口类“GUIDANCE”的任何DBI定义的子类。 优化器仅依赖于该接口中定义的方法； DBI 在定义子类时可以自由添加额外的方法。</p>
<h3 id="3-1-Operators-and-Their-Arguments"><a href="#3-1-Operators-and-Their-Arguments" class="headerlink" title="3.1 Operators and Their Arguments"></a><strong>3.1 Operators and Their Arguments</strong></h3><p>Central to any database query optimizer are the sets of operators supported in the query language and in the query evaluation engine. Notice that these two sets are different; we call them logical and physical operators [Gra93]. While previous extensible operators required that these two sets be disjunct, we have abandoned this requirement. The ”class OP-ARG” in the Cascades optimizer interface includes both logical and physical operators. For each operator, one method called ”is-logical” indicates whether or not an operator is a logical operator, while a second method called ”is-physical” indicates whether or not an operator is a physical operator. In fact, it is possible that an operator is neither logical or physical; such an operator might be useful if the optimization is organized as an expansion grammar including ”non-terminals” like the Starburst optimizer [Loh88]. On the other hand, a DBI who wishes to do so can easily retain a strict separation of logical and physical operators, e.g., by defining subclasses with suitable definitions for the methods ”is-logical” and ”is-physical” and by defining all operators as subclasses of these two classes.</p>
<p>任何数据库查询优化器的核心都是查询语言和查询评估引擎中支持的运算符集。 请注意，这两组是不同的； 我们称它们为逻辑和物理运算符[Gra93]。 虽然以前的可扩展运算符要求这两个集合是分离的，但我们已经放弃了这一要求。 Cascades 优化器接口中的“类 OP-ARG”包括逻辑运算符和物理运算符。 对于每个运算符，一种称为“is-logic”的方法指示该运算符是否是逻辑运算符，而第二种称为“is-physical”的方法指示该运算符是否是物理运算符。 事实上，操作符有可能既不是逻辑操作符也不是物理操作符； 如果优化被组织为包含“非终结符”的扩展语法（如 Starburst 优化器 [Loh88]），这样的运算符可能会很有用。 另一方面，希望这样做的 DBI 可以轻松地保持逻辑和物理运算符的严格分离，例如，通过为方法“is-logic”和“is-physical”定义适当的定义来定义子类，并定义所有 运算符作为这两个类的子类。</p>
<p>The definition of operators includes their arguments. Thus, no separate mechanisms are required or provided for ”argument transfer” as in EXODUS and Volcano. Notice, however, that there are two crucial facilities that permits and encourage modeling predicates etc., which had been modeled as operator arguments in all our pro- totypes constructed in the EXODUS and Volcano frameworks, as primary operators in the logical and physical algebra’s. First, an operator can be both logical and physical, which is natural for single-record predicates, called ”sargable” in System R [SAC79]. Second, specific predicate transformations, e.g., splitting from a complex pred- icate those components that can be pushed through a join, which are most easily and efficiently implemented in a DBI function rather than as rules to be interpreted by the optimizer’s search engine, can easily be realized in rules that invoke a DBI-supplied to map an expression to substitute expressions (one or more). Thus, after the EXODUS and the Volcano work has been repeatedly criticized that predicate manipulation has been very cum- bersome, the Cascades optimizer offers much improved facilities.</p>
<p>运算符的定义包括它们的参数。 因此，不需要或为 EXODUS 和 Volcano 中的“参数转移”提供单独的机制。 然而，请注意，有两个关键的设施允许和鼓励建模谓词等，它们在我们在 EXODUS 和 Volcano 框架中构建的所有原型中被建模为运算符参数，作为逻辑和物理代数中的主要运算符。 首先，运算符既可以是逻辑运算符，也可以是物理运算符，这对于单记录谓词来说是很自然的，在 System R [SAC79] 中称为“sargable”。 其次，特定的谓词转换，例如，从复杂的谓词中分离出那些可以通过连接推送的组件，这些组件在 DBI 函数中最容易、最有效地实现，而不是作为由优化器的搜索引擎解释的规则，可以 很容易在调用 DBI 提供的规则中实现，以将表达式映射到替换表达式（一个或多个）。 因此，在 EXODUS 和 Volcano 工作多次被批评谓词操作非常繁琐之后，Cascades 优化器提供了大大改进的设施。</p>
<p>The optimizer’s design does not include assumptions about the logical and physical algebra’s to be optimized; therefore, no query or plan operators are built into the optimizer. For use in rules, however, there are two special operators, called ”LEAF-OP” and ”TREE-OP.” The leaf operator can be used as leaf in any rule; during matching, it matches any subtree. Before a rule is applied, an expression is extracted from the search memory that matches the rule’s pattern; where the rule’s pattern has leaves, the extracted expression also has leaf operators that refer (via an array index) to equivalence classes in the search memory. The tree operator is like the leaf operator except that the extracted expression contains an entire expression, independent of its size or complexity, down to the leaf operators in the logical algebra. This operator is particularly useful in connection with function rules, which are described below.</p>
<p>优化器的设计不包括有关要优化的逻辑和物理代数的假设； 因此，优化器中没有内置任何查询或计划运算符。 然而，为了在规则中使用，有两个特殊的运算符，称为“LEAF-OP”和“TREE-OP”。 叶子运算符可以用作任何规则中的叶子； 匹配时，匹配任意子树。 在应用规则之前，从搜索内存中提取与规则模式匹配的表达式； 如果规则的模式具有叶子，则提取的表达式还具有叶子运算符，这些运算符（通过数组索引）引用搜索内存中的等价类。 树运算符类似于叶运算符，只不过提取的表达式包含整个表达式（与其大小或复杂性无关），直至逻辑代数中的叶运算符。 该运算符在与函数规则结合时特别有用，如下所述。</p>
<p>Beyond the methods ”is-logical” and ”is-physical,” all operators must provide a method ”opt- cutoff”. Given a set of moves during an optimization task, this method determines how many of those will be pursued, obviously the most promising ones. By default, all possible moves will be pursued, because exhaustive search guarantees that the optimal plan will be found. There is also a small set of methods that must be provided only for those operators that have been declared logical. For pattern matching and for finding duplicate expressions, methods for matching and hashing are required. Methods for finding and improving logical properties are used to determine an original set of properties (e.g., the schema) and then to improve it when alternative expressions have been found (e.g., more bounds on selectivity or the output size). Finally, for exploration tasks, an operator may be called upon to initialize a pattern memory and to decide how many moves to pursue during an exploration task.</p>
<p>除了“is-logic”和“is-physical”方法之外，所有操作员都必须提供“opt-cutoff”方法。 给定优化任务期间的一组移动，此方法确定将执行其中的多少个，显然是最有希望的移动。 默认情况下，将追求所有可能的移动，因为穷举搜索保证找到最佳计划。 还有一小组必须仅为已声明为逻辑的运算符提供的方法。 对于模式匹配和查找重复表达式，需要匹配和散列方法。 查找和改进逻辑属性的方法用于确定一组原始属性（例如，模式），然后在找到替代表达式（例如，选择性或输出大小的更多界限）时对其进行改进。 最后，对于探索任务，操作员可能被要求初始化模式存储器并决定在探索任务期间要进行多少次移动。</p>
<p>Similarly, there are some methods for physical operators. Obviously, there is a method to determine an oper- ator’s (physical) output properties, i.e., properties of the representation. Moreover, there are three methods that compute and inspect costs. The first of these calculates the local cost of an algorithm, without any regard to the costs of its inputs. The second one combines the costs and physical properties of an algorithm’s inputs into the cost of an entire subplan. The third of these methods verifies, between optimizing two inputs of an algorithm, that the cost limit has not been exceeded yet, and computes a new cost limit to be used when optimizing the next input. Finally, just as the last method maps an expression’s cost limit to a cost limit for one of its inputs, there is a method that maps the optimization goal for an expression to an optimization goal for one of its inputs, i.e., a cost limit and required and excluded physical properties, called ”input-reqd-prop.” Let us discuss properties and their methods next.</p>
<p>同样，物理操作符也有一些方法。 显然，有一种方法可以确定算子的（物理）输出属性，即表示的属性。 此外，还有三种计算和检查成本的方法。 第一个计算算法的本地成本，而不考虑其输入的成本。 第二个将算法输入的成本和物理属性结合到整个子计划的成本中。 第三种方法在优化算法的两个输入之间验证是否尚未超出成本限制，并计算优化下一个输入时要使用的新成本限制。 最后，正如最后一种方法将表达式的成本限制映射到其输入之一的成本限制一样，有一种方法将表达式的优化目标映射到其输入之一的优化目标，即成本限制和 必需和排除的物理属性，称为“input-reqd-prop”。 接下来让我们讨论属性及其方法。</p>
<h3 id="3-2-Logical-and-Physical-Properties-Costs"><a href="#3-2-Logical-and-Physical-Properties-Costs" class="headerlink" title="3.2 Logical and Physical Properties, Costs"></a><strong>3.2 Logical and Physical Properties, Costs</strong></h3><p>The interface to the interface for anticipated execution costs, the ”class COST,” is very simple, since instances of costs are created and returned by methods associated with other classes, e.g., operators. Beyond destruction and printing, the only method for costs is a comparison method. Similarly, the only method for the encapsulation of logical properties, the ”class SYNTH-LOG- PROP,” is a hash function that permits faster retrieval of duplicate expressions Since even this function does not apply to physical expressions, the encapsulation for physical prop- erties, the ”class SYNTH-PHYS-PROP,” has no methods at all. The class for required physical properties, the ”class REQD-PHYS-PROP,” has only one method associated with it, which determines whether a synthesized physical property instance covers the required physical properties. If one set of properties is more specific than another, e.g., one indicates a result sorted on attributes ”A, B, C” and the one requires sort order on ”A, B” only, the comparison method returns the value ”MORE.” The default implementation of this method returns the value ”UNDEFINED.”</p>
<p>预期执行成本的接口“类 COST”非常简单，因为成本实例是由与其他类（例如运算符）关联的方法创建和返回的。 除了销毁和印刷之外，衡量成本的唯一方法就是比较法。 类似地，封装逻辑属性的唯一方法“类 SYNTH-LOG-PROP”是一个散列函数，它允许更快地检索重复表达式。由于即使该函数也不适用于物理表达式，所以对物理属性的封装 erties，“SYNTH-PHYS-PROP 类”根本没有方法。 所需物理属性的类，“类REQD-PHYS-PROP”，只有一个与其关联的方法，该方法确定合成的物理属性实例是否涵盖所需的物理属性。 如果一组属性比另一组更具体，例如，一组属性指示按属性“A、B、C”排序的结果，而一组属性只需要按“A、B”排序，则比较方法返回值“MORE”。 ” 此方法的默认实现返回值“UNDEFINED”。</p>
<h3 id="3-3-Expression-Trees"><a href="#3-3-Expression-Trees" class="headerlink" title="3.3 Expression Trees"></a><strong>3.3 Expression Trees</strong></h3><p>In order to communicate expressions between the DBI and the optimizer, e.g., as queries, as plans, or in rules, another abstract data type is part of the interface, call the ”class EXPR.” Each instance of this class is a node in a tree, consisting of an operator and to pointers to input nodes. Obviously, the number of children in any expres- sion node must be equal to the arity function of the node’s operator. Methods on an expression node, beyond constructor, destructor, and printing, include methods to extract the operator or one of the inputs as well as a matching method, which recursively traverses two expression trees and invokes the matching method for each node’s operator.</p>
<p>为了在 DBI 和优化器之间传递表达式（例如，作为查询、作为计划或在规则中），另一种抽象数据类型是接口的一部分，称为“类 EXPR”。 此类的每个实例都是树中的一个节点，由一个运算符和指向输入节点的指针组成。 显然，任何表达式节点中的子节点数量必须等于节点运算符的元数函数。 除了构造函数、析构函数和打印之外，表达式节点上的方法还包括提取运算符或输入之一的方法以及匹配方法，该方法递归遍历两个表达式树并为每个节点的运算符调用匹配方法。</p>
<p><strong>3.4 Search Guidance</strong></p>
<p>In addition to pattern, cost limits, and required and excluded physical properties, rule application can also con- trolled by heuristics represented by instances of the ”class GUIDANCE.” Its purpose is to transfer optimization heuristics from one rule application to the next. Notice that costs and properties pertain to the expressions be- ing manipulated and to the intermediate result those expressions will product when a query plan is executed; the guidance class captures knowledge about the search process and heuristics for future search activities. For ex- ample, some rules such as commutativity rules are to applied only once; for those, a simple guidance structure and a rule class are provided as part of the DBI interface, called ”ONCE-GUIDANCE” and ”ONCE-RULE.”</p>
<p>除了模式、成本限制以及必需和排除的物理属性之外，规则应用还可以通过“类 GUIDANCE”实例所代表的启发法来控制。 其目的是将优化启发式从一个规则应用程序转移到下一个规则应用程序。 请注意，成本和属性与正在操作的表达式以及执行查询计划时这些表达式将产生的中间结果有关； 指导类获取有关搜索过程的知识以及未来搜索活动的启发法。 例如，某些规则（例如交换性规则）仅适用一次； 对于这些，一个简单的指导结构和一个规则类作为 DBI 接口的一部分提供，称为“ONCE-GUIDANCE”和“ONCE-RULE”。</p>
<p>Some researchers have advocated to divide a query optimizer’s rule set into ”modules” that can be invoked one at a time, e.g., Mitchell et al. [MDZ93]. Guidance structures can easily facilitate this design: a guidance structure indicates which module is to be chosen, and each rule checks this indication in its promise (or condition) function and then creates suitable indications when creating guidance structures for its</p>
<p> newly created expressions and their inputs.</p>
<p>一些研究人员主张将查询优化器的规则集划分为可以一次调用一个的“模块”，例如，Mitchell 等人。 [MDZ93]。 指导结构可以轻松地促进这种设计：指导结构指示要选择哪个模块，每个规则在其承诺（或条件）函数中检查该指示，然后在为其新创建的表达式及其输入创建指导结构时创建合适的指示 。</p>
<p><strong>3.5 Pattern Memory</strong></p>
<p>In addition to the search guidance, exploration effort can be restricted by use of the pattern memory. The purpose of the pattern memory is to prevent that the same group is explored unnecessarily, e.g., twice for the same pattern. There is one instance of a pattern memory associated with each group. Before a group is explored for a pattern, the pattern memory is permitted to add the pattern to itself and is asked to determine whether or not exploration should take place. In the most simple search, in which exploration for any pattern is performed by exhaustive application of transformation rules, the pattern memory needs to contain only a Boolean, i.e., a memory whether or not the group has been explored previously. More sophisticated pattern memories would store each pattern.</p>
<p>除了搜索指导之外，还可以通过使用模式存储器来限制探索工作。 模式记忆的目的是防止不必要地探索同一组，例如同一模式两次。 每一组都有一个与模式存储器相关联的实例。 在对一个组进行模式探索之前，模式存储器被允许将该模式添加到自身中，并被要求确定是否应该进行探索。 在最简单的搜索中，通过详尽地应用转换规则来执行对任何模式的探索，模式存储器只需要包含一个布尔值，即无论该组之前是否已被探索过的存储器。 更复杂的模式存储器将存储每个模式。</p>
<p>Obviously, the pattern memory interacts with the exploration promise function. For the most simple promise function that always admits exhaustive search, the simple pattern memory above is suitable. It is left to the DBI to design pattern memory and promise functions most suitable to the algebra to be optimized.</p>
<p>显然，模式记忆与探索承诺函数相互作用。 对于总是允许穷举搜索的最简单的 Promise 函数，上面的简单模式内存是合适的。 DBI 负责设计模式内存并承诺最适合待优化代数的函数。</p>
<p>Beyond checking whether a given pattern already exists in the memory, and saving it to detect a second ex- ploration with the same pattern, the most complex method for pattern memories is to merge two pattern memories into one. This method is required when two groups of equivalent expressions are detected to be actually one, i.e., when a transformed expression already occurs in a different group in the search memory.</p>
<p>除了检查给定模式是否已经存在于内存中，并将其保存以检测相同模式的第二次探索之外，模式记忆最复杂的方法是将两个模式记忆合并为一个。 当检测到两组等效表达式实际上是一组时，即当变换后的表达式已经出现在搜索存储器中的不同组中时，需要此方法。</p>
<h3 id="3-6-Rules"><a href="#3-6-Rules" class="headerlink" title="3.6 Rules"></a><strong>3.6 Rules</strong></h3><p>Next to operators, the other important class of objects in the Cascades optimizer are rules. Notice that rules are objects; thus, new ones can be created at run-time, they can be printed, etc. While other rule-based optimizers, in particular the EXODUS and Volcano optimizer generators, divide logical and physical operators as well as (logical) transformation and (physical) implementation rules into disjoint sets, the Cascades optimizer does not distinguish between those rules, other than by invoking the is-logical and is-physical methods on newly created expressions. All rules are instances of the ”class RULE,” which provides for rule name, an antecedent (the ”be- fore” pattern), and a consequent (the substitute). Pattern and substitute are represented as expression trees, which were discussed above.</p>
<p>除了运算符之外，Cascades 优化器中另一类重要的对象是规则。 请注意，规则是对象； 因此，可以在运行时创建新的运算符，可以打印它们等。而其他基于规则的优化器，特别是 EXODUS 和 Volcano 优化器生成器，将逻辑运算符和物理运算符分开，以及（逻辑）转换和（物理） ) 实现规则到不相交的集合中，Cascades 优化器不会区分这些规则，除非对新创建的表达式调用 is-logic 和 is-physical 方法。 所有规则都是“类 RULE”的实例，它提供了规则名称、前件（“之前”模式）和后件（替代项）。 模式和替代被表示为表达式树，这在上面已经讨论过。</p>
<p>In their simplest case, rules do not contain more than that; whenever the pattern is found or can be created with exploration tasks, the substitute expression is included in the search memory. Both a rule’s pattern and sub- stitute can be arbitrarily complex. In the EXODUS and Volcano optimizer generators, an implementation rule’s substitute could not consist of more than a single implementation operator; in the Cascades design, this restriction has been removed. The remaining restriction is that all but the substitute’s top operator must be logical opera- tors. For example, it is possible to transform a (logical) join operator into a (physical) nested loops operator with a (logical) selection on its inner input, thus, detaching the selection predicate from the join algorithm and pushing it into the inner input tree.</p>
<p>在最简单的情况下，规则仅包含这些内容； 每当找到或可以通过探索任务创建模式时，替代表达式就会包含在搜索内存中。 规则的模式和替代规则都可以任意复杂。 在 EXODUS 和 Volcano 优化器生成器中，实现规则的替代品不能包含多个实现运算符； 在Cascades设计中，这个限制已经被去掉了。 剩下的限制是除了替换的顶级运算符之外的所有运算符都必须是逻辑运算符。 例如，可以将（逻辑）连接运算符转换为（物理）嵌套循环运算符，并在其内部输入上进行（逻辑）选择，从而将选择谓词与连接算法分离并将其推入内部输入 树。</p>
<p>For more sophisticated rules, two types of condition functions are supported. All of them consider not only the rule but also the current optimization goal, i.e., cost limit and required and excluded physical properties. First, before exploration starts, ”promise” functions informs the optimizer how useful the rule might be. There is one promise function for optimization tasks and one for exploration tasks. For unguided exhaustive search, all promise functions should return the value 1.0. A value of 0 or less will prevent the optimizer from further work for the current rule and expression. The default promise function returns 0 if a specific physical property is required, 2 if the substitute is an implementation algorithm, and 1 otherwise. If the cutoff methods associated with the operators choose exhaustive search (see above), the return value of the promise function will not change the quality of the final query evaluation plan, although it may affect the order in which plans are found, pruning effectiveness, and therefore the time an optimization takes.</p>
<p>对于更复杂的规则，支持两种类型的条件函数。 它们不仅考虑了规则，还考虑了当前的优化目标，即成本限制以及所需和排除的物理属性。 首先，在探索开始之前，“promise”函数会通知优化器该规则的有用性。 有一种用于优化任务的承诺函数，一种用于探索任务的承诺函数。 对于无引导的穷举搜索，所有 Promise 函数都应返回值 1.0。 0 或更小的值将阻止优化器进一步处理当前规则和表达式。 如果需要特定的物理属性，则默认的 Promise 函数返回 0；如果替代是实现算法，则返回 2；否则返回 1。 如果与运算符相关的截止方法选择穷举搜索（见上文），则 Promise 函数的返回值不会改变最终查询评估计划的质量，尽管它可能会影响计划被发现的顺序、剪枝有效性、 以及优化所需的时间。</p>
<p>Since the promise functions are invoked before exploration of subgroups, i.e., before entire expression trees corresponding to a rule’s pattern have been explored and extracted from the search memory, a ”condition” func- tion checks whether a rule is truly applicable after exploration is complete and a complete set of operators corre- sponding to the pattern in the rule is available. Whereas the promise functions return a real value that expresses grades of promise, the condition function returns a Boolean to indicate whether or not the rule is applicable.</p>
<p>由于 Promise 函数是在探索子组之前调用的，即在探索并从搜索内存中提取与规则模式相对应的整个表达式树之前，“条件”函数会在探索完成后检查规则是否真正适用 并且与规则中的模式对应的完整的运算符集是可用的。 承诺函数返回表示承诺等级的实际值，而条件函数返回布尔值以指示规则是否适用。</p>
<p>In addition to promise and condition functions, a small set of methods is associated with rules. Of course, there are constructor, destructor, and print methods, as well as method to extract the pattern, the substitute, the rule’s name, and its arity (the pattern’s number of leaf operators). The ”rule-type” method indicates whether a  rule is a simple rule (as described so far) or a function rule (to be described shortly). The ”top-match” method determines whether or not an operator in the search memory matches the top operator in the rule’s pattern; this method is the only built-in check before an promise function is invoked. The method ”opt-cases” indicates how often a physical expression is to be optimized with different physical properties. In all but a few cases, this will be one; one of the few exception is a merge-join algorithm with two equality clauses (say ”R.A &#x3D;&#x3D; S.A and R.B &#x3D;&#x3D; S.B”) that should be optimized for two sort orders (sorted on ”A, B” and on ”B, A”). By default, this method returns 1. The remaining methods all create new guidance structures to be used when optimizing a newly created expression and its inputs. There are two methods each for optimization and for exploration, and two each for the new expression and for its inputs, called ”opt-guidance,” ”expl-guidance,” ”input-opt-guidance,” and ”input- expl-guidance.” By default, all of them return ”NULL,” i.e., no specific guidance.</p>
<p>除了承诺和条件函数之外，还有一小组与规则相关的方法。 当然，还有构造函数、析构函数和打印方法，以及提取模式、替代项、规则名称及其数量（模式的叶运算符的数量）的方法。 “规则类型”方法指示规则是简单规则（如上所述）还是函数规则（稍后描述）。 “top-match”方法确定搜索存储器中的操作符是否与规则模式中的顶部操作符匹配； 此方法是调用 Promise 函数之前唯一的内置检查。 方法“opt-cases”指示使用不同物理属性优化物理表达式的频率。 除了少数情况外，在所有情况下，这都是一个； 少数例外之一是具有两个相等子句（例如“R.A &#x3D;&#x3D; S.A 和 R.B &#x3D;&#x3D; S.B”）的合并连接算法，该算法应针对两种排序顺序进行优化（按“A，B”和“B”排序， A”）。 默认情况下，此方法返回 1。其余方法都创建新的指导结构，以便在优化新创建的表达式及其输入时使用。 优化和探索各有两种方法，新表达式及其输入各有两种方法，称为“opt-guidance”、“expl-guidance”、“input-opt-guidance”和“input-expl-” 指导。” 默认情况下，它们都返回“NULL”，即没有具体指导。</p>
<p>If a rule’s substitute consists of only a leaf operator, the rule is a reduction rule. If a reduction rule is appli- cable, two groups in the search memory will be merged. On the other hand, if a rule’s pattern consists of only a leaf operator, the rule is an expansion rule that is always applicable. The Cascades optimizer must rely on the DBI to design appropriate promise and condition functions to avoid useless transformations. Nonetheless, there is an important class of situations in which expansion rules are useful, namely the insertion of physical operators that enforce or guarantee desired physical properties. Such rules may also be called enforcer rules. Consider the inputs to a merge-join’s inputs, which must be sorted. An enforcer rule may insert a sort operation, the rule’s promise and condition functions must permit this rule only of sort order is required, and the sort operator’s ”input- reqd-prop” method must set excluded properties to avoid consideration of plans that produce their output in the desired sort order as input to the sort operator.</p>
<p>如果规则的替代仅由叶运算符组成，则该规则是归约规则。 如果适用缩减规则，搜索存储器中的两组将被合并。 另一方面，如果规则的模式仅由叶运算符组成，则该规则是始终适用的扩展规则。 Cascades优化器必须依赖DBI来设计适当的承诺和条件函数以避免无用的转换。 尽管如此，有一类重要的情况下扩展规则是有用的，即插入强制或保证所需物理属性的物理运算符。 此类规则也可称为执行者规则。 考虑合并连接输入的输入，这些输入必须进行排序。 执行者规则可以插入排序操作，规则的承诺和条件函数必须允许仅需要排序顺序的规则，并且排序运算符的“input-reqd-prop”方法必须设置排除的属性，以避免考虑产生其结果的计划。 以所需的排序顺序输出作为排序运算符的输入。</p>
<p>In some situations, it is easier to write a function that directly transforms an expression than to design and control a rule set for the same transformation. For example, dividing a complex join predicate into clauses that apply to left, right, and both inputs is a deterministic process best implemented by a single function. For those cases, the Cascades optimizer supports a second class of rules, called the ”class FUNCTION-RULE.” Once an expression is extracted that corresponds to the rule’s pattern, an iterator method is invoked repeatedly to create all substitutes for the expression. Note that the extracted expression can be arbitrarily deep and complex if the tree operator (see above) is employed in the rule’s pattern. Thus, tree operators and function rules permit the DBI to write just about any transformation. In the extreme case, a set of function rules could perform all query transformations, although that would defeat some of the Cascades framework’s purpose.</p>
<p>在某些情况下，编写直接转换表达式的函数比设计和控制同一转换的规则集更容易。 例如，将复杂的连接谓词划分为适用于左、右和两个输入的子句是一个确定性过程，最好由单个函数实现。 对于这些情况，Cascades 优化器支持第二类规则，称为“类 FUNCTION-RULE”。 一旦提取出与规则模式相对应的表达式，就会重复调用迭代器方法来创建该表达式的所有替代项。 请注意，如果在规则模式中使用树运算符（见上文），则提取的表达式可以是任意深度和复杂的。 因此，树运算符和函数规则允许 DBI 编写任何转换。 在极端情况下，一组函数规则可以执行所有查询转换，尽管这会违背 Cascades 框架的某些目的。</p>
<h2 id="4-Future-Work"><a href="#4-Future-Work" class="headerlink" title="4 Future Work"></a><strong>4 Future Work</strong></h2><p>Of course, there is a lot of work that should be done to make the Cascades optimizer more useful and complete. First, the optimizer has not yet gone through a thorough evaluation and tuning phase. Second, building addi- tional optimizers based on this framework will undoubtedly show many weaknesses not yet apparent. Third, one or more generators that produce Cascades specifications from higher-level data model and algebra descriptions would be very useful. Fourth, we already know of a number of desirable improvements for the search strategy and its implementation.</p>
<p>当然，要使 Cascades 优化器更加有用和完整，还有很多工作要做。 首先，优化器尚未经历彻底的评估和调整阶段。 其次，基于这个框架构建额外的优化器无疑会显示出许多尚未明显的弱点。 第三，从更高级别的数据模型和代数描述生成级联规范的一个或多个生成器将非常有用。 第四，我们已经知道搜索策略及其实施方面有许多值得改进的地方。</p>
<p>The Cascades optimizer was designed to be reasonably fast, although extensibility was a more important de- sign goal. Among the artifacts of separating the optimizer framework and the DBI’s specification of operators, cost functions, etc. are extensive use of virtual methods, a very large number of references between structures, and very frequent object allocation and deallocation. While unavoidable, there probably is room for improve- ment, in particular if one is willing to give up the strong separation that permits modifications of DBI code without recompiling Cascades code. Before this ”de-modularization” step is taken, however, a strong argument should be made based on a measurement study that this would indeed improve an optimizer’s performance.</p>
<p>尽管可扩展性是更重要的设计目标，但 Cascades 优化器的设计速度相当快。 将优化器框架与 DBI 的运算符、成本函数等规范分开的工件包括虚拟方法的广泛使用、结构之间的大量引用以及非常频繁的对象分配和释放。 虽然不可避免，但可能还有改进的空间，特别是如果愿意放弃允许修改 DBI 代码而无需重新编译 Cascades 代码的强分离。 然而，在采取这种“去模块化”步骤之前，应该根据测量研究提出强有力的论据，证明这确实会提高优化器的性能。</p>
<h2 id="5-Summary-and-Conclusions"><a href="#5-Summary-and-Conclusions" class="headerlink" title="5 Summary and Conclusions"></a><strong>5 Summary and Conclusions</strong></h2><p>Beyond a better and more robust implementation than found in the EXODUS and Volcano optimizer generators, the Cascades optimizer offers a number of advantages, without giving up modularity, extensibility, dynamic pro- gramming, and memorization explored in those earlier prototypes. First, predicates and other item operations can conveniently modeled as part of the query and plan algebra. Operators that are both logical and physical; thus, it is easy to specify operators that may appear both in the optimizer input (the query) an din its output (the plan). Function rules and the tree operator permit direct manipulation of even complex trees of item operations using DBI-supplied functions. Second, enforcers such as sorting are normal operators in all ways; in particular, they are inserted into a plan based on explicit rules. In Volcano, they were special operators that did not appear in any rule. Third, both exploration (enumeration of equivalent logical expressions) and optimization (mapping a logical to a physical expression) can be guided and controlled by the DBI. Together with the more robust imple- mentation as required for industrial deployment, we believe that the Cascades optimizer represents a substantial improvement over earlier extensible database query optimizers.</p>
<p>除了比 EXODUS 和 Volcano 优化器生成器更好、更强大的实现之外，Cascades 优化器还提供了许多优点，同时又不放弃早期原型中探索的模块化、可扩展性、动态编程和记忆功能。 首先，谓词和其他项操作可以方便地建模为查询和计划代数的一部分。 逻辑和物理运算符； 因此，很容易指定可能同时出现在优化器输入（查询）和输出（计划）中的运算符。 函数规则和树运算符允许使用 DBI 提供的函数直接操作甚至复杂的项目操作树。 其次，像排序这样的执行者在所有方面都是正常的操作者； 特别是，它们被插入到基于明确规则的计划中。 在火山中，他们是没有出现在任何规则中的特殊干员。 第三，探索（等效逻辑表达式的枚举）和优化（将逻辑表达式映射到物理表达式）都可以由 DBI 引导和控制。 结合工业部署所需的更强大的实现，我们相信 Cascades 优化器比早期的可扩展数据库查询优化器有了重大改进。</p>
<h2 id="6-Acknowledgments"><a href="#6-Acknowledgments" class="headerlink" title="6 Acknowledgments"></a><strong>6 Acknowledgments</strong></h2><p>The query processing group at Tandem has been very helpful in forcing me to address the hard problems unre- solved in the EXODUS and Volcano optimizer generators and in finding effective and usable solutions. David Maier has been a great sounding board for ideas during the design and development of the Cascades optimizer.</p>
<p>Tandem 的查询处理小组非常有帮助，迫使我解决 EXODUS 和 Volcano 优化器生成器中未解决的难题，并找到有效且可用的解决方案。 在 Cascades 优化器的设计和开发过程中，David Maier 一直是一个很好的意见反馈者。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>Consensus, made thrive</title>
    <url>/2024/01/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/06-Consensus,%20made%20thrive/</url>
    <content><![CDATA[<p><a href="https://www.cockroachlabs.com/blog/consensus-made-thrive/">https://www.cockroachlabs.com/blog/consensus-made-thrive/</a></p>
<h1 id="Consensus-made-thrive"><a href="#Consensus-made-thrive" class="headerlink" title="Consensus, made thrive"></a>Consensus, made thrive</h1><p>When you write data to CockroachDB (for example, if you insert a row into a table through the SQL client), we take care of replication for you. To do this, we use a <a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">consensus protocol</a> – an algorithm which makes sure that your data is safely stored on multiple machines, and that those machines agree on the current state even if some of them are temporarily disconnected.</p>
<p>当您将数据写入 CockroachDB 时（例如，如果您通过 SQL 客户端向表中插入一行），我们会为您处理复制。 为此，我们使用共识协议——一种确保您的数据安全存储在多台机器上的算法，并且即使其中一些机器暂时断开连接，这些机器也能就当前状态达成一致。</p>
<p>In this post, I will give an overview of common implementation concerns and how we address these concerns in CockroachDB. Then I will abandon these earthly constraints and explore how we could improve consensus algorithms. Specifically, what would it take to make them faster?</p>
<p>在这篇文章中，我将概述常见的实施问题以及我们如何在 CockroachDB 中解决这些问题。 然后我将放弃这些现实的限制并探索如何改进共识算法。 具体来说，怎样才能让它们更快？</p>
<span id="more"></span>

<h2 id="Consensus-Algorithms-Applied"><a href="#Consensus-Algorithms-Applied" class="headerlink" title="Consensus Algorithms Applied"></a>Consensus Algorithms Applied</h2><p>Consensus algorithms are inherently distributed, and the problem they solve is fundamental to any piece of software which wants to keep a consistent state across multiple machines. After several decades, the body of research on them seemingly presents you with a variety of implementations options to choose from. However, as pointed out in <a href="http://research.google.com/archive/paxos_made_live.html">Google’s “Paxos Made Live,”</a> <strong>using consensus algorithms in the real world is not quite as simple:</strong> the things that matter most for real implementations are often mere side notes in their respective papers.</p>
<p>共识算法本质上是分布式的，它们解决的问题对于任何想要在多台机器上保持一致状态的软件来说都是基础。 几十年后，对它们的研究似乎为您提供了多种实现选项可供选择。 然而，正如 Google 的“Paxos Made Live”中指出的那样，在现实世界中使用共识算法并不那么简单：对于实际实现来说最重要的事情通常只是各自论文中的旁注。</p>
<p>A typical consensus algorithm accepts operations from a client, and puts them in an ordered log (which in turn is kept on each of the replicas), acknowledging an operation as successful to the client once it is known that the operation has been persisted in a majority of the replicas’ logs. Each of the replicas in turn execute operations from that log in order, advancing their state. This means that at a fixed point in time, the replicas may not be identical, but they are advancing through the same log (meaning that if you give them time to all catch up, they will be in the same state) – the best you can hope for in a distributed system.</p>
<p>典型的共识算法接受来自客户端的操作，并将它们放入有序日志中（该日志又保存在每个副本上），一旦知道该操作已保存在一个副本中，则向客户端确认操作成功。 大多数副本的日志。 每个副本依次按顺序执行该日志中的操作，从而推进其状态。 这意味着在某个固定的时间点，副本可能不相同，但它们通过相同的日志前进（这意味着如果你给它们时间全部赶上，它们将处于相同的状态）——最好的 可以希望在分布式系统中。</p>
<p><strong>Typical Concerns When Implementing a Consensus Algorithm:</strong></p>
<p>实施共识算法时的典型问题：</p>
<ul>
<li><p><strong>Log Truncation</strong>: Having all of the operations in an ordered log is fine, but that log can’t grow forever! When all replicas have caught up, older log entries should be discarded.</p>
<p>日志截断：将所有操作放在有序日志中很好，但该日志不能永远增长！ 当所有副本都赶上时，应丢弃较旧的日志条目。</p>
</li>
<li><p><strong>Snapshotting</strong>: Since the log can’t be kept forever, after an extended period of downtime of a replica, there must be an alternative way of catching it up. The only option is transferring a snapshot of the data and a log position from which to resume.</p>
<p>快照：由于日志无法永久保留，因此在副本长时间停机后，必须有其他方法来捕获它。 唯一的选择是传输数据快照和要恢复的日志位置。</p>
</li>
<li><p><strong>Membership Changes</strong>: These are very tricky to get right. As we add a replica to the group, the size of a majority changes. A lot of decisions have to be made: which majority size is active while the membership changes? Does the new replica have any say in the group while it’s being added? When does it receive a snapshot? Can a snapshot be sent before the membership change is carried out, to minimize the impact of the change? Removal is similarly iffy, and the consensus group is typically more vulnerable while the process is ongoing.</p>
<p>会员变更：这些是非常棘手的。 当我们向组中添加副本时，多数副本的大小会发生变化。 必须做出很多决定：当成员发生变化时，哪个多数规模是活跃的？ 新副本在添加时在组中是否有发言权？ 它什么时候收到快照？ 是否可以在成员资格更改之前发送快照，以尽量减少更改的影响？ 删除也同样存在不确定性，并且在该过程正在进行时，共识小组通常更容易受到攻击。</p>
</li>
<li><p><strong>Replay Protection</strong>: commands proposed by a client may be executed multiple times (or never, depending on the implementation). While one client proposal ideally leads to exactly one executed command in almost all cases, <a href="http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/">general exactly-once delivery is impossible in a distributed system</a>. In practice, this means keeping state about already executed commands, or even better, using only idempotent commands.</p>
<p>重播保护：客户端提出的命令可能会执行多次（或永远不会执行，具体取决于实现）。 虽然在几乎所有情况下，理想情况下，一个客户端提案都会导致恰好执行一个命令，但在分布式系统中，一般的一次性交付是不可能的。 实际上，这意味着保留已执行命令的状态，或者更好的是，仅使用幂等命令。</p>
</li>
<li><p><strong>Read Leases</strong>: when using a vanilla consensus protocol, all read operations of the replicated state have to go through that consensus protocol, or they may read stale data[1], which is a consistency violation. In many applications, the vast majority of operations <em>are</em> reads, and going through consensus for those can be prohibitively expensive.</p>
<p>读租约：当使用普通共识协议时，复制状态的所有读取操作都必须经过该共识协议，否则它们可能会读取过时的数据[1]，这是违反一致性的。 在许多应用程序中，绝大多数操作都是读取，而对这些操作达成共识可能会非常昂贵。</p>
</li>
</ul>
<p>These and many others (which aren’t as readily summarized) make it hard to work an instance of a consensus protocol into a real application, let alone <a href="https://www.cockroachlabs.com/blog/scaling-raft/">thousands of them</a>.</p>
<p>这些和许多其他问题（不容易概括）使得将共识协议实例应用到实际应用程序中变得困难，更不用说数千个应用程序了。</p>
<h2 id="“Raft-Made-Live”"><a href="#“Raft-Made-Live”" class="headerlink" title="“Raft Made Live”"></a>“Raft Made Live”</h2><p>At CockroachDB, we have most of the above concerns sufficiently covered. The author of <a href="https://raft.github.io/">Raft</a>, our consensus protocol of choice, did a pretty good job at providing comprehensive instructions for much of the above. We truncate our log appropriately, regardless of whether all replicas <a href="https://github.com/cockroachdb/cockroach/pull/7438">are up or not</a>. We send snapshots when appropriate, and soon we will also send <a href="https://github.com/cockroachdb/cockroach/pull/7468">pre-emptive snapshots</a> during membership changes. We implement replay protection using <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a> and a <a href="https://github.com/cockroachdb/cockroach/pull/6961">consensus-level component</a>. And, last but not least, we have a stable leading replica which gets to serve reads locally.</p>
<p>在 CockroachDB，我们已经充分解决了上述大部分问题。 我们选择的共识协议 Raft 的作者在为上述大部分内容提供全面的说明方面做得非常好。 无论所有副本是否已启动，我们都会适当地截断日志。 我们会在适当的时候发送快照，很快我们还将在成员资格变更期间发送抢先快照。 我们使用 MVCC 和共识级组件实现重放保护。 最后但并非最不重要的一点是，我们有一个稳定的领先副本，可以在本地提供读取服务。</p>
<p>That’s all fine and well, but there are various areas of improvement. Let’s leave behind the realm of what’s been implemented (at least in CockroachDB, and probably almost everywhere else) and talk about <strong>what should be possible in an ideal world.</strong></p>
<p>这一切都很好，但还有很多需要改进的地方。 让我们抛开已经实现的领域（至少在 CockroachDB 中，可能几乎在其他地方），来讨论一下理想世界中应该实现什么。</p>
<h2 id="Consensus-is-like-caviar-too-expensive-to-splurge-on"><a href="#Consensus-is-like-caviar-too-expensive-to-splurge-on" class="headerlink" title="Consensus is like caviar: too expensive to splurge on"></a>Consensus is like caviar: too expensive to splurge on</h2><p>The most obvious problem with distributed consensus is that it’s inherently slow. A typical consensus operation goes as follows:</p>
<p>分布式共识最明显的问题是它本质上很慢。 典型的共识操作如下：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">               CLIENT</span><br><span class="line">                 |  ʌ</span><br><span class="line">             (1) |  | (4)</span><br><span class="line">                 v  |</span><br><span class="line">                LEADER</span><br><span class="line">            [node1, Oregon]</span><br><span class="line">              /  ʌ    \</span><br><span class="line">       (2)  /  / (3)    \ (5)</span><br><span class="line">          /  /            \</span><br><span class="line">        v  /                v</span><br><span class="line">     FOLLOWER            FOLLOWER</span><br><span class="line">[node2, California]  [node3, Virginia]</span><br><span class="line">                      (responds later)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>A client sends a request to the leader. In turn, the leader must talk to a majority of nodes (including itself), i.e. in the picture it would have to wait for one of the followers (for simplicity we assume that node three is always last). In simple math, assuming that actual message processing takes no time, we get</p>
<p>客户端向领导者发送请求。 反过来，领导者必须与大多数节点（包括其自身）通信，即在图中，它必须等待追随者之一（为简单起见，我们假设节点 3 始终是最后一个）。 在简单的数学中，假设实际的消息处理不需要时间，我们得到</p>
<blockquote>
<p>commit_latency &#x3D; round_trip(client, leader) + round_trip(leader, follower)</p>
</blockquote>
<p>This internal coordination is expensive, and while it’s unavoidable, we can see that the price tag depends heavily on the location of the client. For example, with a client in Oregon, we have <em>roughly</em> zero latency from the client to the leader, and ~30ms round-trip between the leader and the follower in Virginia, for a total commit latency of about 30ms. That doesn’t sound so bad, but let’s look at a client on the east coast instead – it would presumably be close to our Virginia data center, but that doesn’t matter – the leader is in Oregon, and we pay perhaps an 80ms round trip to it, plus the same 30ms as before, adding up to a hefty 110ms.</p>
<p>这种内部协调成本高昂，虽然这是不可避免的，但我们可以看到价格很大程度上取决于客户的位置。 例如，对于俄勒冈州的客户端，从客户端到领导者的延迟大致为零，而弗吉尼亚州的领导者和追随者之间的往返时间约为 30 毫秒，总提交延迟约为 30 毫秒。 这听起来并没有那么糟糕，但让我们看看东海岸的一个客户 - 它可能靠近我们的弗吉尼亚数据中心，但这并不重要 - 领先者在俄勒冈州，我们可能支付 80 毫秒 往返，再加上与之前相同的 30 毫秒，加起来高达 110 毫秒。</p>
<p>This goes to show that once you have consensus, you will do all you can to reduce the time you wait for those transcontinental (or even transmundial) TCP packets. For example, you could ask yourself why in that last example the client couldn’t talk directly to the node in Virginia.</p>
<p>这表明，一旦达成共识，您将尽一切努力减少等待那些跨大陆（甚至跨世界）的 TCP 数据包的时间。 例如，您可以问自己为什么在最后一个示例中客户端无法直接与弗吉尼亚州的节点通信。</p>
<p>There is a relatively recent consensus protocol called EPaxos which allows this[2], though we’ll save it for another blog post. Today, we’re going to deal with a more modest question:</p>
<p>有一个相对较新的共识协议，称为 EPaxos，它允许这样做[2]，尽管我们将把它保存到另一篇博客文章中。 今天，我们要解决一个更温和的问题：</p>
<h2 id="Can-we-make-reads-cheaper"><a href="#Can-we-make-reads-cheaper" class="headerlink" title="Can we make reads cheaper?"></a>Can we make reads cheaper?</h2><p>Read operations may seem innocuous at first. They get served from the leader because that replica is the only one that can guarantee that it’s not reading stale data (since it decides when write operations commit), but read operations don’t have to go through consensus themselves. This means that for our example, we shave 30ms of the commit latency if we only read data. However, <strong>reads are still expensive when you’re far away</strong> from the leader. It seems silly that the client in Virginia can’t read from its local node; sure would be nice to do better, right? And you can! (At least in the literature.)</p>
<p>读取操作乍一看似乎无害。 它们从领导者处获得服务，因为该副本是唯一可以保证它不会读取过时数据的副本（因为它决定何时提交写入操作），但读取操作本身不必经过共识。 这意味着，对于我们的示例，如果仅读取数据，则可以减少 30 毫秒的提交延迟。 然而，当你远离领导者时，读取仍然很昂贵。 弗吉尼亚州的客户端无法从其本地节点读取数据，这似乎很愚蠢； 如果能做得更好当然会很好，对吧？ 你可以！ （至少在文献中是这样。）</p>
<p>The idea is simple:</p>
<blockquote>
<p><strong>By letting the consensus group agree that commands must be committed by a *special majority* of the nodes as opposed to any majority, the nodes in that special majority can be sure to be informed about the latest state of the system.</strong></p>
<p>通过让共识组同意命令必须由特殊多数节点而不是任何多数节点提交，确保该特殊多数节点能够了解系统的最新状态。</p>
</blockquote>
<p>For example, in the above picture, we could set things up so that all nodes agree that nodes one and two are to be included in any majority when committing commands (regardless of who the leader is), and then these replicas could serve reads which are consistent with the consensus log. The resulting algorithm is investigated (in higher generality) in <a href="https://www.cs.cmu.edu/~dga/papers/leases-socc2014.pdf">Paxos Quorum Leases: Fast Reads Without Sacrificing Writes</a>.</p>
<p>例如，在上图中，我们可以进行设置，以便所有节点都同意在提交命令时节点一和节点二将包含在任何多数中（无论领导者是谁），然后这些副本可以提供读取服务 与共识日志一致。 在 Paxos Quorum Leases: Fast Reads Without Sacrificing Writes 中研究了生成的算法（具有更高的通用性）。</p>
<p>In a simpler world, modulo the usual implementation headaches, that could be the end of the story – but there’s an additional bit of complexity hidden here: the fact that CockroachDB is not a simple replicated log, but a full-fledged <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a> database. This means that the key-value pairs we store have a logical timestamp attached to them, and the one invariant that we must uphold is the following:</p>
<p>在一个更简单的世界中，对常见的实现难题进行取模，这可能是故事的结局 - 但这里还隐藏着额外的复杂性：事实上，CockroachDB 不是一个简单的复制日志，而是一个成熟的 MVCC 数据库。 这意味着我们存储的键值对附加了一个逻辑时间戳，并且我们必须遵守的一个不变量如下：</p>
<blockquote>
<p><strong>If a value was ever read at some timestamp, it can only be mutated at higher timestamps.</strong></p>
<p>如果某个值在某个时间戳被读取过，那么它只能在更高的时间戳发生变化。</p>
</blockquote>
<p>That makes perfect sense if you think about it – if you read a certain value at some timestamp, then I should not be able to perform a write that changes the value you already observed.</p>
<p>如果你仔细想想，这是完全有道理的——如果你在某个时间戳读取某个值，那么我应该无法执行更改你已经观察到的值的写入操作。</p>
<p>On the leader, this is achieved by keeping an in-memory <em>timestamp cache</em> – a data structure which given a key and a timestamp will tell you whether the key was read at a higher timestamp previously. This structure must be consulted before proposing a write to consensus to guard us against the scenario described above – if there was such a read, we can’t perform the write.</p>
<p>在领导者上，这是通过保留内存中时间戳缓存来实现的 - 给定密钥和时间戳的数据结构将告诉您之前是否以更高的时间戳读取了该密钥。 在提出写入共识之前必须查阅此结构，以防止出现上述情况——如果存在这样的读取，我们将无法执行写入。</p>
<p>If local reads were served at another replica, naively the leader would have to be notified about that synchronously (in order to write to the timestamp cache) before returning the result of the read to the client – the very thing we wanted to avoid! Or, somewhat better, we could let reads remain cheap for the most part and shift complexity onto writes, requiring them to contact the special majority <em>before</em> proposing to confirm that writing at this timestamp is still possible, and prompting the special majority to not serve reads with conflicting timestamps (at least until they see our command pop out of the consensus protocol).</p>
<p>如果在另一个副本上提供本地读取，则在将读取结果返回给客户端之前，必须天真地同步通知领导者（以便写入时间戳缓存）——这正是我们想要避免的事情！ 或者，更好的是，我们可以让读取在大多数情况下保持便宜，并将复杂性转移到写入上，要求他们在提议确认在此时间戳写入仍然可能之前联系特殊多数，并提示特殊多数不提供读取服务 时间戳冲突（至少在他们看到我们的命令从共识协议中弹出之前）。</p>
<p>Another (much more complicated) option is to incorporate that feature at the consensus level by allowing replicas to <em>reject</em> commands before the commit. In that scenario, roughly the following would occur:</p>
<p>另一个（更复杂的）选项是通过允许副本在提交之前拒绝命令，在共识级别合并该功能。 在这种情况下，大致会发生以下情况：</p>
<ol>
<li><p>Follower 1 serves a local read at timestamp, say, <code>ts=15</code>.</p>
<p>Follower 1 在时间戳处提供本地读取，例如 ts&#x3D;15。</p>
</li>
<li><p>A client asks the leader to write that same key at timestamp <code>ts=9</code>.</p>
<p>客户端要求领导者在时间戳 ts&#x3D;9 处写入相同的密钥。</p>
</li>
<li><p>The leader proposes a corresponding command to consensus.</p>
<p>领导者提出相应的命令以达成共识。</p>
</li>
<li><p>The consensus algorithm on the leader tries to replicate this command to a majority of followers (including the <em>special majority</em>).</p>
<p>领导者的共识算法尝试将此命令复制给大多数追随者（包括特殊多数）。</p>
</li>
<li><p>Each follower checks the command for timestamp cache violations. Some followers may acknowledge the proposal, but on Follower 1, it is rejected due to already having served a read for <code>ts=15</code> prior to the write at <code>ts=9</code>.</p>
<p>每个关注者都会检查命令是否有时间戳缓存违规。 一些追随者可能会认可该提案，但在追随者 1 上，该提案被拒绝，因为在 ts&#x3D;9 写入之前已经提供了 ts&#x3D;15 的读取服务。</p>
</li>
<li><p>The leader, upon receiving the rejection, informs the client and cancels replication of the command suitably (either by turning it into a no-op or by unregistering it completely, depending on what’s possible).</p>
<p>领导者在收到拒绝后，通知客户端并适当取消命令的复制（通过将其变为无操作或完全取消注册，具体取决于可能的情况）。</p>
</li>
</ol>
<p>To the best of my knowledge, such an addition has not been considered for any consensus protocol (and in particular not for the one of most interest to us, Raft). Allowing individual replicas to reject certain commands ad-hoc (i.e. basing their decision on auxiliary unreplicated state) must be considered very carefully and adds considerable complexity (in particular when leadership changes as these commands are in flight).</p>
<p>据我所知，任何共识协议都没有考虑过这样的添加（特别是我们最感兴趣的协议之一，Raft）。 允许各个副本临时拒绝某些命令（即，根据辅助未复制状态做出决定）必须非常仔细地考虑，并且会增加相当大的复杂性（特别是当这些命令正在运行时领导层发生变化时）。</p>
<p>Performing that work is likely a small research paper and a bunch of implementation, but in contrast to many other more complicated endeavor, it seems within reach (and with it, serving local reads from some replicas).</p>
<p>执行这项工作可能只是一篇小型研究论文和一堆实现，但与许多其他更复杂的工作相比，它似乎触手可及（并且可以从某些副本提供本地读取）。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Our usage of consensus algorithms in CockroachDB is fairly standard and covers all the basic needs – but taking a step up is something we’ll be working on in the future. While the likely next step is serving reads from (some) followers, techniques which save round-trips on writes are also appealing, but those go extremely deep down the rabbit hole (and have new, much deeper challenges with respect to serving local reads). As usual, distributed consensus is hard. And if that’s just your cup of tea, <a href="https://cockroa.ch/eng_hiring">you could have that tea every day</a>.</p>
<p>我们在 CockroachDB 中对共识算法的使用相当标准，涵盖了所有基本需求 - 但我们未来将致力于更进一步。 虽然下一步可能是为（某些）关注者提供读取服务，但节省写入往返次数的技术也很有吸引力，但这些技术非常深入兔子洞（并且在服务本地读取方面存在新的、更深层次的挑战） 。 像往常一样，分布式共识很难。 如果这只是你喜欢的茶，那么你可以每天喝这种茶。</p>
<p>[1] Even if the client attempts to talk to the master node, the node it talks to may not be the actual master (though it may think it is), and so commands which have already committed and influenced the outcome of our read may not yet have been executed on the node we’re reading from yet – this violates linearizability on a single register.</p>
<p>即使客户端尝试与主节点通信，它所通信的节点也可能不是实际的主节点（尽管它可能认为是），因此已经提交并影响我们读取结果的命令可能会 尚未在我们正在读取的节点上执行 - 这违反了单个寄存器的线性化能力。</p>
<p>[2] Egalitarian Paxos (<a href="https://www.cs.cmu.edu/~dga/papers/epaxos-sosp2013.pdf">There Is More Consensus In Egalitarian Parliaments</a>)</p>
<p>平等主义Paxos（平等主义议会有更多共识）</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>cockroach</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling Raft</title>
    <url>/2024/01/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/07-Scaling%20Raft/</url>
    <content><![CDATA[<p><a href="https://www.cockroachlabs.com/blog/scaling-raft/">https://www.cockroachlabs.com/blog/scaling-raft/</a></p>
<h1 id="Scaling-Raft"><a href="#Scaling-Raft" class="headerlink" title="Scaling Raft"></a>Scaling Raft</h1><p>In <a href="https://github.com/cockroachdb/cockroach">CockroachDB</a>, we use the <a href="https://raftconsensus.github.io/">Raft consensus algorithm</a> to ensure that your data remains consistent even when machines fail. In most systems that use Raft, such as <a href="https://github.com/coreos/etcd">etcd</a> and <a href="https://www.consul.io/">Consul</a>, the entire system is one Raft consensus group. In CockroachDB, however, the data is divided into <em>ranges</em>, each with its own consensus group. This means that each node may be participating in hundreds of thousands of consensus groups. This presents some unique challenges, which we have addressed by introducing a layer on top of Raft that we call <a href="https://github.com/cockroachdb/cockroach/blob/8187c2551352a6c28eba021effaebcbfe523d78c/docs/RFCS/20151213_dismantle_multiraft.md">MultiRaft</a>.</p>
<p>在 CockroachDB 中，我们使用 Raft 共识算法来确保即使机器出现故障，您的数据也保持一致。 在大多数使用 Raft 的系统中，例如 etcd、Consul，整个系统就是一个 Raft 共识组。 然而，在 CockroachDB 中，数据被分为多个范围，每个范围都有自己的共识组。 这意味着每个节点可能参与数十万个共识组。 这带来了一些独特的挑战，我们通过在 Raft 之上引入一个称为 MultiRaft 的层来解决这些挑战。</p>
<span id="more"></span>

<p>With a single range, one node (out of three or five) is elected leader, and it periodically sends heartbeat messages to the followers.</p>
<p>在单一范围内，一个节点（三到五个）被选为领导者，并定期向追随者发送心跳消息。</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/04/multinode1-300x216.png?auto=format,compress&max-w=700"></p>
<p>As the system grows to include more ranges, so does the amount of traffic required to handle heartbeats.</p>
<p>随着系统发展到包含更多范围，处理心跳所需的流量也随之增加。</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/04/multinode2-300x216.png?auto=format,compress&max-w=700"></p>
<p>The number of ranges is much larger than the number of nodes (keeping the ranges small helps improve recovery time when a node fails), so many ranges will have overlapping membership. This is where MultiRaft comes in: instead of allowing each range to run Raft independently, we manage an entire node’s worth of ranges as a group. Each pair of nodes only needs to exchange heartbeats once per tick, no matter how many ranges they have in common.</p>
<p>范围的数量远大于节点的数量（保持较小的范围有助于提高节点故障时的恢复时间），因此许多范围将具有重叠的成员资格。 这就是 MultiRaft 的用武之地：我们不再允许每个范围独立运行 Raft，而是将整个节点的范围作为一个组进行管理。 每对节点每个时钟周期只需要交换一次心跳，无论它们有多少个共同范围。</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/04/multinode3-300x212.png?auto=format,compress&max-w=700"></p>
<p>In addition to reducing heartbeat network traffic, MultiRaft can improve efficiency in other areas. For example, MultiRaft only needs a small, constant number of goroutines (currently 3) instead of one goroutine per range.</p>
<p>除了减少心跳网络流量之外，MultiRaft 还可以提高其他方面的效率。 例如，MultiRaft 仅需要少量、恒定数量的 goroutine（目前为 3 个），而不是每个范围一个 goroutine。</p>
<p>Implementing and testing a consensus algorithm is a daunting task, so we are pleased to be working closely with the etcd team from CoreOS instead of starting from scratch. The <a href="https://github.com/coreos/etcd/tree/master/raft">raft implementation in etcd</a> is built around clean abstractions that we found easy to adapt to our rather unusual requirements, and we have been able to contribute improvements back to etcd and the community.</p>
<p>实现和测试共识算法是一项艰巨的任务，因此我们很高兴与 CoreOS 的 etcd 团队密切合作，而不是从头开始。 etcd 中的 raft 实现是围绕干净的抽象构建的，我们发现这些抽象很容易适应我们相当不寻常的需求，并且我们已经能够为 etcd 和社区做出改进。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>How we built a serverless SQL database</title>
    <url>/2024/01/01/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/05-How%20we%20built%20a%20serverless%20SQL%20database/</url>
    <content><![CDATA[<p>原文 <a href="https://www.cockroachlabs.com/blog/how-we-built-cockroachdb-serverless/">https://www.cockroachlabs.com/blog/how-we-built-cockroachdb-serverless/</a></p>
<p>We recently announced <a href="https://www.cockroachlabs.com/blog/announcing-cockroachdb-serverless/">general availability (GA) for Serverless</a>, with support for change data capture (CDC), backup and restore, and a 99.99% uptime SLA. Read on to learn how CockroachDB Serverless works from the inside out, and why we can give it away for free – not free for some limited period, but free. It required some significant and fascinating engineering to get us there. I think you’ll enjoy reading about it in this blog or watching the recent presentation I gave with my colleague Emily Horing:</p>
<p>我们最近宣布了无服务器的全面可用性 (GA)，支持变更数据捕获 (CDC)、备份和恢复以及 99.99% 的正常运行时间 SLA。 请继续阅读，了解 CockroachDB Serverless 从内到外的工作原理，以及为什么我们可以免费赠送它——不是在有限的时间内免费，而是免费。 我们需要一些重要且令人着迷的工程才能实现这一目标。 我想您会喜欢在这个博客中阅读相关内容或观看我最近与同事 Emily Horing 进行的演示：</p>
<span id="more"></span>

<h2 id="What-is-CockroachDB-Serverless"><a href="#What-is-CockroachDB-Serverless" class="headerlink" title="What is CockroachDB Serverless?"></a>What is CockroachDB Serverless?</h2><p>If you’ve created a database before, you probably had to estimate the size and number of servers to use based on the expected traffic. If you guessed too low, your database would fall over under load and cause an outage. If you guessed too high or if your traffic came in bursts, you’d waste money on servers that are just sitting idle. Could there be a better way?</p>
<p>如果您之前创建过数据库，则可能必须根据预期流量估计要使用的服务器的大小和数量。 如果您猜测得太低，您的数据库将在负载下崩溃并导致中断。 如果您猜测过高或者流量突然增加，您就会在闲置的服务器上浪费金钱。 还能有更好的办法吗？</p>
<p>Serverless means you don’t have to think about servers. Of course there are servers hard at work handling your application’s requests, but that’s our problem, not yours. We do all the hard work behind the scenes of allocating, configuring, and maintaining the servers. Instead of paying for servers, you pay for the requests that your application makes to the database and the storage that your data consumes.</p>
<p>无服务器意味着您不必考虑服务器。 当然，有些服务器正在努力处理您的应用程序的请求，但这是我们的问题，而不是您的问题。 我们在幕后完成分配、配置和维护服务器的所有艰苦工作。 您无需为服务器付费，而是为应用程序向数据库发出的请求以及数据消耗的存储付费。</p>
<p>You pay only for what you actually use, without needing to figure out up-front what that might be. If you use more, then we’ll automatically allocate more hardware to handle the increased load. If you use less, then you’ll pay less, or even nothing at all. And you’ll never be surprised by a bill, because you can set a guaranteed monthly resource limit. We’ll alert you as you approach that limit and give you options for how to respond.</p>
<p>您只需为实际使用的内容付费，无需预先弄清楚可能是什么。 如果您使用更多，那么我们将自动分配更多硬件来处理增加的负载。 如果你使用得少，那么你支付的费用就会少一些，甚至根本不需要支付任何费用。 而且您永远不会对账单感到惊讶，因为您可以设置有保证的每月资源限制。 当您接近该限制时，我们会提醒您，并为您提供如何应对的选项。</p>
<p>With just a few clicks or an API call, you can create a fully-featured CockroachDB database in seconds. You get an “always on” database that survives data center failure and keeps multiple encrypted copies of your data so you don’t lose it to hackers or hardware failure. It automatically and transparently scales to meet your needs, no matter how big or small, with no changes to your application. It supports online schema migrations, Postgres compatibility, and gives you unrestricted access to <a href="https://www.cockroachlabs.com/docs/stable/enterprise-licensing">Enterprise features</a>.</p>
<p>只需点击几下或 API 调用，您就可以在几秒钟内创建功能齐全的 CockroachDB 数据库。 您将获得一个“始终在线”的数据库，该数据库可以在数据中心发生故障时幸存下来，并保留数据的多个加密副本，这样您就不会因黑客或硬件故障而丢失数据。 它会自动、透明地扩展以满足您的需求，无论大小，而无需更改您的应用程序。 它支持在线架构迁移、Postgres 兼容性，并让您可以不受限制地访问企业功能。</p>
<p>Oh, and please use your favorite language, SDK, or tooling in whatever application environment you choose; using CockroachDB Serverless does not mean you have to use a Serverless compute service like AWS Lambda or Google Cloud Functions (though, <a href="https://www.cockroachlabs.com/docs/stable/serverless-function-best-practices">that’s a great tool</a> too!).</p>
<p>哦，请在您选择的任何应用程序环境中使用您最喜欢的语言、SDK 或工具； 使用 CockroachDB Serverless 并不意味着您必须使用 AWS Lambda 或 Google Cloud Functions 等无服务器计算服务（不过，这也是一个很棒的工具！）。</p>
<p>How can we afford to give this away? Well, certainly we’re hoping that some of you will build successful apps that “go big” and you’ll become paying customers. But beyond that, we’ve created an innovative Serverless architecture that allows us to securely host thousands of virtualized CockroachDB database clusters on a single underlying physical CockroachDB database cluster. This means that a tiny database with a few kilobytes of storage and a handful of requests costs us almost nothing to run, because it’s running on just a small slice of the physical hardware. I’ll explain how all this works in more detail below, but here’s a diagram to get you thinking:</p>
<p>我们怎么能承担得起放弃这个呢？ 好吧，我们当然希望你们中的一些人能够构建成功的应用程序，“变大”，并且你们将成为付费客户。 但除此之外，我们还创建了一个创新的无服务器架构，使我们能够在单个底层物理 CockroachDB 数据库集群上安全地托管数千个虚拟化 CockroachDB 数据库集群。 这意味着一个具有几千字节存储空间和少量请求的小型数据库几乎不需要我们运行任何成本，因为它只在一小部分物理硬件上运行。 我将在下面更详细地解释这一切是如何工作的，但这里有一个图表可以让您思考：</p>
<p><img src="https://crl2020.imgix.net/img/serverless-white-paper-illustrationspg2-02-1-.png?auto=format,compress&max-w=640"></p>
<h2 id="Single-Tenant-Architecture"><a href="#Single-Tenant-Architecture" class="headerlink" title="Single-Tenant Architecture"></a>Single-Tenant Architecture</h2><p>Before now, a single physical CockroachDB cluster was intended for dedicated use by a single user or organization. That is called single-tenancy. Over the past several CockroachDB releases, we’ve quietly been adding multi-tenancy support, which enables the physical CockroachDB cluster to be shared by multiple users or organizations (called “tenants”). Each tenant gets its own virtualized CockroachDB cluster that is hosted on the physical CockroachDB cluster and yet is secure and isolated from other tenants’ clusters. You’re probably familiar with how virtual machines (VMs) work, right? It’s kind of like that, only for database clusters.</p>
<p>在此之前，单个物理 CockroachDB 集群旨在供单个用户或组织专用。 这就是所谓的单租户。 在过去的几个 CockroachDB 版本中，我们一直在悄悄添加多租户支持，这使得物理 CockroachDB 集群能够由多个用户或组织（称为“租户”）共享。 每个租户都有自己的虚拟化 CockroachDB 集群，该集群托管在物理 CockroachDB 集群上，但安全且与其他租户的集群隔离。 您可能熟悉虚拟机 (VM) 的工作原理，对吧？ 有点像那样，仅适用于数据库集群。</p>
<p>Before I can meaningfully explain how multi-tenancy works, I need to review the <a href="https://www.cockroachlabs.com/docs/stable/architecture/overview">single-tenant architecture</a>. To start with, a single-tenant CockroachDB cluster consists of an arbitrary number of nodes. Each node is used for both data storage and computation, and is typically hosted on its own machine. Within a single node, CockroachDB has a layered architecture. At the highest level is the SQL layer, which parses, optimizes, and executes SQL statements. It does this by a <a href="https://www.cockroachlabs.com/blog/distributed-sql-key-value-store/">clever translation of higher-level SQL statements to simple read and write requests</a> that are sent to the underlying key-value (KV) layer.</p>
<p>在我能够有意义地解释多租户如何工作之前，我需要回顾一下单租户架构。 首先，单租户 CockroachDB 集群由任意数量的节点组成。 每个节点都用于数据存储和计算，并且通常托管在自己的机器上。 在单个节点内，CockroachDB 具有分层架构。 最高层是SQL层，它解析、优化和执行SQL语句。 它通过巧妙地将高级 SQL 语句转换为发送到底层键值 (KV) 层的简单读写请求来实现此目的。</p>
<p>The KV layer maintains a transactional, distributed, replicated key-value store. That’s a mouthful, so let me break it down. Each key is a unique string that maps to an arbitrary value, like in a dictionary. KV stores these key-value pairs in sorted order for fast lookup. Multiple key-value pairs are also grouped into ranges. Each range contains a contiguous, non-overlapping portion of the total key-value pairs, sorted by key. Ranges are distributed across the available nodes and are also replicated at least three times, for high-availability. Key-value pairs can be added, removed, and updated in all-or-nothing transactions. Here is a simplified example of how a higher-level SQL statement gets translated into a simple KV GET call:</p>
<p>KV 层维护一个事务性、分布式、复制的键值存储。 这实在是太拗口了，所以让我来分解一下。 每个键都是一个唯一的字符串，映射到任意值，就像在字典中一样。 KV 按排序顺序存储这些键值对，以便快速查找。 多个键值对也被分组到范围中。 每个范围包含总键值对的连续、不重叠的部分，按键排序。 范围分布在可用节点上，并且至少复制三次，以实现高可用性。 可以在全有或全无事务中添加、删除和更新键值对。 下面是一个高级 SQL 语句如何转换为简单的 KV GET 调用的简化示例：</p>
<p><img src="https://crl2020.imgix.net/img/serverless-white-paper-illustrations-03-1.png?auto=format,compress&max-w=640"></p>
<p>In single-tenant CockroachDB, the SQL layer is co-located with the KV layer on each node and in the same process. While the SQL layer always calls into the KV instance that runs on the same node, KV will often “fan-out” additional calls to other instances of KV running on other nodes. This is because the data needed by SQL is often located in ranges that are scattered across nodes in the cluster.</p>
<p>在单租户 CockroachDB 中，SQL 层与 KV 层位于每个节点上且位于同一进程中。 虽然 SQL 层总是调用在同一节点上运行的 KV 实例，但 KV 通常会“扇出”对在其他节点上运行的其他 KV 实例的额外调用。 这是因为 SQL 所需的数据通常位于分散在集群中节点的范围内。</p>
<h3 id="Multi-Tenant-Architecture"><a href="#Multi-Tenant-Architecture" class="headerlink" title="Multi-Tenant Architecture"></a>Multi-Tenant Architecture</h3><p>How do we extend that single-tenant architecture to support multiple tenants? Each tenant should feel like they have their own dedicated CockroachDB cluster, and should be isolated from other tenants in terms of performance and security. But that’s very difficult to achieve if we attempt to share the SQL layer across tenants. One tenant’s runaway SQL query could easily disrupt the performance of other tenants in the same process. In addition, sharing the same process would introduce many cross-tenant security threats that are difficult to reliably mitigate.</p>
<p>我们如何扩展单租户架构以支持多个租户？ 每个租户都应该感觉自己拥有自己专用的 CockroachDB 集群，并且应该在性能和安全性方面与其他租户隔离。 但如果我们尝试跨租户共享 SQL 层，则很难实现这一点。 一个租户失控的 SQL 查询很容易破坏同一进程中其他租户的性能。 此外，共享同一进程会引入许多难以可靠缓解的跨租户安全威胁。</p>
<p>One possible solution to these problems would be to give each tenant its own set of isolated processes that run both the SQL and KV layers. However, that creates a different problem: we would be unable to share the key-value store across tenants. That eliminates one of the major benefits of a multi-tenant architecture: the ability to efficiently pack together the data of many tiny tenants in a shared storage layer.</p>
<p>解决这些问题的一个可能的解决方案是为每个租户提供一组自己的独立进程，这些进程同时运行 SQL 层和 KV 层。 然而，这会产生一个不同的问题：我们无法在租户之间共享键值存储。 这消除了多租户架构的主要优势之一：能够在共享存储层中有效地将许多小型租户的数据打包在一起。</p>
<p>After mulling over this problem, we realized that the dilemma can be elegantly solved by isolating some components and sharing other components. Given that the SQL layer is so difficult to share, we decided to isolate that in per-tenant processes, along with the <a href="https://www.cockroachlabs.com/docs/stable/architecture/transaction-layer">transactional</a> and <a href="https://www.cockroachlabs.com/docs/stable/architecture/distribution-layer">distribution</a> components from the KV layer. Meanwhile, the KV replication and storage components continue to run on storage nodes that are shared across all tenants. By making this separation, we get “the best of both worlds” – the security and isolation of per-tenant SQL processes and the efficiency of shared storage nodes. Here is an updated diagram showing two isolated per-tenant SQL nodes interacting with a shared storage layer:</p>
<p>经过思考这个问题，我们意识到可以通过隔离一些组件并共享其他组件来优雅地解决这个困境。 鉴于 SQL 层很难共享，我们决定将其与 KV 层的事务和分发组件一起隔离在每个租户进程中。 同时，KV 复制和存储组件继续在所有租户共享的存储节点上运行。 通过这种分离，我们获得了“两全其美”——每租户 SQL 进程的安全性和隔离性以及共享存储节点的效率。 下面是更新后的图表，显示了两个独立的每租户 SQL 节点与共享存储层交互：</p>
<p><img src="https://crl2020.imgix.net/img/serverless-white-paper-illustrations-04-1.png?auto=format,compress&max-w=640"></p>
<p>The storage nodes no longer run tenant SQL queries, but they still leverage the sophisticated infrastructure that powers single-tenant CockroachDB. Node failures are detected and repaired without impacting data availability. <a href="https://www.cockroachlabs.com/docs/stable/architecture/replication-layer#leases">Leaseholders</a>, which serve reads and coordinate writes for each range, move according to activity. Busy ranges are automatically split; quiet ranges are merged. Ranges are rebalanced across nodes based on load. The storage layer caches hot ranges in memory and pushes cold ones to disk. Three-way replication across availability zones ensures that your data is safely stored and highly available.</p>
<p>存储节点不再运行租户 SQL 查询，但它们仍然利用为单租户 CockroachDB 提供支持的复杂基础设施。 检测和修复节点故障不会影响数据可用性。 租用者负责为每个范围提供读取和协调写入服务，并根据活动进行移动。 繁忙范围自动分割； 安静范围被合并。 根据负载在节点之间重新平衡范围。 存储层将热范围缓存在内存中，并将冷范围推送到磁盘。 跨可用区的三向复制可确保您的数据安全存储且高度可用。</p>
<p>After seeing this architecture, you might be wondering about the security of the shared storage nodes. We spent significant time designing and implementing strong security measures to protect tenant data. Each tenant receives an isolated, protected portion of the KV keyspace. This is accomplished by prefixing <a href="https://www.cockroachlabs.com/blog/distributed-sql-key-value-store/">every key generated by the SQL layer</a> with the tenant’s unique identifier. Rather than generating a key like <code>/&lt;table-id&gt;/&lt;index-id&gt;/&lt;key&gt;</code>, SQL will generate a key like <code>/&lt;tenant-id&gt;/&lt;table-id&gt;/&lt;index-id&gt;/&lt;key&gt;</code>. This means that key-value pairs generated by different tenants are isolated in their own ranges. Furthermore, the storage nodes authenticate all communication from the SQL nodes and ensure that each tenant can only touch keys that are prefixed by its own tenant identifier.</p>
<p>看到这个架构后，您可能想知道共享存储节点的安全性。 我们花费了大量时间设计和实施强大的安全措施来保护租户数据。 每个租户都会收到 KV 键空间的一个隔离的、受保护的部分。 这是通过为 SQL 层生成的每个键添加租户的唯一标识符作为前缀来实现的。 SQL 不会生成像 &#x2F;<table-id>&#x2F;<index-id>&#x2F;<key> 这样的键，而是会生成像 &#x2F;<tenant-id>&#x2F;<table-id>&#x2F;<index-id>&#x2F;&lt;key 这样的键 &gt;。 这意味着不同租户生成的键值对被隔离在自己的范围内。 此外，存储节点对来自 SQL 节点的所有通信进行身份验证，并确保每个租户只能触摸以其自己的租户标识符为前缀的密钥。</p>
<p>Besides security, we were also concerned about ensuring basic quality of service across tenants. What happens if KV calls from multiple tenants threaten to overload a storage node? In that case, <a href="https://www.cockroachlabs.com/blog/admission-control-in-cockroachdb/">CockroachDB admission control</a> kicks in. The admission control system integrates with the Go scheduler and maintains queues of work that ensure fairness across tenants. Each tenant’s GET, PUT, and DELETE requests are given a roughly equal allocation of CPU time and storage I&#x2F;O. This ensures that a single tenant cannot monopolize resources on a storage node.</p>
<p>除了安全之外，我们还担心确保租户的基本服务质量。 如果来自多个租户的 KV 调用可能导致存储节点过载，会发生什么情况？ 在这种情况下，CockroachDB 准入控制就会发挥作用。准入控制系统与 Go 调度程序集成并维护工作队列，以确保租户之间的公平性。 每个租户的 GET、PUT 和 DELETE 请求都会获得大致相等的 CPU 时间和存储 I&#x2F;O 分配。 这样可以保证单个租户不能独占存储节点上的资源。</p>
<h2 id="Serverless-Architecture"><a href="#Serverless-Architecture" class="headerlink" title="Serverless Architecture"></a>Serverless Architecture</h2><p>Wait…wasn’t the last section about the Serverless architecture? Well, yes and no. As discussed, we’ve made significant upgrades to the core database architecture to support multi-tenancy. But that’s only half of the story. We also needed to make big enhancements in how we deploy and operate multi-tenant CockroachDB clusters in order to make Serverless possible.</p>
<p>等等……最后一节不是关于 Serverless 架构的吗？ 嗯，是的，也不是。 正如所讨论的，我们对核心数据库架构进行了重大升级以支持多租户。 但这只是故事的一半。 我们还需要对多租户 CockroachDB 集群的部署和操作方式进行重大改进，以使无服务器成为可能。</p>
<p>Our managed cloud service uses Kubernetes (K8s) to operate Serverless clusters, including both shared storage nodes and per-tenant SQL nodes. Each node runs in its own K8s pod, which is not much more than a Docker container with a virtualized network and a bounded CPU and memory capacity. Dig down deeper, and you’ll discover a Linux cgroup that can reliably limit the CPU and memory consumption for the processes. This allows us to easily meter and limit SQL resource consumption on a per-tenant basis. It also minimizes interference between pods that are scheduled on the same machine, giving each tenant a high-quality experience even when other tenants are running heavy workloads.</p>
<p>我们的托管云服务使用 Kubernetes (K8s) 来操作无服务器集群，包括共享存储节点和每租户 SQL 节点。 每个节点都在自己的 K8s pod 中运行，该 pod 只不过是一个具有虚拟化网络以及有限的 CPU 和内存容量的 Docker 容器。 深入挖掘，您会发现 Linux cgroup 可以可靠地限制进程的 CPU 和内存消耗。 这使我们能够轻松地计量和限制每个租户的 SQL 资源消耗。 它还最大限度地减少了安排在同一台机器上的 Pod 之间的干扰，即使其他租户运行繁重的工作负载，也能为每个租户提供高质量的体验。</p>
<p>Here is a high-level (simplified) representation of what a typical setup looks like:</p>
<p>以下是典型设置的高级（简化）表示：<img src="https://crl2020.imgix.net/img/serverless-white-paper-illustrations-05-1.png?auto=format,compress&max-w=640"></p>
<p>What are those “proxy pods” doing in the K8s cluster? It turns out they’re pretty useful:</p>
<p> 这些“代理 Pod”在 K8s 集群中做什么？ 事实证明它们非常有用：</p>
<ul>
<li><p>They allow many tenants to share the same IP address. When a new connection arrives, the proxy “sniffs” the incoming Postgres connection packets in order to find the tenant identifier in a SNI header or aPG connection option. Now it knows which SQL pods it should route that connection to.</p>
<p>它们允许许多租户共享相同的 IP 地址。 当新连接到达时，代理“嗅探”传入的 Postgres 连接数据包，以便在 SNI 标头或 aPG 连接选项中查找租户标识符。 现在它知道应该将该连接路由到哪些 SQL Pod。</p>
</li>
<li><p>They balance load across a tenant’s available SQL pods, using a “least connections” algorithm.</p>
<p>它们使用“最少连接”算法来平衡租户可用 SQL Pod 之间的负载。</p>
</li>
<li><p>They detect and respond to suspected abuse of the service. This is one of the security measures we take for the protection of your data.</p>
<p>他们检测并响应可疑的服务滥用行为。 这是我们为保护您的数据而采取的安全措施之一。</p>
</li>
<li><p>They automatically resume tenant clusters that have been paused due to inactivity. We’ll get into more detail on that in the Scaling section below.</p>
<p>它们会自动恢复因不活动而暂停的租户集群。 我们将在下面的“缩放”部分中详细介绍这一点。</p>
</li>
</ul>
<p>After the cloud load balancer routes a new connection to one of the proxy pods, the proxy pod will in turn forward that connection to a SQL pod owned by the connecting tenant. Each SQL pod is dedicated to just one tenant, and multiple SQL pods can be owned by the same tenant. Network security rules prevent SQL pods from talking to one another, unless they are owned by the same tenant. Finally, the SQL pods communicate via the KV layer to access data managed by the shared storage pods, each of which stores that data in a cloud provider block storage system like AWS EBS or GCP PD.</p>
<p>云负载均衡器将新连接路由到其中一个代理 Pod 后，代理 Pod 会将该连接转发到连接租户拥有的 SQL Pod。 每个 SQL Pod 仅专用于一个租户，同一租户可以拥有多个 SQL Pod。 网络安全规则阻止 SQL Pod 相互通信，除非它们属于同一租户。 最后，SQL Pod 通过 KV 层进行通信，以访问由共享存储 Pod 管理的数据，每个存储 Pod 都将该数据存储在 AWS EBS 或 GCP PD 等云提供商块存储系统中。</p>
<p>One of the best things about Serverless clusters is how fast they can be created. A regular Dedicated cluster takes 20-30 minutes to launch, since it has to create a cloud provider project, spin up new VMs, attach block storage devices, allocate IP and DNS addresses, and more. By contrast, a Serverless cluster takes just a few seconds to create, since we only need to instruct K8s to create a new SQL pod on an existing VM that it is already managing.</p>
<p>无服务器集群的优点之一是它们的创建速度有多快。 常规专用集群需要 20-30 分钟才能启动，因为它必须创建云提供商项目、启动新虚拟机、连接块存储设备、分配 IP 和 DNS 地址等。 相比之下，无服务器集群的创建只需要几秒钟，因为我们只需要指示 K8s 在它已经管理的现有虚拟机上创建一个新的 SQL Pod。</p>
<p>Besides speed of creation, Serverless SQL pods also have a big cost advantage. They can be packed together on a VM, sharing the same OS as well as available CPU and memory. This substantially reduces the cost of running “long-tail” tenants that have minuscule workloads, since they can each use just a small slice of the hardware. Contrast this with a dedicated VM, which generally requires at least 1 vCPU and 1GB of memory to be reserved for it.</p>
<p>除了创建速度之外，Serverless SQL Pod 还具有很大的成本优势。 它们可以打包在一个虚拟机上，共享相同的操作系统以及可用的 CPU 和内存。 这大大降低了运行工作负载极小的“长尾”租户的成本，因为他们每个人只能使用一小部分硬件。 与此相比，专用虚拟机通常需要为其保留至少 1 个 vCPU 和 1GB 内存。</p>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><p>As the amount of data owned by a tenant grows, and the frequency with which that data is accessed grows, the tenant’s data will be split into a growing number of KV ranges which will be spread across more shared storage pods. Data scaling of this kind is already well supported by CockroachDB, and operates in about the same way in multi-tenant clusters as it always has in single-tenant clusters. I won’t cover that kind of scaling in any more detail here.</p>
<p>随着租户拥有的数据量的增加以及数据访问频率的增加，租户的数据将被分割成越来越多的 KV 范围，这些 KV 范围将分布在更多的共享存储 Pod 中。 CockroachDB 已经很好地支持了这种类型的数据扩展，并且在多租户集群中的运行方式与在单租户集群中的运行方式大致相同。 我不会在这里更详细地介绍这种缩放。</p>
<p>Similarly, as the number of SQL queries and transactions run against a tenant’s data increases, the compute resources allocated to that tenant must grow proportionally. One tenant’s workload may need dozens or even hundreds of vCPUs to execute, while another tenant’s workload may just need a part-time fraction of a vCPU. In fact, we expect most tenants to not need any CPU at all. This is because a large proportion of developers who try CockroachDB Serverless are just “kicking the tires”. They’ll create a cluster, maybe run a few queries against it, and then abandon it, possibly for good. Even keeping a fraction of a vCPU idling for their cluster would be a tremendous waste of resources when multiplied by all inactive clusters. And even for tenants who regularly use their cluster, SQL traffic load is not constant; it may greatly fluctuate from day to day and hour to hour, or even second to second.</p>
<p>同样，随着针对租户数据运行的 SQL 查询和事务数量增加，分配给该租户的计算资源也必须成比例增长。 一个租户的工作负载可能需要数十甚至数百个 vCPU 来执行，而另一个租户的工作负载可能只需要 vCPU 的兼职部分。 事实上，我们预计大多数租户根本不需要任何 CPU。 这是因为尝试 CockroachDB Serverless 的开发人员中有很大一部分只是“尝试一下”。 他们将创建一个集群，也许对其运行一些查询，然后放弃它，可能会永远放弃它。 当乘以所有不活动集群时，即使让集群的一小部分 vCPU 处于空闲状态，也会造成巨大的资源浪费。 即使对于经常使用集群的租户来说，SQL 流量负载也不是恒定的； 它可能每天、每小时、甚至每秒都有很大的波动。</p>
<p>How does CockroachDB Serverless handle such a wide range of shifting resource needs? By dynamically allocating the right number of SQL pods to each tenant, based on its second-to-second traffic load. New capacity can be assigned instantly in the best case and within seconds in the worst case. This allows even extreme spikes in tenant traffic to be handled smoothly and with low latency. Similarly, as traffic falls, any SQL pod that is no longer needed can be shut down, with any remaining SQL connections transparently migrated to other pods for that tenant. If traffic falls to zero and no SQL connections remain, then all SQL pods owned by the now inactive tenant are terminated. As soon as new traffic arrives, a new SQL pod can be spun back up within a few hundred milliseconds. This allows a seldom-used CockroachDB Serverless cluster to still offer production-grade latencies for almost no cost to Cockroach Labs, and no cost at all to the user.</p>
<p>CockroachDB Serverless 如何处理如此广泛的不断变化的资源需求？ 根据每个租户的每秒流量负载，动态地为每个租户分配正确数量的 SQL Pod。 在最好的情况下可以立即分配新容量，在最坏的情况下可以在几秒钟内分配新容量。 这使得即使租户流量出现极端峰值也能以低延迟顺利处理。 同样，随着流量下降，任何不再需要的 SQL Pod 都可以关闭，任何剩余的 SQL 连接都会透明地迁移到该租户的其他 Pod。 如果流量降至零并且没有 SQL 连接剩余，则当前不活动租户拥有的所有 SQL Pod 将被终止。 一旦新流量到达，新的 SQL Pod 就可以在几百毫秒内恢复运行。 这使得很少使用的 CockroachDB Serverless 集群仍然可以提供生产级延迟，而 Cockroach Labs 几乎不需要任何成本，用户也不需要任何成本。</p>
<p>Such responsive scaling is only possible because multi-tenant CockroachDB splits the SQL layer from the KV storage layer. Because SQL pods are stateless, they can be created and destroyed at will, without impacting the consistency or durability of tenant data. There is no need for complex coordination between pods, or for careful commissioning and decommissioning of pods, as we must do with the stateful storage pods to ensure that all data stays consistent and available. Unlike storage pods, which typically remain running for extended periods of time, SQL pods are ephemeral and may be shut down within minutes of starting up.</p>
<p>这种响应式扩展之所以成为可能，是因为多租户 CockroachDB 将 SQL 层与 KV 存储层分开。 由于 SQL Pod 是无状态的，因此可以随意创建和销毁它们，而不会影响租户数据的一致性或持久性。 不需要在 Pod 之间进行复杂的协调，也不需要仔细调试和停用 Pod，而我们必须使用有状态存储 Pod 来确保所有数据保持一致和可用。 与通常长时间运行的存储 Pod 不同，SQL Pod 是短暂的，可能会在启动后几分钟内关闭。</p>
<h3 id="The-Autoscaler"><a href="#The-Autoscaler" class="headerlink" title="The Autoscaler"></a>The Autoscaler</h3><p>Let’s dig a little deeper into the mechanics of scaling. Within every Serverless cluster, there is an autoscaler component that is responsible for determining the ideal number of SQL pods that should be assigned to each tenant, whether that be one, many, or zero. The autoscaler monitors the CPU load on every SQL pod in the cluster, and calculates the number of SQL pods based on two metrics:</p>
<p>让我们更深入地研究缩放机制。 在每个无服务器集群中，都有一个自动缩放器组件，负责确定应分配给每个租户的 SQL Pod 的理想数量，无论是 1 个、多个还是 0 个。 自动缩放器监视集群中每个 SQL Pod 上的 CPU 负载，并根据两个指标计算 SQL Pod 的数量：</p>
<ul>
<li>Average CPU usage over the last 5 minutes. 过去 5 分钟的平均 CPU 使用率。</li>
<li>Peak CPU usage during the last 5 minutes. 过去 5 分钟内 CPU 使用率峰值。</li>
</ul>
<p>Average CPU usage determines the “baseline” number of SQL pods that will be assigned to the tenant. The baseline deliberately over-provisions SQL pods so that there is spare CPU available in each pod for instant bursting. However, if peak CPU usage recently exceeded even the higher over-provisioned threshold, then the autoscaler accounts for that by increasing the number of SQL pods past the baseline. This algorithm combines the stability of a moving average with the responsiveness of an instantaneous maximum. The autoscaler avoids too-frequent scaling, but can still quickly detect and react to large spikes in load.</p>
<p>平均 CPU 使用率决定了将分配给租户的 SQL Pod 的“基准”数量。 该基线故意过度配置 SQL Pod，以便每个 Pod 中有可用的备用 CPU 进行即时爆发。 但是，如果峰值 CPU 使用率最近甚至超过了更高的超额配置阈值，则自动缩放程序会通过将 SQL Pod 数量增加到超过基线来解决这一问题。 该算法结合了移动平均值的稳定性和瞬时最大值的响应能力。 自动缩放器避免了过于频繁的缩放，但仍然可以快速检测负载的大峰值并做出反应。</p>
<p>Once the autoscaler has derived the ideal number of SQL pods, it triggers a K8s reconciliation process that adds or removes pods in order to reach the ideal number. The following diagram shows the possible outcomes:</p>
<p>一旦自动缩放器得出理想的 SQL Pod 数量，它就会触发 K8s 协调过程，添加或删除 Pod 以达到理想数量。 下图显示了可能的结果：<img src="https://crl2020.imgix.net/img/serverless-white-paper-illustrations-06-1.png?auto=format,compress&max-w=640"></p>
<p>As the diagram shows, we maintain a pool of “prewarmed” pods that are ready to go at a moment’s notice; they just need to be “stamped” with the tenant’s identifier and security certificates. This takes a fraction of a second to do, versus the 20-30 seconds it takes for K8s to create a pod from scratch. If instead, pods need to be removed, they are not abruptly terminated, because that would also result in the rude termination of all SQL connections to that pod. Rather, the pods are put into a draining state, which gives them a chance to shed their SQL connections more gracefully. Some connections might be closed by the application; other connections will be transparently migrated by the proxy from the draining pods to other pods that are still active.A draining pod is terminated once all connections are gone or once 10 minutes have passed, whichever comes first.</p>
<p>如图所示，我们维护了一个“预热”的 Pod 池，随时可以启动； 他们只需要“盖上”租户的标识符和安全证书即可。 这只需几分之一秒的时间即可完成，而 K8s 从头开始创建 pod 需要 20-30 秒。 相反，如果需要删除 pod，它们不会突然终止，因为这也会导致粗暴终止与该 pod 的所有 SQL 连接。 相反，Pod 会进入耗尽状态，这使它们有机会更优雅地摆脱 SQL 连接。 某些连接可能会被应用程序关闭； 其他连接将由代理透明地从排出 Pod 迁移到其他仍处于活动状态的 Pod。一旦所有连接消失或经过 10 分钟（以先到者为准），排出 Pod 就会终止。</p>
<p>If application load falls to zero, then the autoscaler will eventually decide to suspend the tenant, which means that all of its SQL pods are removed. Once the tenant no longer owns any SQL pods, it does not consume any CPU, I&#x2F;O, or bandwidth. The only cost is for storage of its data, which is relatively cheap compared to other resources. This is one of the reasons that we can offer free database clusters to all of you.</p>
<p>如果应用程序负载降至零，则自动缩放程序最终将决定暂停租户，这意味着其所有 SQL Pod 都将被删除。 一旦租户不再拥有任何 SQL Pod，它就不会消耗任何 CPU、I&#x2F;O 或带宽。 唯一的成本是数据的存储，与其他资源相比相对便宜。 这是我们可以向大家提供免费数据库集群的原因之一。</p>
<p>However, there is one problem left to solve. How can a tenant connect to its cluster if there are no SQL pods assigned to it? To answer that question, remember that a set of proxy pods runs in every Serverless cluster. Each SQL connection initiated by an external client is intercepted by a proxy pod and then forwarded to a SQL pod assigned to the tenant. However, if the proxy finds that there are currently no SQL pods assigned to the tenant, then it triggers the same K8s reconciliation process that the autoscaler uses for scaling. A new pod is pulled from the prewarmed pool of SQL pods and stamped, and is now available for connections. The entire resumption process takes a fraction of a second, and we’re actively working on bringing that time down further.</p>
<p>然而，还有一个问题需要解决。 如果没有分配给租户的 SQL Pod，租户如何连接到其集群？ 要回答这个问题，请记住每个无服务器集群中都运行一组代理 Pod。 外部客户端发起的每个 SQL 连接都会被代理 Pod 拦截，然后转发到分配给租户的 SQL Pod。 但是，如果代理发现当前没有分配给租户的 SQL Pod，则会触发自动缩放程序用于缩放的相同 K8s 协调过程。 从预热的 SQL Pod 池中提取一个新 Pod 并进行标记，现在可用于连接。 整个恢复过程只需要几分之一秒，我们正在积极努力进一步缩短这一时间。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Now that you know how CockroachDB Serverless works, I encourage you to head on over to <a href="https://cockroachlabs.cloud/?referralId=andykblog">https://cockroachlabs.cloud/</a> and give it a try. If you have questions about any of this, please join us in our <a href="https://www.cockroachlabs.com/join-community/">Community Slack channel</a> and ask away. We’d also love to hear about your experience with CockroachDB Serverless and any positive or negative feedback. We’ll be working hard to improve it over the coming months, so <a href="https://cockroachlabs.cloud/signup/?referralId=andykblog">sign up for an account</a> and get updates on our progress. And if you’d love to help us take CockroachDB to the next level, <a href="https://www.cockroachlabs.com/careers/">we’re hiring</a>.</p>
<p>现在您已经了解了 CockroachDB Serverless 的工作原理，我鼓励您前往 <a href="https://cockroachlabs.cloud/">https://cockroachlabs.cloud/</a> 并尝试一下。 如果您对此有任何疑问，请加入我们的 Community Slack 频道并提问。 我们也很想听听您使用 CockroachDB Serverless 的体验以及任何正面或负面的反馈。 我们将在未来几个月努力改进它，因此请注册一个帐户并获取有关我们进展的最新信息。 如果您愿意帮助我们将 CockroachDB 提升到一个新的水平，我们正在招聘。</p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><ul>
<li>Free online course: <a href="https://university.cockroachlabs.com/courses/course-v1:crl+intro-to-serverless+self-paced/about">Introduction to Serverless Databases and CockroachDB Serverless</a>. This course introduces the core concepts behind serverless databases and gives you the tools you need to get started with CockroachDB Serverless</li>
<li>Bring us your feedback: <a href="https://www.cockroachlabs.com/join-community/?referralId=andykblog">Join our Slack community</a> and let us know your thoughts!</li>
<li>We’re hiring! <a href="https://www.cockroachlabs.com/careers/?referralId=andykblog">Join the team</a> building CockroachDB. Remote-friendly, family-friendly, and taking on some of the biggest challenges in data and app development.</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>cockroach</tag>
      </tags>
  </entry>
  <entry>
    <title>How CockroachDB checks replication</title>
    <url>/2024/01/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/08-How%20CockroachDB%20checks%20replication/</url>
    <content><![CDATA[<p><a href="https://www.cockroachlabs.com/blog/trust-but-verify-cockroachdb-checks-replication/">https://www.cockroachlabs.com/blog/trust-but-verify-cockroachdb-checks-replication/</a></p>
<h1 id="Trust-but-verify-How-CockroachDB-checks-replication"><a href="#Trust-but-verify-How-CockroachDB-checks-replication" class="headerlink" title="Trust, but verify: How CockroachDB checks replication"></a>Trust, but verify: How CockroachDB checks replication</h1><p>We built survivability into the DNA of CockroachDB. And while we had a lot of fun doing so, and are confident that we have built a solution on a firm foundation, we felt a nagging concern: Does CockroachDB really survive? When data is written to the database, will a failure really not end up in data loss? So to assuage those concerns, we adopted a Russian maxim: <em>“Dovorey, no provorey – Trust, but Verify.”</em></p>
<p>我们将生存能力融入了 CockroachDB 的 DNA 中。 虽然我们这样做很有趣，并且相信我们已经在坚实的基础上构建了一个解决方案，但我们感到一个挥之不去的担忧：CockroachDB 真的能生存下来吗？ 当数据写入数据库时，失败真的不会导致数据丢失吗？ 因此，为了缓解这些担忧，我们采用了一句俄罗斯格言：“Dovorey，no provorey – 信任，但验证。”</p>
<span id="more"></span>

<p>To understand CockroachDB’s survivability promise, you must first understand our key-value store and replication model, as they form the foundation for survivability.</p>
<p>要了解 CockroachDB 的生存性承诺，您必须首先了解我们的键值存储和复制模型，因为它们构成了生存性的基础。</p>
<p>CockroachDB is a SQL database built on top of a distributed consistent key-value store. The entire key-value space is split into multiple contiguous key-value ranges spread across many nodes. CockroachDB uses Raft for consensus-based replicated writes, guaranteeing that a write to any key is durable, or in other words, that it survives. On each write to a key-value range, the range is synchronously replicated from a Raft leader to multiple replicas residing on different nodes. Database writes are replayed on all replicas in the same order as on the leader, guaranteeing consistent replication. This model forms the basis of CockroachDB survivability: if a node dies, a few others have an exact copy of the data that is lost.</p>
<p>CockroachDB 是一个构建在分布式一致键值存储之上的 SQL 数据库。 整个键值空间被分成多个连续的键值范围，分布在许多节点上。 CockroachDB 使用 Raft 进行基于共识的复制写入，保证对任何键的写入都是持久的，或者换句话说，它能够存活。 每次写入键值范围时，该范围都会从 Raft 领导者同步复制到驻留在不同节点上的多个副本。 数据库写入会按照与领导者相同的顺序在所有副本上重播，从而保证复制的一致性。 该模型构成了 CockroachDB 生存能力的基础：如果一个节点死亡，其他一些节点将拥有丢失数据的精确副本。</p>
<p>While we’d like to trust our survivability model, we went one step further: we added verification. We built a subsystem that periodically verifies that all replicas of a range have identical data, so that when one of them has to step up to replace a lost node, the data served is what is expected.</p>
<p>虽然我们希望相信我们的生存模型，但我们更进一步：我们添加了验证。 我们构建了一个子系统，定期验证某个范围的所有副本是否具有相同的数据，以便当其中一个副本必须取代丢失的节点时，所提供的数据就是预期的。</p>
<h2 id="How-CockroachDB-Checks-Replication"><a href="#How-CockroachDB-Checks-Replication" class="headerlink" title="How CockroachDB Checks Replication"></a>How CockroachDB Checks Replication</h2><p>Every node in the cluster runs an independent consistency checker. The checker scans through all the local replicas in a continuous loop over a 24 hour cycle, and runs a consistency check on each range for which it is the Raft leader:</p>
<p>集群中的每个节点都运行一个独立的一致性检查器。 检查器以 24 小时为周期连续循环扫描所有本地副本，并对它作为 Raft 领导者的每个范围运行一致性检查：</p>
<ol>
<li><p>The leader and replicas agree on the snapshot of the data to verify and assign a unique ID to it.</p>
<p>领导者和副本就数据快照达成一致，以验证数据并为其分配唯一的 ID。</p>
</li>
<li><p>A SHA-512 based checksum on the selected snapshot is computed in parallel on the leader and all replicas.</p>
<p>在领导者和所有副本上并行计算所选快照的基于 SHA-512 的校验和。</p>
</li>
<li><p>The leader shares its checksum and the unique snapshot ID with all replicas. A replica compares the supplied checksum with the one it has computed on its own. On seeing a different checksum, it declares a failure in replication.</p>
<p>领导者与所有副本共享其校验和和唯一快照 ID。 副本将提供的校验和与它自己计算的校验和进行比较。 当看到不同的校验和时，它声明复制失败。</p>
</li>
</ol>
<p>In the rare circumstance that it finds a replication problem, the checker logs an error or panics. Unfortunately, it is not possible for the system to identify whether the Raft leader or its replica is responsible for a faulty replication, and a replication problem cannot be repaired.</p>
<p>在极少数情况下，检查程序发现复制问题，会记录错误或出现紧急情况。 不幸的是，系统无法识别Raft领导者或其副本是否对错误的复制负责，并且复制问题无法修复。</p>
<p>Logging or panicking on detecting an inconsistency problem is great, but how does one debug such problems? To aid the user in debugging, on seeing a checksum mismatch, a second consistency check with an advanced diff setting is triggered on the range. The second consistency check publishes the entire range snapshot from the leader to all replicas, so that they can diff the entire snapshot against their own version and log a diff of the keys that are inconsistent. We were able to use this second consistency check mechanism to debug and fix many problems in replication.</p>
<p>在检测到不一致问题时进行记录或恐慌固然很好，但如何调试此类问题呢？ 为了帮助用户进行调试，在看到校验和不匹配时，会在范围上触发使用高级差异设置的第二次一致性检查。 第二次一致性检查将整个范围快照从领导者发布到所有副本，以便它们可以将整个快照与自己的版本进行比较，并记录不一致的键的差异。 我们能够使用第二种一致性检查机制来调试和修复复制中的许多问题。</p>
<p>While we had hoped we had built the perfect system, the verifier uncovered a few bugs!</p>
<p>虽然我们希望我们已经构建了完美的系统，但验证者发现了一些错误！</p>
<h2 id="Bugs-and-Fixes"><a href="#Bugs-and-Fixes" class="headerlink" title="Bugs and Fixes"></a>Bugs and Fixes</h2><p>We fixed a number of hairy bugs:</p>
<ol>
<li><p><strong>The same data can look different:</strong> The order of entries in protocol buffer maps is undefined in the standard. For some cockroach internal data, we were using protocol buffer maps and replicating it via Raft. All the replicas would separately convert the protocol buffers into a wire format while writing them to disk. Since the wire format is non-deterministic in the standard and implementation, we saw the same data with <strong>a different wire encoding</strong> on different replicas (fixed via gogo&#x2F;protobuf <a href="https://github.com/gogo/protobuf/pull/156">#156</a>).</p>
<p>相同的数据可能看起来不同：协议缓冲区映射中的条目顺序在标准中未定义。 对于一些 cockroach 内部数据，我们使用协议缓冲区映射并通过 Raft 复制它。 所有副本都会将协议缓冲区分别转换为有线格式，同时将其写入磁盘。 由于有线格式在标准和实现中是不确定的，我们在不同的副本上看到相同的数据具有不同的有线编码（通过 gogo&#x2F;protobuf #156 修复）。</p>
</li>
<li><p><strong>Backdoor writes on the replicas (<a href="https://github.com/cockroachdb/cockroach/issues/5090">#5090</a>):</strong> We collect statistics on every range in the system and replicate them. Writes to the statistics are allowed only on the leader, with replicas simply replaying the operations. We were occasionally updating the statistics on the replicas outside of Raft consensus (fixed via <a href="https://github.com/cockroachdb/cockroach/pull/5133">#5133</a>).</p>
<p>后门写入副本（#5090）：我们收集系统中每个范围的统计信息并复制它们。 仅允许在领导者上写入统计信息，而副本仅重播操作。 我们偶尔会更新 Raft 共识之外的副本的统计信息（通过 #5133 修复）。</p>
</li>
<li><p><strong>Internal time series data was being merged incorrectly:</strong> The CockroachDB UI keeps track of monitoring time series data which is replicated. The replicas merge the time series data when read, but an occasional legitimate read would creep in on the leader, causing a bug in the merge functionality (fixed via <a href="https://github.com/cockroachdb/cockroach/pull/5515">#5515</a>).</p>
<p>内部时间序列数据合并不正确：CockroachDB UI 跟踪监视复制的时间序列数据。 副本在读取时合并时间序列数据，但偶尔的合法读取会悄悄进入领导者，导致合并功能出现错误（通过＃5515修复）。</p>
</li>
<li><p><strong>Floating point addition might be non-deterministic:</strong> While fixing the above time series issue, we got super paranoid and as a defensive measure decided to not depend on the replica replay of floating point addition. We were aware of floating point addition being <a href="http://www.walkingrandomly.com/?p=5380">non-associative</a>, and although we knew our floating point additions were being replayed in a definite order and didn’t depend on the associativity property, we adhered to the mantra, “only the paranoid survive,” and got rid of them (fixed via <a href="https://github.com/cockroachdb/cockroach/pull/5905">#5905</a>).</p>
<p>浮点加法可能是不确定的：在解决上述时间序列问题时，我们变得超级偏执，作为一种防御措施，决定不依赖于浮点加法的副本重放。 我们知道浮点加法是非关联性的，尽管我们知道我们的浮点加法是以明确的顺序重播并且不依赖于关联性属性，但我们坚持这样的口头禅：“只有偏执狂才能生存” 并摆脱了它们（通过#5905修复）。</p>
</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We’ve built a new database that you can trust to survive. With trust comes verification, and we built that into CockroachDB. CockroachDB offers other features like indexes, unique columns, and foreign keys, that you can trust to work properly. We plan on building automatic online verification mechanisms for them, too. We look forward to discussing them in the future.</p>
<p>我们建立了一个值得您信赖的新数据库。 信任带来验证，我们将其内置到 CockroachDB 中。 CockroachDB 提供其他功能，如索引、唯一列和外键，您可以相信它们可以正常工作。 我们也计划为他们建立自动在线验证机制。 我们期待将来讨论它们。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>cockroach</tag>
      </tags>
  </entry>
  <entry>
    <title>CockroachDB stability post-mortem From 1 node to 100 nodes</title>
    <url>/2023/01/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/09-CockroachDB%20stability%20post-mortem%20From%201%20node%20to%20100%20nodes/</url>
    <content><![CDATA[<p><a href="https://www.cockroachlabs.com/blog/cockroachdb-stability-from-1-node-to-100-nodes/">https://www.cockroachlabs.com/blog/cockroachdb-stability-from-1-node-to-100-nodes/</a></p>
<p>In August, we published a blog post entitled “<a href="https://www.cockroachlabs.com/blog/cant-run-100-node-cockroachdb-cluster/">Why Can’t I Run a 100-Node CockroachDB Cluster?</a>”. The post outlined difficulties we encountered stabilizing CockroachDB. CockroachDB stability (or the lack of) had become significant enough that we designated it a “code yellow” issue, a concept borrowed from Google that means a problem is so pressing that it merits promotion to a primary concern of the company. For us, the code yellow was more than warranted; a database program isn’t worth the bytes to store its binary if it lacks stability.</p>
<p>8 月份，我们发表了一篇题为“为什么我不能运行 100 节点 CockroachDB 集群？”的博客文章。 这篇文章概述了我们在稳定 CockroachDB 时遇到的困难。 CockroachDB 稳定性（或缺乏稳定性）已经变得足够严重，以至于我们将其指定为“黄色代码”问题，这是一个借用自 Google 的概念，意味着问题非常紧迫，值得提升为公司的首要关注点。 对我们来说，黄色代码是值得的； 如果数据库程序缺乏稳定性，那么它就不值得用字节来存储其二进制文件。</p>
<p>In this post, I’ll set the stage with some background, then cover hypotheses for root causes of instability, our communication strategy, some interesting technical details, outcomes for stabilization efforts, and conclusions. It’s a long post, so bear with me!</p>
<p>在这篇文章中，我将介绍一些背景知识，然后介绍不稳定的根本原因的假设、我们的沟通策略、一些有趣的技术细节、稳定工作的结果和结论。 这是一篇很长的文章，所以请耐心等待！</p>
<p><strong>TL;DR: We achieved most of our stability goal. While we’re still working on some of the chaos scenarios, the system is easily stable at many more than 10 node clusters – we’ve tested it successfully at 100 nodes</strong>.</p>
<p>TL;DR：我们实现了大部分稳定性目标。 虽然我们仍在处理一些混乱场景，但系统在超过 10 个节点的集群上很容易保持稳定——我们已经在 100 个节点上成功测试了它。</p>
<span id="more"></span>

<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>To better set the stage: we announced the CockroachDB Beta release in April, after more than a year of development. Over the five months of progress on the beta, concerns over correctness, performance, and the general push for new features dominated our focus. We incorrectly assumed stability would be an emergent property of forward progress, just as long as everyone was paying some attention to, and fixing stability bugs whenever they were encountered. But by August, despite a team of 20 developers, we couldn’t stand up a 10-node cluster for two weeks without major performance and stability issues.</p>
<p>为了更好地做好准备：经过一年多的开发，我们于 4 月份宣布了 CockroachDB Beta 版本。 在测试版的五个月进展中，对正确性、性能和新功能的总体推动的担忧主导了我们的重点。 我们错误地认为只要每个人都关注并修复遇到的稳定性错误，稳定性就会成为前进的一个新兴属性。 但到了 8 月份，尽管我们的团队有 20 名开发人员，但我们仍无法在不出现重大性能和稳定性问题的情况下让 10 节点集群运行两周。</p>
<p>Nothing could more effectively convey the gulf that had opened up between our stability expectations and reality than the increasingly frequent mentions of instability as a punchline in the office. Despite feeling that we were just one or two pull requests away from stability, the inevitable chuckle nevertheless morphed into a insidious critique. This blog post chronicles our journey to establish a baseline of stability for CockroachDB.</p>
<p>没有什么比办公室里越来越频繁地提到不稳定问题更能有效地传达我们对稳定的期望与现实之间出现的鸿沟了。 尽管感觉我们离稳定只有一两个拉请求，但不可避免的笑声仍然演变成一种阴险的批评。 这篇博文记录了我们为 CockroachDB 建立稳定性基线的过程。</p>
<h2 id="Hypotheses-on-Root-Causes"><a href="#Hypotheses-on-Root-Causes" class="headerlink" title="Hypotheses on Root Causes"></a>Hypotheses on Root Causes</h2><p>关于根本原因的假设</p>
<p>What caused instability? Obviously there were technical oversights and unexpectedly complex interactions between system components. A better question is: what was <em>preventing</em> us from achieving stability? Perhaps surprisingly, our hypotheses came down to a mix of mostly process and management failures, not engineering. We identified three root causes:</p>
<p>是什么导致了不稳定？ 显然，系统组件之间存在技术疏忽和意外复杂的交互。 更好的问题是：是什么阻碍了我们实现稳定？ 也许令人惊讶的是，我们的假设主要归结为流程和管理失败，而不是工程失败。 我们确定了三个根本原因：</p>
<ol>
<li><p><strong>The rapid pace of development was obscuring, or contributing to, instability faster than solutions could be developed.</strong> Imagine a delicate surgery with an excessive amount of blood welling up in the incision. You must stop the bleeding first in order to operate. This analogy suggested we’d need to work on stability fixes in isolation from normal development. We accomplished this by splitting our <em>master</em> branch into two branches: the <em>master</em> branch would be dedicated to stability, freezing with the exception of pull requests targeting stability. All other development would continue in a <em>develop</em> branch.</p>
<p>快速的发展速度比解决方案的制定速度更快地掩盖或助长了不稳定因素。 想象一下一场精密的手术，切口中涌出过量的血液。 必须先止血才能进行手术。 这个类比表明我们需要独立于正常开发来进行稳定性修复。 我们通过将主分支分成两个分支来实现这一点：主分支将致力于稳定性，除了针对稳定性的拉取请求外，冻结分支。 所有其他开发将在开发分支中继续。</p>
</li>
<li><p><strong>While many engineers were paying attention to stability, there was no focused team and no clear leader.</strong> Imagine many cooks in the kitchen working independently on the same dish, without anyone responsible for the result tasting right. To complicate matters, imagine many of the chefs making experimental contributions… Time to designate a head chef. We chose Peter Mattis, one of our co-founders. He leads engineering and is particularly good at diagnosing and fixing complex systems, and so was an obvious choice. Instead of his previously diffuse set of goals to develop myriad functionality and review significant amounts of code, we agreed that he would largely limit his focus to stability and become less available for other duties. The key objective here was to enable focus and establish accountability.</p>
<p>虽然许多工程师都注重稳定性，但没有专注的团队，也没有明确的领导者。 想象一下厨房里的许多厨师独立地制作同一道菜，没有人对结果的味道负责。 让事情变得复杂的是，想象一下许多厨师都在做出实验性的贡献……是时候指定一位主厨了。 我们选择了我们的联合创始人之一彼得·马蒂斯。 他领导工程部门，特别擅长诊断和修复复杂系统，因此是一个显而易见的选择。 我们一致认为，他将在很大程度上将注意力集中在稳定性上，并减少承担其他职责的时间，而不是他之前制定的分散的目标来开发无数的功能和审查大量的代码。 这里的主要目标是集中注意力并建立问责制。</p>
</li>
<li><p><strong>Instability was localized in a core set of components, which were undergoing too many disparate changes for anyone to fully understand and review.</strong> Perhaps a smaller team could apply more scrutiny to fewer, careful changes and achieve what had eluded a larger team. We downsized the team working on core components (the transactional, distributed key-value store), composed of five engineers with the most familiarity with that part of the codebase. We even changed seating arrangements, which felt dangerous and counter-cultural, as normally we randomly distribute engineers so that project teams naturally resist balkanization.</p>
<p>不稳定性集中在一组核心组件中，这些组件正在经历太多不同的变化，任何人都无法完全理解和审查。 也许较小的团队可以对更少、更仔细的变更进行更多的审查，并实现较大团队无法实现的目标。 我们缩小了负责核心组件（事务性分布式键值存储）的团队规模，该团队由五名最熟悉代码库该部分的工程师组成。 我们甚至改变了座位安排，这感觉很危险且反文化，因为通常我们随机分配工程师，这样项目团队自然会抵制巴尔干化。</p>
</li>
</ol>
<h2 id="Communication"><a href="#Communication" class="headerlink" title="Communication"></a>Communication</h2><p>The decision to do something about stability happened quickly. I’d come home from an August vacation blithely assuming instability a solved problem. Unfortunately, new and seemingly worse problems had cropped up. This finally provided enough perspective to galvanize us into action. Our engineering management team discussed the problem in earnest, considered likely causes, and laid out a course of action over the course of a weekend. To proceed, we had to communicate the decisions internally to the team at Cockroach Labs, and after some soul searching, externally to the community at large.</p>
<p>我们很快就决定采取一些措施来保持稳定。 八月份的假期结束后，我满心欢喜地认为不稳定问题已得到解决。 不幸的是，新的、看似更严重的问题又出现了。 这最终提供了足够的视角来激励我们采取行动。 我们的工程管理团队认真讨论了该问题，考虑了可能的原因，并在周末制定了行动方案。 为了继续下去，我们必须在内部向 Cockroach Labs 的团队传达这些决定，并在一番自我反省之后，向外部整个社区传达这些决定。</p>
<p>One of our values at Cockroach Labs is transparency. Internally, we are open about our stability goals and our successes or failures to meet them. But just being transparent about a problem isn’t enough; where we fell down was in being honest with ourselves about the magnitude of the problem and what it meant for the company.</p>
<p>Cockroach Labs 的价值观之一是透明度。 在内部，我们对我们的稳定目标以及实现这些目标的成功或失败持开放态度。 但仅仅对问题保持透明是不够的。 我们失败的地方在于对自己诚实地了解问题的严重性以及它对公司的意义。</p>
<p>Once decided, we drafted a detailed email announcing the code yellow to the team. Where we succeeded was in clearly defining the problem and risks, actions to be taken, and most importantly: code yellow exit criteria. <strong>Exit criteria must be measurable and achievable!</strong> We decided on “a 10-node cluster running for two weeks under chaos conditions without data loss or unexpected downtime.”</p>
<p>一旦决定，我们起草了一封详细的电子邮件，向团队宣布黄色代码。 我们成功的地方在于清楚地定义了问题和风险、要采取的行动，最重要的是：黄色代码退出标准。 退出标准必须是可衡量和可实现的！ 我们决定“一个 10 节点集群在混乱条件下运行两周，不会丢失数据或意外停机”。</p>
<p>Where we didn’t succeed was in how precipitous the decision and communication seemed to some members of the team. We received feedback that the decision lacked sufficient deliberation and the implementation felt “railroaded”.</p>
<p>我们没有成功的地方在于，在团队的一些成员看来，决策和沟通显得过于草率。 我们收到的反馈称，该决定缺乏充分的深思熟虑，而且实施起来感觉“不顺利”。</p>
<p>We didn’t decide immediately to communicate the code yellow externally, although consensus quickly formed around the necessity. For one thing, we’re building an open source project and we make an effort to use Gitter instead of Slack for engineering discussions, so the community at large can participate. It would be a step backwards to withhold this important change in focus. For another thing, the community surely was aware of our stability problems and this was an opportunity to clarify and set expectations.</p>
<p>尽管很快就必要性达成了共识，但我们并没有立即决定向外部传达黄色代码。 一方面，我们正在构建一个开源项目，并努力使用 Gitter 而不是 Slack 进行工程讨论，以便整个社区都可以参与。 如果不关注这一重要的焦点变化，那将是一种倒退。 另一方面，社区肯定意识到了我们的稳定性问题，这是一个澄清和设定期望的机会。</p>
<p>Nevertheless, the task of actually writing a <a href="https://www.cockroachlabs.com/blog/cant-run-100-node-cockroachdb-cluster/">blog post to announce the stability code yellow</a> wasn’t easy and wasn’t free of misgivings. Raise your hand if you like airing your problems in public… Unsurprisingly, there was criticism from <a href="https://news.ycombinator.com/item?id=12361921">Hacker News commentators</a>, but there were also supportive voices. In the end, maintaining community transparency was the right decision and we hope established trust.</p>
<p>然而，实际撰写博客文章来宣布稳定性代码黄色的任务并不容易，而且也存在疑虑。 如果你喜欢在公共场合表达你的问题，请举手……不出所料，黑客新闻评论员提出了批评，但也有支持的声音。 最后，保持社区透明度是正确的决定，我们希望建立信任。</p>
<h2 id="Technical-Details"><a href="#Technical-Details" class="headerlink" title="Technical Details"></a>Technical Details</h2><p>With changes to process and team structure decided, and the necessary communication undertaken, we embarked on an intense drive to address the factors contributing to instability in priority order. We of course had no idea how long this would take. Anywhere from one to three months was the general consensus. In the end, we achieved our code yellow exit criteria in five weeks.</p>
<p>在决定改变流程和团队结构并进行必要的沟通后，我们开始大力解决导致优先顺序不稳定的因素。 我们当然不知道这需要多长时间。 普遍共识是一到三个月。 最终，我们在五周内达到了黄色代码退出标准。</p>
<p>What did we fix? Well, instability appeared in various guises, including clusters slowing precipitously or deadlocking, out-of-memory panics (OOMs), and data corruption (detected via periodic replica checksum comparisons).</p>
<p>我们修复了什么？ 不稳定以各种形式出现，包括集群急剧减速或死锁、内存不足恐慌 (OOM) 以及数据损坏（通过定期副本校验和比较检测到）。</p>
<h3 id="Rebalancing-via-Snapshots"><a href="#Rebalancing-via-Snapshots" class="headerlink" title="Rebalancing via Snapshots"></a>Rebalancing via Snapshots</h3><p>The generation and communication of replica snapshots, used to rebalance and repair data in a CockroachDB cluster, was our most persistent adversary in the battle for stability. Snapshots use significant disk and network IO, and mechanisms that limit their memory consumption and processing time while holding important locks were originally considered unnecessary for beta stability. Much of the work to tame snapshots occurred during the months leading up to the stability code yellow, which hints at their significance. Over the course of addressing snapshots, we reduced their memory usage with streaming RPCs, and made structural changes to avoid holding important locks during generation. However, the true cause of snapshot instability proved to be a trivial oversight, but it was simply not visible through the fog of cluster stabilization – at least, not until <em>after</em> we’d mostly eliminated the obvious symptoms of snapshot badness.</p>
<p>副本快照的生成和通信（用于重新平衡和修复 CockroachDB 集群中的数据）是我们在稳定性之战中最顽固的对手。 快照使用大量的磁盘和网络 IO，并且在持有重要锁的同时限制其内存消耗和处理时间的机制最初被认为对于 beta 稳定性来说是不必要的。 控制快照的大部分工作都是在稳定代码黄色之前的几个月内进行的，这暗示了它们的重要性。 在处理快照的过程中，我们通过流式 RPC 减少了它们的内存使用，并进行了结构更改以避免在生成过程中持有重要的锁。 然而，事实证明，快照不稳定的真正原因是一个微不足道的疏忽，但透过集群稳定的迷雾根本看不到它——至少在我们基本上消除了快照不良的明显症状之后。</p>
<p>Snapshots are used by nodes to replicate information to other nodes for repair (if a node is lost), or rebalancing (to spread load evenly between nodes in a cluster). Rebalancing is accomplished with a straightforward algorithm:</p>
<p>节点使用快照将信息复制到其他节点以进行修复（如果节点丢失）或重新平衡（在集群中的节点之间均匀分布负载）。 重新平衡是通过一个简单的算法完成的：</p>
<p><strong>1.</strong> Nodes periodically advertise the number of range replicas they maintain.</p>
<p>节点定期通告其维护的范围副本数量。</p>
<p><strong>2.</strong> Each node computes the mean replica count across all nodes, and decides:</p>
<p>每个节点计算所有节点的平均副本数，并决定：</p>
<ul>
<li>If a node is underfull compared to the mean, it does nothing 如果一个节点与均值相比未满，则它不会执行任何操作</li>
<li>If overfull, it rebalances via snapshot to an underfull node 如果超满，它会通过快照重新平衡到未满的节点</li>
</ul>
<p>Our error was in making this judgement too literally, without applying enough of a threshold around the mean in order to avoid “thrashing”. See the animated diagram below which shows two scenarios.</p>
<p>我们的错误在于过于字面地做出了这个判断，而没有在均值周围应用足够的阈值以避免“抖动”。 请参阅下面的动画图，其中显示了两种场景。</p>
<p><strong>看原文中的动画</strong></p>
<p><em><strong>Simulation 1.</strong> In the left “Exact Mean” simulation, we rebalance to within a replica of the mean; this will never stop rebalancing. Notice that far more RPCs are sent and the simulation never reaches equilibrium.</em></p>
<p>模拟 1. 在左侧的“精确均值”模拟中，我们重新平衡到均值的副本内； 这永远不会停止再平衡。 请注意，发送了更多的 RPC，并且模拟永远不会达到平衡。</p>
<p><em>In the right “Threshold of Mean” simulation, we rebalance to within a threshold of the mean, which quickly reaches equilibrium. In practice, continuously rebalancing crowded out other, more salient, work being done in the cluster.</em></p>
<p>在右侧的“均值阈值”模拟中，我们重新平衡到均值阈值内，从而很快达到平衡。 在实践中，不断的重新平衡会挤出集群中正在完成的其他更重要的工作。</p>
<h3 id="Lock-Refactoring"><a href="#Lock-Refactoring" class="headerlink" title="Lock Refactoring"></a>Lock Refactoring</h3><p>Tracing tools were invaluable in diagnosing lock-contention as a cause of excessively slow or deadlocked clusters. Most of these symptoms were caused by holding common locks during processing steps which could sometimes take an order of magnitude longer than originally supposed. Pileups over common locks resulted in RPC traffic jams and excessive client latencies. The solution was lock refactoring.</p>
<p>跟踪工具对于诊断由于集群速度过慢或死锁而引起的锁争用非常重要。 大多数这些症状是由于在处理步骤期间持有公共锁而引起的，这些步骤有时可能比最初预期的时间长一个数量级。 常见锁的堆积导致 RPC 流量堵塞和客户端延迟过大。 解决方案是锁重构。</p>
<p>Locks held during Raft processing, in particular, proved problematic as commands for ranges were executed serially, holding a single lock per range. This limited parallelization and caused egregious contention for long-running commands, notably replica snapshot generation. Garbage collection of replica data after rebalancing was previously protected by a common lock in order to avoid tricky consistency issues. Replica GC work is time consuming and impractical to do while holding a per-node lock covering actions on all stores. In both cases, the expedient solution of coarse-grained locking proved inadequate and required refactoring.</p>
<p>事实证明，Raft 处理期间持有的锁尤其存在问题，因为范围命令是串行执行的，每个范围持有一个锁。 这种限制并行化并导致长时间运行的命令的严重争用，特别是副本快照生成。 重新平衡后副本数据的垃圾收集之前受到公共锁的保护，以避免棘手的一致性问题。 在持有覆盖所有存储上的操作的每节点锁的情况下，副本 GC 工作非常耗时且不切实际。 在这两种情况下，粗粒度锁定的权宜解决方案都被证明是不够的，需要重构。</p>
<h3 id="Tracing-Tools"><a href="#Tracing-Tools" class="headerlink" title="Tracing Tools"></a>Tracing Tools</h3><p>Ironically, the same tracing tools used to diagnose degenerate locking behavior were themselves stability culprits. Our internal tracing tools were pedantically storing complete dumps of KV and Raft commands while those spans were held in a trace’s ring buffer. This was fine for small commands, but quickly caused Out-of-Memory (OOM) errors for larger commands, especially pre-streaming snapshots. A silver lining to our various OOM-related difficulties was development of fine-grained memory consumption metrics, tight integration with Go and C++ heap profiling tools, and integration with <a href="http://lightstep.com/">Lightstep</a>, a distributed tracing system inspired by Google’s Dapper.</p>
<p>讽刺的是，用于诊断退化锁定行为的相同跟踪工具本身就是稳定性的罪魁祸首。 我们的内部跟踪工具迂腐地存储 KV 和 Raft 命令的完整转储，而这些跨度则保存在跟踪的环形缓冲区中。 这对于小型命令来说没什么问题，但对于较大的命令，尤其是预流快照，很快就会导致内存不足 (OOM) 错误。 解决各种与 OOM 相关的困难的一线希望是开发细粒度的内存消耗指标、与 Go 和 C++ 堆分析工具的紧密集成，以及与 Lightstep（受 Google Dapper 启发的分布式跟踪系统）的集成。</p>
<h3 id="Corruption-腐败！"><a href="#Corruption-腐败！" class="headerlink" title="Corruption! 腐败！"></a>Corruption! 腐败！</h3><p>OOMs and deadlocks are often diagnosed and fixed through honest labor that pays an honest wage. What keeps us up at night are seemingly impossible corruption errors. Some of these occur between replicas (i.e. replicas don’t agree on a common checksum of their contents). Others are visible when system invariants are broken. These kinds of problems have been rare, though we found one during our stability code yellow.</p>
<p>OOM 和僵局通常可以通过支付诚实工资的诚实劳动来诊断和解决。 让我们彻夜难眠的是看似不可能的腐败错误。 其中一些发生在副本之间（即副本不同意其内容的通用校验和）。 当系统不变量被破坏时，其他的问题就会显现出来。 此类问题很少见，但我们在稳定性代码黄色期间发现了一个问题。</p>
<p>CockroachDB uses a bi-level index to access data in the system. The first level lives on a special bootstrap range, advertised via gossip to all nodes. It contains addressing information for the second level, which lives on an arbitrary number of subsequent ranges. The second level, finally, contains addressing information for the actual system data, which lives on the remaining ranges.</p>
<p>CockroachDB使用双层索引来访问系统中的数据。 第一级存在于一个特殊的引导范围内，通过八卦向所有节点通告。 它包含第二层的寻址信息，该信息存在于任意数量的后续范围中。 最后，第二层包含实际系统数据的寻址信息，该数据位于其余范围内。</p>
<p>Addressing records are updated when ranges split and are rebalanced or repaired. They are updated like any other data in the system, using distributed transactions, and should always be consistent. However, a second level index addressing record went unexpectedly missing. Luckily, Ben Darnell, our resident coding Sherlock Holmes, was able to theorize a gap in our model which could account for the problem, despite requiring an obscure and unlikely sequence of events, and perfect timing.  It’s amazing what a brilliant engineer can intuit from code inspection alone. Also, there ought to be a maxim that in a sufficiently large distributed system, anything that can happen, will happen.</p>
<p>当范围分裂并重新平衡或修复时，寻址记录会被更新。 它们像系统中的任何其他数据一样使用分布式事务进行更新，并且应该始终保持一致。 然而，二级索引寻址记录意外丢失。 幸运的是，我们的常驻编码夏洛克·福尔摩斯的本·达内尔（Ben Darnell）能够对我们模型中的一个缺口进行理论分析，从而可以解释这个问题，尽管需要一个模糊且不可能的事件序列和完美的时机。 令人惊奇的是，一位出色的工程师仅通过代码检查就能直觉到什么。 另外，应该有一条格言：在足够大的分布式系统中，任何可能发生的事情都会发生。</p>
<h3 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h3><p>Last, and certainly not least, we waged an epic struggle to tame Raft, our distributed consensus algorithm. In a resonant theme of these technical explanations, we had originally concluded that improvements to Raft that were on the drawing board could wait until after our general availability release. They were seen as necessary for much larger clusters, while the Raft algorithm’s impedance mismatch with CockroachDB’s architecture could simply be ignored for the time being. This proved a faulty assumption.</p>
<p>最后，也是最重要的一点，我们为驯服我们的分布式共识算法 Raft 进行了一场史诗般的斗争。 在这些技术解释的一个共鸣主题中，我们最初得出的结论是，对绘图板上的 Raft 的改进可以等到我们的正式版本发布之后。 它们被认为对于更大的集群是必要的，而 Raft 算法与 CockroachDB 架构的阻抗不匹配可以暂时忽略。 事实证明这是一个错误的假设。</p>
<p>Impedance mismatch? Yes, it turns out that Raft is a very busy protocol and typically suited to applications where only a small number of distinct instances, or “Raft groups”, are required. However, CockroachDB maintains a Raft group per range, and a large cluster will have hundreds of thousands or millions of ranges. Each Raft group elects a leader to coordinate updates, and the leader engages in periodic heartbeats to followers. If a heartbeat is missed, followers elect a new leader. For a large CockroachDB cluster, this meant a huge amount of heartbeat traffic, proportional to the total number of ranges in the system, not just ranges being actively read or written, and it was causing massive amounts of network traffic. This, in conjunction with lock contention and snapshots, would cause chain reactions. For example, too many heartbeats would fill network queues causing heartbeats to be missed, leading to reelection storms, thus bringing overall progress to a halt or causing node panics due to unconstrained memory usage. We had to fix this dynamic.</p>
<p>阻抗不匹配？ 是的，事实证明 Raft 是一个非常繁忙的协议，通常适合只需要少量不同实例或“Raft 组”的应用程序。 然而，CockroachDB 为每个范围维护一个 Raft 组，一个大型集群将有数十万或数百万个范围。 每个 Raft 组都会选出一个领导者来协调更新，领导者会定期向追随者发出心跳。 如果心跳丢失，追随者就会选出新的领导者。 对于大型 CockroachDB 集群来说，这意味着大量的心跳流量，与系统中范围的总数成正比，而不仅仅是主动读取或写入的范围，并且它导致了大量的网络流量。 这与锁争用和快照相结合，会导致连锁反应。 例如，太多的心跳会填满网络队列，导致心跳丢失，从而导致重选风暴，从而导致整体进度停止或由于内存使用不受限制而导致节点恐慌。 我们必须解决这个问题。</p>
<p>We undertook two significant changes. The first was lazy initialization of Raft groups. Previously, we’d cycle through every replica contained on a node at startup time, causing each to participate in their respective Raft groups as followers. Being lazy dramatically eased communication load on node startup. However, being lazy isn’t free: Raft groups require more time to respond to the first read or write request if they’re still “cold”, leading to higher latency variance. Still, the benefits outweighed that cost.</p>
<p>我们进行了两项重大变革。 第一个是 Raft 组的延迟初始化。 以前，我们会在启动时循环遍历节点上包含的每个副本，使每个副本作为追随者参与各自的 Raft 组。 懒惰极大地减轻了节点启动时的通信负载。 然而，偷懒并不是免费的：如果 Raft 组仍然“冷”，则需要更多时间来响应第一个读取或写入请求，从而导致更高的延迟方差。 尽管如此，好处还是超过了成本。</p>
<p>The success of lazy initialization led to a further insight: if Raft groups didn’t need to be active immediately after startup, why couldn’t they simply be decommissioned after use? We called this process “quiescence”, and applied it to Raft groups where all participants were fully replicated with no pending traffic remaining. The final heartbeat to the Raft group contains a special flag, telling participants to quiesce instead of being ready to campaign for a new leader if the leader fails further heartbeats.</p>
<p>延迟初始化的成功让我们有了进一步的认识：如果 Raft 组不需要在启动后立即处于活动状态，为什么不能在使用后简单地停用它们呢？ 我们将这个过程称为“静止”，并将其应用于 Raft 组，其中所有参与者都被完全复制，没有剩余的待处理流量。 Raft 组的最后一次心跳包含一个特殊的标志，如果领导者进一步的心跳失败，则告诉参与者保持静止，而不是准备竞选新的领导者。</p>
<p><strong>看原文中的动画</strong></p>
<p><em><strong>Simulation 2.</strong> In the left “Naive Raft” simulation, notice near constant sequence of heartbeats, denoted by the red RPCs between Raft groups. These are constant despite the slow trickle of writes from applications. In the right “Quiescing Raft” simulation, the Raft heartbeats occur only in order to quiesce after write traffic.</em></p>
<p>模拟 2. 在左侧的“Naive Raft”模拟中，请注意接近恒定的心跳序列，由 Raft 组之间的红色 RPC 表示。 尽管应用程序的写入速度缓慢，但这些都是恒定的。 在右侧的“Quiescing Raft”模拟中，Raft 心跳仅发生，以便在写入流量后停顿。</p>
<p>In addition to other changes, such as Raft batching, we managed to meaningfully reduce background traffic. <strong>By doing so, we also directly contributed to another key product goal, to constrain network, disk, and CPU usage to be directly proportional to the amount of data being read or written</strong>, and never proportional to the total size of data stored in the cluster.</p>
<p>除了 Raft 批处理等其他变化之外，我们还设法显着减少了后台流量。 通过这样做，我们还直接为另一个关键产品目标做出了贡献，即限制网络、磁盘和 CPU 使用率与读取或写入的数据量成正比，而不是与集群中存储的数据总大小成正比 。</p>
<h2 id="Outcomes-结果"><a href="#Outcomes-结果" class="headerlink" title="Outcomes 结果"></a>Outcomes 结果</h2><p>How did our process and management initiatives fare in addressing the three hypothesized root causes?</p>
<p>我们的流程和管理举措在解决三个假设的根本原因方面表现如何？</p>
<h3 id="Working-With-Two-Branches"><a href="#Working-With-Two-Branches" class="headerlink" title="Working With Two Branches"></a>Working With Two Branches</h3><p>Splitting the <em>master</em> branch was not without costs. It added significant overhead in near-daily merges from the <em>master</em> branch to <em>develop</em> in order to avoid conflicts and maintain compatibility with stability fixes. We effectively excluded changes to “core” packages from the <em>develop</em> branch in order to avoid a massive merge down the road. This held up some developer efforts, refactorings in particular, making it unpopular. In particular, Tamir Duberstein was a martyr for the stability cause, suffering the daily merge from <em>master</em> to <em>develop</em> at first quietly, and then with mounting frustration.</p>
<p>拆分主分支并不是没有成本的。 它增加了从主分支到开发的几乎日常合并的显着开销，以避免冲突并保持与稳定性修复的兼容性。 我们有效地从开发分支中排除了对“核心”包的更改，以避免将来发生大规模合并。 这阻碍了一些开发人员的努力，尤其是重构，使其不受欢迎。 尤其是塔米尔·杜伯斯坦（Tamir Duberstein），他是稳定事业的烈士，每天都在忍受着大师的融合，一开始默默地发展，后来却越来越沮丧。</p>
<p>Was the split branch necessary? A look at the data suggests not. There was significant churn in the <em>develop</em> branch, which counted 300 more commits during the split branch epoch. Despite that, there was no regression in stability when the branches were merged. We suspect that the successful merge is more the result of limits on changes to core components than to the split branches. While there is probably a psychological benefit to working in isolation on a stability branch, nobody is now arguing that was a crucial factor.</p>
<p>分裂分支有必要吗？ 数据显示并非如此。 开发分支中存在显着的流失，在分裂分支时期又增加了 300 次提交。 尽管如此，分支合并时稳定性并没有下降。 我们怀疑成功的合并更多地是对核心组件的更改限制的结果，而不是对拆分分支的限制。 虽然在稳定分支上独立工作可能有心理上的好处，但现在没有人认为这是一个关键因素。</p>
<h3 id="The-CockroachDB-Stability-Team"><a href="#The-CockroachDB-Stability-Team" class="headerlink" title="The CockroachDB Stability Team"></a>The CockroachDB Stability Team</h3><p>Designating a team with stability as the specific focus, and putting a single person in charge, proved invaluable. In our case, we drafted very experienced engineers, which may have led to a productivity hit in other areas. Since this was temporary, it was easy to justify given the severity of the problem.</p>
<p>事实证明，指定一个以稳定性为重点的团队并由一个人负责是非常有价值的。 就我们而言，我们招募了经验丰富的工程师，这可能会导致其他领域的生产力受到打击。 由于这是暂时的，考虑到问题的严重性，很容易证明其合理性。</p>
<p>Relocating team members for closer proximity <em>felt</em> like it meaningfully increased focus and productivity when we started. However, we ended up conducting a natural experiment on the efficacy of proximity. First two, and then three, out of the five stability team members ended up working remotely. Despite the increasing ratio of remote engineers, we did not notice an adverse impact on execution.</p>
<p>当我们开始时，重新安置团队成员以使其更加接近，感觉这有意义地提高了注意力和生产力。 然而，我们最终对邻近效应进行了一项自然实验。 五名稳定团队成员中，前两名、然后三名最终选择了远程工作。 尽管远程工程师的比例不断增加，但我们没有注意到对执行力的不利影响。</p>
<p>What ended up being more important than proximity were daily “stability sync” stand ups. These served as the backbone for coordination, and required only 30 minutes each morning. The agenda is (and remains): 1) status of each test cluster; 2) who’s working on what; 3) group discussion on clearing any blocking issues.</p>
<p>最终比接近更重要的是每日“稳定性同步”站会。 这些是协调的支柱，每天早上只需要 30 分钟。 议程是（并且仍然是）：1）每个测试集群的状态； 2）谁在做什么； 3）小组讨论清除任何阻塞问题。</p>
<p>We also held a twice-weekly “stability war room” and pressed any and all interested engineers into the role of “production monkey” each week. A production monkey is an engineer dedicated to overseeing production deployments and monitoring. Many contributions came from beyond the stability team, and the war rooms were a central point of coordination for the larger engineering org. Everyone pitching in with production duties raised awareness and familiarized engineers with deployment and debugging tools.</p>
<p>我们还举办了每周两次的“稳定作战室”，每周迫使所有感兴趣的工程师扮演“生产猴子”的角色。 生产猴子是专门负责监督生产部署和监控的工程师。 许多贡献来自稳定团队之外，而作战室是更大的工程组织的协调中心点。 每个参与生产职责的人都提高了意识，并让工程师熟悉了部署和调试工具。</p>
<h3 id="Fewer-People-More-Scrutiny-更少的人，更多的审查"><a href="#Fewer-People-More-Scrutiny-更少的人，更多的审查" class="headerlink" title="Fewer People, More Scrutiny 更少的人，更多的审查"></a>Fewer People, More Scrutiny 更少的人，更多的审查</h3><p>A smaller team with a mandate for greater scrutiny was a crucial success factor. In a testament to that, the structure has become more or less permanent. An analogy for achieving stability and then maintaining it is to imagine swimming in the ocean at night with little sense of what’s below or in which direction the shoreline is. We were <em>pretty sure</em> we weren’t far from a spot we could put our feet down and stop swimming, but every time we tried, we couldn’t touch bottom. Now that we’ve finally found a stable place, we can proceed with confidence; if we step off into nothingness, we can swim back a pace to reassess from a position of safety.</p>
<p>规模较小、接受更严格审查的团队是成功的关键因素。 证明这一点的是，该结构或多或少已经变得永久。 实现稳定并维持稳定的一个类比是想象一下晚上在海洋中游泳，几乎不知道下面是什么或海岸线在哪个方向。 我们非常确定我们离可以放下脚并停止游泳的地方不远了，但每次我们尝试时，我们都无法触底。 现在我们终于找到了一个稳定的地方，我们可以放心地继续前进了； 如果我们走进虚无，我们可以向后游一段距离，从安全的位置重新评估。</p>
<p>We now merge non-trivial changes to core components one-at-a-time by deploying the immediately-prior SHA, verifying it over the course of several hours of load, and then deploying the non-trivial change to verify expected behavior without regressions. This process works and has proven dramatically effective.</p>
<p>现在，我们通过部署前一 SHA，在几个小时的负载过程中验证它，然后部署重要更改以验证预期行为而不回归，将重要更改一次合并到核心组件 。 这个过程确实有效，并且已被证明非常有效。</p>
<p>The smaller stability team instituted obsessive review and gatekeeping for changes to core components. In effect, we went from a state of significant concurrency and decentralized review to a smaller number of clearly delineated efforts and centralized review.</p>
<p>规模较小的稳定性团队对核心组件的变更进行了严格的审查和把关。 实际上，我们从高度并发和分散审查的状态转变为少量明确划分的工作和集中审查。</p>
<p>Somewhat counter-intuitively, the smaller team saw an <em>increase per engineer</em> in pull request activity (see stream chart below).</p>
<p>有点违反直觉的是，较小的团队发现每个工程师的拉取请求活动有所增加（请参见下面的流程图）。</p>
<h2 id="Conclusions-on-CockroachDB-Stability"><a href="#Conclusions-on-CockroachDB-Stability" class="headerlink" title="Conclusions on CockroachDB Stability"></a>Conclusions on CockroachDB Stability</h2><p>In hindsight (and in Hacker News commentary), it seems negligent to have allowed stability to become such a pressing concern. Shouldn’t we have realized earlier that the problem wasn’t going away without changing our approach? One explanation is the analogy of the frog in the slowly heating pot of water. Working so closely with the system, day in and day out, we failed to notice how stark the contrast had become between our stability expectations pre-beta and the reality in the months that followed. There were many distractions: rapid churn in the code base, new engineers starting to contribute, and no team with stability as its primary focus. In the end, we jumped out of the pot, but not before the water had gotten pretty damn hot.</p>
<p>事后看来（以及《黑客新闻》的评论），让稳定性成为如此紧迫的问题似乎是疏忽大意。 难道我们不应该早点意识到，如果不改变我们的方法，问题就不会消失吗？ 一种解释是用缓慢加热的锅中的青蛙来比喻。 日复一日地与系统密切合作，我们没有注意到测试前的稳定性预期与接下来几个月的现实之间存在多么鲜明的对比。 有很多干扰因素：代码库的快速变动、新工程师开始贡献、没有团队以稳定性为主要关注点。 最后，我们跳出了锅，但水已经变得非常热了。</p>
<p>Many of us at Cockroach Labs had worked previously on complex systems which took their own sweet time to stabilize. Enough, that we hold a deep-seated belief that such problems are tractable. We posited that if we stopped all other work on the system, a small group of dedicated engineers could fix stability in a matter of weeks. I can’t stress enough how powerful belief in an achievable solution can be.</p>
<p>Cockroach Labs 的许多人之前都曾研究过复杂的系统，但这些系统花了很长时间才稳定下来。 我们坚信这些问题是可以解决的，这就足够了。 我们假设，如果我们停止系统上的所有其他工作，一小群专门的工程师可以在几周内修复稳定性。 我无法充分强调对可实现的解决方案的信念是多么强大。</p>
<h3 id="Could-we-have-avoided-instability"><a href="#Could-we-have-avoided-instability" class="headerlink" title="Could we have avoided instability?"></a>Could we have avoided instability?</h3><p>Ah, the big question, and here I’m going to use “I” instead of “we”.</p>
<p>啊，这是一个大问题，在这里我将使用“我”而不是“我们”。</p>
<p><a href="https://news.ycombinator.com/item?id=12362473">Hacker News commentary</a> on my previous blog post reveals differing viewpoints. What I’m going to say next is simply conjecture as I can’t assert the counterfactual is possible, and nobody can assert that it’s impossible. However, since I’m unaware of any complex, distributed system having avoided a period of instability, I’ll weakly assert that it’s quite unlikely. So, I’ll present an argument from experience, with the clear knowledge that it’s a fallacy. Enough of a disclaimer?</p>
<p>《黑客新闻》对我之前博客文章的评论揭示了不同的观点。 接下来我要说的只是猜想，因为我不能断言反事实是可能的，也没有人能断言这是不可能的。 然而，由于我不知道有任何复杂的分布式系统避免了一段不稳定时期，所以我会弱弱地断言这是不太可能的。 因此，我将根据经验提出一个论点，并清楚地知道这是一个谬论。 足够的免责声明吗？</p>
<p>I’ve worked on several systems in the same mold as CockroachDB and none required less than months to stabilize. Chalk one up for personal anecdote. While I didn’t work on Spanner at Google, my understanding is that it took a long time to stabilize. I’ve heard estimates as long as 18 months. Many popular non-distributed databases, both SQL and NoSQL, open source and commercial, took years to stabilize. Chalk several up for anecdotal hearsay.</p>
<p>我曾在多个与 CockroachDB 相同的系统上工作过，但没有一个系统需要不到几个月的时间才能稳定下来。 记下个人轶事。 虽然我没有在 Google 从事过 Spanner 的工作，但我的理解是它需要很长时间才能稳定下来。 我听说过长达 18 个月的估计。 许多流行的非分布式数据库，无论是 SQL 还是 NoSQL，无论是开源的还是商业的，都需要数年时间才能稳定下来。 记下一些轶事传闻。</p>
<p>While <a href="https://www.microsoft.com/en-us/research/publication/ironfleet-proving-practical-distributed-systems-correct/">proving distributed systems correct</a> is possible, it likely wouldn’t apply to the kinds of stability problems which have plagued CockroachDB. After all, the system worked as designed in most cases; there was emergent behavior as a result of complex interactions.</p>
<p>虽然证明分布式系统的正确性是可能的，但它可能不适用于困扰 CockroachDB 的稳定性问题。 毕竟，在大多数情况下，系统都按设计运行。 由于复杂的相互作用，出现了紧急行为。</p>
<p>I’d like to conclude with several practical suggestions for mitigating instability in future efforts.</p>
<p>最后，我想提出几项切实可行的建议，以减轻未来工作中的不稳定因素。</p>
<ul>
<li><p>Define a less ambitious minimally viable product (MVP) and hope to suffer less emergent complexity and a smaller period of instability. Proceed from there in an incremental fashion, preventing further instability with a careful process to catch regressions.</p>
<p>定义一个不太雄心勃勃的最低限度可行产品（MVP），并希望遭遇更少的紧急复杂性和更短的不稳定期。 从这里开始以渐进的方式进行，通过仔细的过程来捕捉回归，防止进一步的不稳定。</p>
</li>
<li><p>When a system is functionally complete, proceed immediately to a laser focus on stability. Form a team with an experienced technical lead, and make stability its sole focus. Resist having everyone working on stability. Clearly define accountability and ownership.</p>
<p>当系统功能完成后，立即开始重点关注稳定性。 组建一支经验丰富的技术领导团队，并以稳定性为唯一关注点。 抵制让每个人都致力于稳定。 明确界定责任和所有权。</p>
</li>
<li><p>Systems like CockroachDB must be tested in a real world setting. However, there is significant overhead to debugging a cluster on AWS. The cycle to develop, deploy, and debug using the cloud <strong>is very slow</strong>. An incredibly helpful intermediate step is to deploy clusters locally as part of every engineer’s normal development cycle (use multiple processes on different ports). See the <a href="https://github.com/cockroachdb/cockroach/tree/master/pkg/cmd/allocsim">allocsim</a> and <a href="https://github.com/cockroachdb/cockroach/tree/master/pkg/cmd/zerosum">zerosum</a> tools.</p>
<p>像 CockroachDB 这样的系统必须在现实环境中进行测试。 然而，在 AWS 上调试集群会产生大量开销。 使用云进行开发、部署和调试的周期非常慢。 一个非常有用的中间步骤是在本地部署集群，作为每个工程师正常开发周期的一部分（在不同端口上使用多个进程）。 请参阅 allocsim 和 Zerosum 工具。</p>
</li>
</ul>
<p>Does building a <a href="https://www.cockroachlabs.com/blog/what-is-distributed-sql/">distributed SQL</a> system and untangling all of its parts sound like your ideal Tuesday morning? If so, we’re hiring! Check out our open positions <a href="https://cockroa.ch/eng_hiring">here</a>.</p>
<p>构建一个分布式 SQL 系统并理清其所有部分听起来是不是您理想的周二早上？ 如果是这样，我们正在招聘！ 在这里查看我们的空缺职位。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Joint consensus in CockroachDB</title>
    <url>/2024/01/25/2-%E6%95%B0%E6%8D%AE%E5%BA%93/1-cockroachdb/cockroach-blogs/10-Joint%20consensus%20in%20CockroachDB/</url>
    <content><![CDATA[<h1 id="Availability-and-region-failure-Joint-consensus-in-CockroachDB"><a href="#Availability-and-region-failure-Joint-consensus-in-CockroachDB" class="headerlink" title="Availability and region failure: Joint consensus in CockroachDB"></a>Availability and region failure: Joint consensus in CockroachDB</h1><p>At Cockroach Labs, we <a href="https://www.cockroachlabs.com/blog/raft-is-so-fetch/">write</a> <a href="https://www.cockroachlabs.com/blog/scaling-raft/">quite</a> <a href="https://www.cockroachlabs.com/blog/consensus-made-thrive/">a bit</a> about consensus algorithms. They are a critical component of CockroachDB and we rely on them in the lower layers of our transactional, scalable, distributed key-value store. In fact, large clusters can contain tens of thousands of consensus groups because in CockroachDB, every Range (similar to a shard) is an independent consensus group. Under the hood, we run a large number of instances of <a href="https://raft.github.io/">Raft</a> (a consensus algorithm), which has come with interesting engineering challenges. This post dives into one that we’ve tackled recently: adding support for atomic replication changes (“Joint Quorums”) to <a href="https://github.com/etcd-io/etcd/blob/master/raft/README.md">etcd&#x2F;raft</a> and using them in CockroachDB to improve resilience against region failures.</p>
<p>在 Cockroach Labs，我们写了很多关于共识算法的文章。 它们是 CockroachDB 的关键组件，我们在事务性、可扩展、分布式键值存储的较低层中依赖它们。 事实上，大型集群可以包含数万个共识组，因为在 CockroachDB 中，每个 Range（类似于分片）都是一个独立的共识组。 在幕后，我们运行了大量 Raft（一种共识算法）实例，这带来了有趣的工程挑战。 这篇文章深入探讨了我们最近解决的一个问题：向 etcd&#x2F;raft 添加对原子复制更改（“联合仲裁”）的支持，并在 CockroachDB 中使用它们来提高针对区域故障的恢复能力。</p>
<span id="more"></span>

<p>A replication change is a configuration change of a Range, that is, a change in where the consistent copies of that Range should be stored. Let’s use a standard deployment topology to illustrate this.</p>
<p>复制更改是 Range 的配置更改，即该 Range 的一致副本的存储位置的更改。 让我们使用标准部署拓扑来说明这一点。</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/Screen-Shot-2019-11-26-at-12.31.10-PM.png?auto=format,compress&max-w=700"></p>
<p>The above deployment has three regions (i.e. data centers). CockroachDB enables globally deployed applications, so these regions may well be placed across the globe. We see that there are two nodes in each of the regions X, Y and Z, and we see a Range which has one replica (“copy”) in each of the regions. This deployment survives the failure of a single region: consensus replication will continue to work as long as a majority of replicas are available. If a region fails, we lose at most one replica and two - a majority - remain, so the database will continue to operate normally after a short timeout. If we placed two replicas in, say, Z and the third replica in Y, a failure of region X would take two of the replicas with it, leaving only a single replica available; this single survivor would not be able to serve requests:</p>
<p>上述部署具有三个区域（即数据中心）。 CockroachDB 支持全球部署应用程序，因此这些区域很可能位于全球各地。 我们看到 X、Y 和 Z 区域中的每个区域都有两个节点，并且我们看到一个 Range，每个区域中都有一个副本（“副本”）。 此部署可以在单个区域出现故障时继续运行：只要大多数副本可用，共识复制就会继续工作。 如果一个区域发生故障，我们最多会丢失一个副本，并保留两个（大多数）副本，因此数据库将在短暂超时后继续正常运行。 如果我们在 Z 中放置两个副本，在 Y 中放置第三个副本，则区域 X 的故障将带走其中两个副本，仅留下一个副本可用； 该幸存者将无法满足请求：</p>
<img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/T01P-1.gif?auto=format,compress&max-w=700" style="zoom: 67%;" />

<p>CockroachDB dynamically adjusts the data placement to account for shifts in node utilization. As a result, it may want to laterally move the replica from one node to another within, say, region X. To make it concrete, let’s say we want to move it from <em>X</em>2 to <em>X</em>1. We may want to do this because the operator has specified that <em>X</em>2 should go down for maintenance, or <em>X</em>2 has much higher CPU usage than <em>X</em>1.</p>
<p>CockroachDB 动态调整数据放置以适应节点利用率的变化。 因此，它可能希望将副本从一个节点横向移动到区域 X 内的另一个节点。为了具体起见，假设我们希望将其从 <em>X</em>2 移动到 <em>X</em>1。 我们可能想要这样做，因为操作员已指定 <em>X</em>2 应停机进行维护，或者 <em>X</em>2 的 CPU 使用率比 <em>X</em>1 高得多。</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/T02P-1.gif?auto=format,compress&max-w=700"></p>
<p>As the diagram shows, the Raft group under consideration is initially located on <em>X</em>2,<em>Y</em>1,<em>Z</em>2. The active configuration of this group will be the majority configuration <em>X</em>2,<em>Y</em>1,<em>Z</em>2, meaning two out of these three are required to make a decision (such as electing a leader, or committing a log entry).</p>
<p>如图所示，所考虑的 Raft 组最初位于<em>X</em>2、<em>Y</em>1、<em>Z</em>2 上。 该组的活动配置将是多数配置<em>X</em>2、<em>Y</em>1、<em>Z</em>2，这意味着需要这三个配置中的两个来做出决定（例如选举领导者或提交日志条目） ）</p>
<p>We want to laterally move from <em>X</em>2 to <em>X</em>1, that is, we’d like to end up with replicas on <em>X</em>1,<em>Y</em>1, and <em>Z</em>2 in the corresponding configuration (<em>X</em>1,<em>Y</em>1,<em>Z</em>2). But how does that actually happen?</p>
<p>我们希望从 <em>X</em>2 横向移动到 <em>X</em>1，也就是说，我们希望最终在相应配置中的 <em>X</em>1、<em>Y</em>1 和 <em>Z</em>2 上获得副本 (* X<em>1、</em>Y<em>1、</em>Z*2)。 但这实际上是如何发生的呢？</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/Screen-Shot-2019-11-26-at-12.39.12-PM.png?auto=format,compress&max-w=700"></p>
<p>In Raft, a configuration change is initiated by proposing a special log entry which, when received by a replica, switches it over to the new configuration. Since the replicas forming the Range receive this command at different points in time, they do not switch over in a coordinated fashion and care must be taken to avoid the dreaded “split brain” scenario, in which two distinct groups of peers both think they have the right to make decisions. Here’s how this could happen in our particular example:</p>
<p>在 Raft 中，配置更改是通过提出一个特殊的日志条目来启动的，当副本收到该日志条目时，会将其切换到新配置。 由于形成范围的副本在不同的时间点接收到此命令，因此它们不会以协调的方式进行切换，并且必须小心避免可怕的“裂脑”场景，在这种情况下，两个不同的对等组都认为它们拥有 做出决定的权利。 在我们的特定示例中，这是如何发生的：</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/T04P.gif?auto=format,compress&max-w=700"></p>
<p><em>Y</em>1 is the first node to receive the configuration change in its log, and it immediately switches to <em>C</em>2&#x3D;<em>X</em>1, <em>Y</em>1, <em>Z</em>2. It catches up <em>X</em>1, which consequently also switches to <em>C</em>2. <em>X</em>1 and <em>Y</em>1 form a quorum of <em>C</em>2, so they can now append log entries without consulting with either <em>X</em>2 or <em>Z</em>2. But - both <em>X</em>2 and <em>Z</em>2 are still using <em>C</em>1&#x3D;(<em>X</em>2, <em>Y</em>1, <em>Z</em>2) and have no idea a new configuration is active elsewhere. If they happen to not learn about this in a timely manner from <em>Y</em>1 - imagine a well-placed network disruption - they might decide to elect a leader between themselves and may start appending their own entries at conflicting log positions - split brain. Membership changes are tricky!</p>
<p><em>Y</em>1 是第一个在其日志中接收配置更改的节点，它立即切换到 <em>C</em>2&#x3D;<em>X</em>1, <em>Y</em>1, <em>Z</em>2。 它赶上<em>X</em>1，因此也切换到<em>C</em>2。 <em>X</em>1 和 <em>Y</em>1 形成 <em>C</em>2 的法定人数，因此它们现在可以附加日志条目，而无需咨询 <em>X</em>2 或 <em>Z</em>2。 但是 - <em>X</em>2 和 <em>Z</em>2 仍在使用 <em>C</em>1&#x3D;(<em>X</em>2, <em>Y</em>1, <em>Z</em>2) 并且不知道新配置在其他地方处于活动状态。 如果他们碰巧没有及时从 <em>Y</em>1 处了解到这一点 - 想象一下适当的网络中断 - 他们可能会决定在他们之间选举一个领导者，并可能开始在冲突的日志位置附加自己的条目 - 脑裂。 会员变更很棘手！</p>
<p>One way to look at the above is that we were trying to “change too much at once”: we effectively added a node (<em>X</em>1) and removed a node (<em>X</em>2) at the same time. Maybe things would turn out OK if we carried these changes out individually, waiting for a majority to have received the first change before starting the second?</p>
<p>看待上述情况的一种方法是，我们试图“一次改变太多”：我们有效地添加了一个节点（<em>X</em>1）并同时删除了一个节点（<em>X</em>2）。 如果我们单独进行这些更改，等待大多数人收到第一个更改后再开始第二个更改，也许事情会好起来？</p>
<p>It turns out that this is true. Let’s add <em>X</em>1 first before removing <em>X</em>2. This means that in the above illustration we’ll have <em>C</em>2&#x3D;(<em>X</em>1, <em>X</em>2, <em>Y</em>1, <em>Z</em>2). Note that now that there are four nodes, there are three nodes required for majority consensus. This means that when <em>X</em>1 and <em>Y</em>1 have both switched to the new configuration, they can’t make their own decisions just yet - they need to loop in one additional peer (and tell it about <em>C</em>2), that is, either <em>X</em>2 or <em>Z</em>2. Whichever one they pick effectively leaves a single replica using <em>C</em>1, but without a friend to form a separate quorum with. Similarly, we can convince ourselves that removing one node at a time is safe, too.</p>
<p>事实证明这是真的。 我们先添加<em>X</em>1，然后再删除<em>X</em>2。 这意味着在上图中我们将有 <em>C</em>2&#x3D;(<em>X</em>1, <em>X</em>2, <em>Y</em>1, <em>Z</em>2)。 请注意，现在有四个节点，需要三个节点才能达成多数共识。 这意味着当 <em>X</em>1 和 <em>Y</em>1 都切换到新配置时，它们还不能做出自己的决定 - 它们需要循环一个额外的对等点（并告诉它有关 <em>C</em>2 的信息） ，即 <em>X</em>2 或 <em>Z</em>2。 无论他们选择哪一个，都会使用 <em>C</em>1 有效地留下一个副本，但没有朋友可以与之形成单独的仲裁。 同样，我们可以说服自己，一次删除一个节点也是安全的。</p>
<p>Breaking complex configuration changes such as lateral moves into “safer” individual parts is how CockroachDB worked for a long time. However, does it work with our deployment above? Let’s take another look at the intermediate state we’re in after adding <em>X</em>1, but before removing <em>X</em>2 (similar problems occur if we remove <em>X</em>2 first, then add <em>X</em>1):</p>
<p>打破复杂的配置更改（例如横向移动）到“更安全”的单独部分是 CockroachDB 长期以来的工作方式。 但是，它适用于我们上面的部署吗？ 我们再看一下添加<em>X</em>1之后、删除<em>X</em>2之前的中间状态（如果我们先删除<em>X</em>2，然后添加<em>X</em>1，也会出现类似的问题）：</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/T03P.gif?auto=format,compress&max-w=700"></p>
<p>Remember how we realized earlier that by placing two replicas in a single region, we could run into trouble? This is exactly what we were forced to do here. If region X fails, we’re in trouble: we lose two replicas at once, leaving two survivors unable to muster up the third replica required for a healthy majority (recall that a group of size four needs three replicas to make progress). As a result, this Range will stop accepting traffic until region X comes back up - violating the guarantees we were expecting from this deployment topology. We might try to argue that configuration changes are rare enough to make this a non-issue, but we’ve found that this does not hold. A CockroachDB cluster maintains many thousands of Ranges; at any given time, there might be a configuration change going on on some Range. But even without taking that into account, compromising availability in ways not transparent to the user is deeply unsatisfying to us.</p>
<p>还记得我们之前如何意识到，通过将两个副本放在一个区域中，我们可能会遇到麻烦吗？ 这正是我们被迫在这里做的事情。 如果区域 X 发生故障，我们就会遇到麻烦：我们同时丢失两个副本，导致两个幸存者无法召集健康多数所需的第三个副本（回想一下，一个规模为 4 的小组需要三个副本才能取得进展）。 因此，该范围将停止接受流量，直到区域 X 恢复正常 - 这违反了我们对此部署拓扑的预期保证。 我们可能会试图争辩说，配置更改非常罕见，因此这不是问题，但我们发现这并不成立。 一个 CockroachDB 集群维护着数千个 Range； 在任何给定时间，某些 Range 上可能会发生配置更改。 但即使不考虑这一点，以对用户不透明的方式损害可用性也让我们非常不满意。</p>
<p>Until recently, CockroachDB mitigated this problem by carrying out the two adjacent membership changes as fast as possible, to minimize the time spent in the vulnerable configuration. However, it was clear that we couldn’t accept this state of affairs permanently and set out to address the issue in the <a href="https://www.cockroachlabs.com/blog/cockroachdb-19dot2-release/">19.2 release of CockroachDB</a>.</p>
<p>直到最近，CockroachDB 通过尽快执行两个相邻的成员资格更改来缓解此问题，以最大限度地减少在易受攻击的配置中花费的时间。 然而，很明显我们不能永远接受这种状况，并着手在 CockroachDB 19.2 版本中解决这个问题。</p>
<p>The solution to our problem is outlined in the dissertation in which the Raft consensus algorithm was first introduced, and is named Joint Consensus. The idea is to pick a better intermediate configuration than we did in our example above - one that doesn’t force us to put two replicas into a single region.</p>
<p>我们问题的解决方案在首次引入 Raft 共识算法的论文中概述，并被命名为“联合共识”。 我们的想法是选择一个比我们在上面的示例中更好的中间配置 - 该配置不会强迫我们将两个副本放入一个区域中。</p>
<p>What if our intermediate configuration instead “joined” the initial and final configuration together, requiring agreement of both? This is exactly what Joint Consensus does. Sticking to our example, we would go from our initial configuration C<em>1&#x3D;<em>X</em>2,<em>Y</em>1,<em>Z</em>2 to the “joint configuration” <em>C</em>1 &amp;&amp; <em>C</em>2 &#x3D; (X</em>2,<em>Y</em>,<em>Z</em>2) &amp;&amp; (<em>X</em>1,<em>Y</em>1,<em>Z</em>2):</p>
<p>如果我们的中间配置将初始配置和最终配置“连接”在一起，需要两者达成一致，该怎么办？ 这正是联合共识所做的。 坚持我们的示例，我们将从初始配置 C<em>1&#x3D;<em>X</em>2,<em>Y</em>1,<em>Z</em>2 变为“联合配置”</em>C<em>1 &amp;&amp; <em>C</em>2 &#x3D; (X</em>2 ,<em>Y</em>,<em>Z</em>2) &amp;&amp; (<em>X</em>1,<em>Y</em>1,<em>Z</em>2):</p>
<p><img src="https://crl2020.imgix.net/wp-content/uploads/2019/11/Screen-Shot-2019-11-26-at-12.40.21-PM.png?auto=format,compress&max-w=700"></p>
<p>In this configuration, making a decision requires agreement of a majority of <em>C</em>1 as well as a majority of <em>C</em>2. Revisiting our earlier counter-example in which <em>X</em>2and <em>Z</em>2 had not received the old configuration yet, we find that the split-brain is impossible: <em>X</em>1 and <em>Y</em>1 (who are using the joint configuration) can’t make a decision without contacting either <em>X</em>2 or <em>Z</em>2, preventing split-brain. At the same time, the joint configuration survives a region outage just fine, since both <em>C</em>1 and <em>C</em>2 do so individually!</p>
<p>在此配置中，做出决定需要 <em>C</em>1 的多数以及 <em>C</em>2 的多数同意。 回顾我们之前的反例，其中<em>X</em>2和<em>Z</em>2尚未收到旧配置，我们发现裂脑是不可能的：<em>X</em>1和<em>Y</em>1（正在使用联合配置） ）在不联系 <em>X</em>2 或 <em>Z</em>2 的情况下无法做出决定，从而防止脑裂。 同时，联合配置能够很好地承受区域中断，因为 <em>C</em>1 和 <em>C</em>2 都是单独执行的！</p>
<p>Hence, the plan was clear: implement joint configuration changes, and use them. This provided a welcome opportunity to contribute back to the community, as we share a Raft implementation with the <a href="https://etcd.io/">etcd project</a>. etcd is a distributed key-value store commonly used for configuration management (notably, it backs <a href="https://kubernetes.io/">Kubernetes</a>), and we’ve been an active maintainer (and user) of its <a href="https://github.com/etcd-io/etcd/tree/master/raft#raft-library">etcd&#x2F;raft</a> library well before Cockroach Labs even sprung into existence in 2015.</p>
<p>因此，计划很明确：实施联合配置更改并使用它们。 这提供了一个回馈社区的好机会，因为我们与 etcd 项目共享 Raft 实现。 etcd 是一种常用于配置管理的分布式键值存储（值得注意的是，它支持 Kubernetes），早在 Cockroach Labs 于 2015 年出现之前，我们就一直是其 etcd&#x2F;raft 库的积极维护者（和用户）。</p>
<p>At this point, it’s time for a juicy confession:</p>
<p>此时此刻，是时候进行一次多汁的坦白了：</p>
<p>etcd&#x2F;raft doesn’t actually <em>really implement</em> the Raft consensus algorithm.</p>
<p>etcd&#x2F;raft 实际上并没有真正实现 Raft 共识算法。</p>
<p>It does closely follow the specification for the most part, but with one marked difference: configuration changes. We’ve explained above that in Raft, a peer should switch to the new configuration the moment it is appended to its log. In etcd&#x2F;raft, the peer switches to the new configuration the moment is committed and applied to the state machine.</p>
<p>它在很大程度上确实严格遵循规范，但有一个明显的区别：配置更改。 我们上面已经解释过，在 Raft 中，对等点应该在新配置追加到其日志时立即切换到新配置。 在 etcd&#x2F;raft 中，对等方在提交并应用于状态机时切换到新配置。</p>
<p>The difference may seem small, but it carries weight. Briefly put,</p>
<p>差异看似很小，但意义重大。 简而言之，</p>
<ul>
<li><p>the “Raft way” is proven correct in the paper, but more awkward use from the app, while</p>
<p>“Raft 方式”在论文中被证明是正确的，但在应用程序中使用起来比较尴尬，而</p>
</li>
<li><p>the “etcd&#x2F;raft way” comes with subtle problems that require subtle fixes, but has a more natural external API.</p>
<p>“etcd&#x2F;raft 方式”存在一些微妙的问题，需要微妙的修复，但具有更自然的外部 API。</p>
</li>
</ul>
<p>We took the opportunity to discuss with the other maintainers whether etcd&#x2F;raft should fall in line with the spec. In the process, we uncovered some previously unknown potential correctness problems. A little later, <a href="https://github.com/hicqu">Peng Qu</a> over from <a href="https://pingcap.com/en/">PingCap</a> (they’re using a <a href="https://www.rust-lang.org/">Rust</a> implementation of Raft very similar to etcd&#x2F;raft) alerted us to <a href="https://github.com/etcd-io/etcd/issues/7625#issuecomment-551041747">yet another problem</a>.</p>
<p>我们借此机会与其他维护者讨论 etcd&#x2F;raft 是否应该符合规范。 在此过程中，我们发现了一些以前未知的潜在正确性问题。 过了一会儿，来自 PingCap 的 Peng Qu（他们正在使用与 etcd&#x2F;raft 非常相似的 Raft 的 Rust 实现）提醒我们注意另一个问题。</p>
<p>After we found and implemented <a href="https://github.com/etcd-io/etcd/issues/7625#issuecomment-489232411">solutions</a> for both problems, we arrived at a good understanding about the additional invariants that truly make etcd&#x2F;raft’s approach safe. At this point, neither we nor the maintainer community felt that changing to the “Raft way” now provided a good return on what would have been a very large investment in etcd&#x2F;raft and all of its implementers (!). In this particular case, it seemed better to be more complicated internally, remain easy to use externally (though with a wart or two), while keeping the battle-tested code we had in place mostly intact.</p>
<p>在我们找到并实施这两个问题的解决方案之后，我们对真正使 etcd&#x2F;raft 方法安全的额外不变量有了很好的理解。 在这一点上，我们和维护者社区都认为，改变为“Raft 方式”现在可以为 etcd&#x2F;raft 及其所有实现者带来巨大的投资带来良好的回报（！）。 在这种特殊情况下，最好在内部变得更复杂，在外部保持易于使用（尽管有一两个缺点），同时保持我们现有的经过实战测试的代码基本完整。</p>
<p>With this detour out of the way, we went ahead and implemented joint configuration changes. Now, a few months and 22 pull requests later, anyone using <a href="https://github.com/etcd-io/etcd/tree/master/raft#raft-library">etcd&#x2F;raft</a> can enjoy the well-maintained fruits of our work. Additionally, we <a href="https://github.com/etcd-io/etcd/blob/master/raft/rafttest/interaction_env.go#L42">added</a> <a href="https://github.com/cockroachdb/datadriven">datadriven</a> testing machinery that significantly simplifies testing complex interactions within Raft peers (see <a href="https://github.com/etcd-io/etcd/blob/master/raft/testdata/confchange_v2_add_double_auto.txt">here</a> for a sample). This significantly simplifies testing and provides fertile grounds for future work or even just explorations.</p>
<p>绕过这个弯路后，我们继续实施联合配置更改。 现在，几个月和 22 个 Pull 请求之后，任何使用 etcd&#x2F;raft 的人都可以享受我们维护良好的工作成果。 此外，我们添加了数据驱动的测试机制，显着简化了 Raft 对等体中复杂交互的测试（请参阅此处的示例）。 这极大地简化了测试，并为未来的工作甚至探索提供了肥沃的基础。</p>
<p>Naturally we also started using this new functionality in CockroachDB’s recent <a href="https://www.cockroachlabs.com/blog/cockroachdb-19dot2-release/">19.2 release</a>. If you haven’t given us a try yet, it’s easy to do so either <a href="https://www.cockroachlabs.com/get-cockroachdb/">locally or in the cloud</a>.</p>
<p>当然，我们也在 CockroachDB 最近的 19.2 版本中开始使用这个新功能。 如果您还没有尝试过，可以在本地或云端轻松尝试。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
</search>
